{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36fc75d-da4e-4beb-ab7c-340a5491a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59dd8a9-7d9b-4675-8134-5fc8550ca9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389f60f4-1176-4220-81e6-484596e579d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"flat_trees.csv.gzip\"\n",
    "\n",
    "dtypes = {\n",
    "    \"tweet_id\":str, \n",
    "    \"created_at\":str,\n",
    "    \"root_account\":str,\n",
    "    \"tree_id\":str,\n",
    "    \"in_reply_to\":str,\n",
    "    \"tree_nr\":int,\n",
    "    \"tweet_nr\":int,\n",
    "    \"text\":str,\n",
    "    \"hate_score\":float,\n",
    "    \"counter_score\":float,\n",
    "    \"TOXICITY\":float,\n",
    "}\n",
    "tweets = pd.read_csv(\n",
    "    Path(src, fname), \n",
    "    compression=\"gzip\", \n",
    "    dtype=dtypes,\n",
    "    usecols=dtypes.keys(),\n",
    "    parse_dates=[\"created_at\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329be44-6099-47f9-9902-81f4f4da04af",
   "metadata": {},
   "source": [
    "# Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ca8e286-4b43-4e0f-9cb0-d22f4e2e70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process texts for emotion detection and upload to GPU cluster for inference\n",
    "'''\n",
    "tweets[\"text_preprocessed\"] = tweets[\"text\"]\\\n",
    "    .apply(lambda x: re.sub(r\"https?:\\/\\/\\S*\", \"\", x, flags=re.MULTILINE))\n",
    "texts = tweets[\"text_preprocessed\"]\n",
    "np.savetxt(\"texts_for_emodetection.txt\", texts, fmt=\"%s\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b944ab1-a38e-471f-9d9e-784d8c3ec4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending incremental file list\n",
      "texts_for_emodetection.txt\n",
      "\n",
      "sent 51,747,100 bytes  received 35 bytes  20,698,854.00 bytes/sec\n",
      "total size is 132,136,005  speedup is 2.55\n"
     ]
    }
   ],
   "source": [
    "# ! rsync -avze ssh texts_for_emodetection.txt jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/\n",
    "# ! rm texts_for_emodetection.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a61f4e-eae7-4722-8b53-039413974025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on nvcluster in /home/jasse/german_emotion_classification/\n",
    "# prediction_multigpu_german.0.1.1.py ../counterspeech-strategies/data/inference/texts_for_emodetection.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14d6b621-79ce-40e8-8e66-d35fbda5893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receiving incremental file list\n",
      "texts_for_emodetection.txt_german_emodetection\n",
      "\n",
      "sent 43 bytes  received 51,219,904 bytes  9,312,717.64 bytes/sec\n",
      "total size is 119,048,517  speedup is 2.32\n"
     ]
    }
   ],
   "source": [
    "# fetch the data from the GPU cluster\n",
    "#! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/texts_for_emodetection.txt_german_emodetection ../../../data/inference/\n",
    "#! mv texts_for_emodetection.txt_german_emodetection emotions.tsv\n",
    "#! xz emotions.tsv --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ba4381-5732-4a55-9790-852cdae8ebb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>disgust</th>\n",
       "      <th>sadness</th>\n",
       "      <th>joy</th>\n",
       "      <th>enthusiasm</th>\n",
       "      <th>pride</th>\n",
       "      <th>hope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.525099</td>\n",
       "      <td>0.557333</td>\n",
       "      <td>0.011350</td>\n",
       "      <td>0.377578</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>0.022883</td>\n",
       "      <td>0.011487</td>\n",
       "      <td>0.086530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.322441</td>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.015819</td>\n",
       "      <td>0.156091</td>\n",
       "      <td>0.224147</td>\n",
       "      <td>0.119278</td>\n",
       "      <td>0.807087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.110118</td>\n",
       "      <td>0.036345</td>\n",
       "      <td>0.009142</td>\n",
       "      <td>0.025803</td>\n",
       "      <td>0.928969</td>\n",
       "      <td>0.049298</td>\n",
       "      <td>0.178908</td>\n",
       "      <td>0.332939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      anger      fear   disgust   sadness       joy  enthusiasm     pride  \\\n",
       "0  0.525099  0.557333  0.011350  0.377578  0.012516    0.022883  0.011487   \n",
       "1  0.322441  0.074408  0.003789  0.015819  0.156091    0.224147  0.119278   \n",
       "2  0.110118  0.036345  0.009142  0.025803  0.928969    0.049298  0.178908   \n",
       "\n",
       "       hope  \n",
       "0  0.086530  \n",
       "1  0.807087  \n",
       "2  0.332939  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the emotion labels\n",
    "dtypes = {\n",
    "    \"anger\":float,\n",
    "    \"fear\":float,\n",
    "    \"disgust\":float,\n",
    "    \"sadness\":float,\n",
    "    \"joy\":float,\n",
    "    \"enthusiasm\":float,\n",
    "    \"pride\":float,\n",
    "    \"hope\":float\n",
    "}\n",
    "\n",
    "emotions = pd.read_csv(\n",
    "    Path(src, \"inference\", \"emotions.tsv.xz\"),\n",
    "    delimiter=\"\\t\",\n",
    "    compression=\"xz\",\n",
    "    dtype=dtypes\n",
    ")\n",
    "emotions.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e9081f-2f1d-4a7e-b607-3165492bc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add emotion labels to tweets data frame. Note that emotion labels are in the\n",
    "# same order as the tweet texts\n",
    "assert len(emotions) == len(tweets)\n",
    "tweets = pd.concat([tweets, emotions], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987112e-c90d-4ed6-8d3c-242371c6fe58",
   "metadata": {},
   "source": [
    "# User IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61e55e6-d90b-419c-a2ad-510c48bc8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../data/\"\n",
    "fname = \"flat_trees_ids_only.csv.gzip\"\n",
    "IDs = pd.read_csv(\n",
    "    Path(src, fname), \n",
    "    dtype={\"tweet_id\":str, \"user_twitter_id\":str},\n",
    "    compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aaa1581-08bd-4cea-aed0-74432520aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.merge(\n",
    "    tweets,\n",
    "    IDs,\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")\n",
    "del IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bbd57a-ab03-440c-b2ea-3afb43a61de2",
   "metadata": {},
   "source": [
    "# Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035dcf6-db06-4f68-871c-200c1b88ed5c",
   "metadata": {},
   "source": [
    "**Note:** The foreign tweets are already removed from the inferred tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef4d537-2137-4dd5-ba20-a3b8876423e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/flat_trees_inferred_foreign.csv.gzip . --progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f14df2-c65c-40b3-93d8-47e147d74d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfname = \"flat_trees_inferred_foreign.csv.gzip\"\\ndf = pd.read_csv(fname, compression=\"gzip\", dtype={\"tweet_id\":str})\\ntweets = pd.merge(tweets, df, how=\"left\", left_on=\"tweet_id\", right_on=\"tweet_id\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "fname = \"flat_trees_inferred_foreign.csv.gzip\"\n",
    "df = pd.read_csv(Path(src, \"inference\", fname), compression=\"gzip\", dtype={\"tweet_id\":str})\n",
    "tweets = pd.merge(tweets, df, how=\"left\", left_on=\"tweet_id\", right_on=\"tweet_id\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa2763-7731-4a33-b09b-f33701f2844c",
   "metadata": {},
   "source": [
    "# Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b3e1101-2b96-45e4-a892-81c3f890ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/inferred_strategy_condensed_flat_trees.csv ../../../data/inference --progress\n",
    "#! xz inferred_strategy_condensed_flat_trees.csv --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2faec1ab-feeb-4831-b418-1eb6aa1ca06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_strategy_condensed_flat_trees.csv.xz\"\n",
    "cols = [\"tweet_id\", \"strategy\", \"construct\", \"opin\", \"sarc\",\n",
    "        \"leave_fact\", \"other_new\"]\n",
    "df = pd.read_csv(\n",
    "    Path(src, \"inference\", fname), \n",
    "    compression=\"xz\", \n",
    "    delimiter=\";\",\n",
    "    usecols=cols,\n",
    "    dtype={\n",
    "        \"tweet_id\":str, \n",
    "        \"strategy\":int, \n",
    "        \"construct\":float,\n",
    "        \"opin\":float, \n",
    "        \"sarc\":float, \n",
    "        \"leave_fact\":float,\n",
    "        \"other_new\":float\n",
    "    }\n",
    ")\n",
    "strategy_dict = {\n",
    "    0:\"construct\",\n",
    "    1:\"opin\",\n",
    "    2:\"sarc\",\n",
    "    3:\"leave_fact\",\n",
    "    4:\"other_new\",\n",
    "}\n",
    "df[\"strategy\"] = df[\"strategy\"].replace(strategy_dict)\n",
    "df = df.rename(columns={\n",
    "    \"construct\":\"strategy_construct\",\n",
    "    \"opin\":\"strategy_opin\",\n",
    "    \"sarc\":\"strategy_sarc\",\n",
    "    \"leave_fact\":\"strategy_leave_fact\",\n",
    "    \"other_new\":\"strategy_other_new\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "545513fc-9c90-4be0-aedc-5638cd3cc9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opin          513078\n",
       "leave_fact    334934\n",
       "construct     248954\n",
       "other_new     197881\n",
       "sarc           17058\n",
       "Name: strategy, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"strategy\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f261d64-674f-4397-a788-34292bc1a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"strategy\", \"strategy_construct\", \"strategy_opin\", \n",
    "        \"strategy_sarc\", \"strategy_leave_fact\", \"strategy_other_new\"]\n",
    "tweets = pd.merge(\n",
    "    tweets, \n",
    "    df[cols], \n",
    "    how=\"left\", \n",
    "    left_on=\"tweet_id\", \n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0920f-b980-444d-908f-09eb519238b3",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ba7c3bf-ba06-4bec-aea7-f1c4efbf437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 infer_group.py ../best_models/model-twitter-xlm-roberta-base_germanhass_epochs-100_batchsize-64_data-confident_examples_group_aug-trans-inferred3_halfcondensed_split-4 /home/jlasse/counterspeech-strategies/data/inference/flat_trees.csv.gzip 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81f3a8-d54f-445d-a2ee-43ad586b4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/inferred_group_halfcondensed_flat_trees.csv ../../../data/inference/ --progress\n",
    "#! xz inferred_group_halfcondensed_flat_trees.csv --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dcdbf2-5b31-487f-b114-0e67a0e77d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_group_halfcondensed_flat_trees.csv.xz\"\n",
    "df = pd.read_csv(\n",
    "    Path(src, \"inference\", fname), \n",
    "    compression=\"xz\",\n",
    "    delimiter=\";\",\n",
    "    dtype={\n",
    "        \"tweet_id\":str, \n",
    "        \"group\":int, \n",
    "        \"in_both\":float, \n",
    "        \"out\":float,\n",
    "        \"neutral_unint\":float\n",
    "    }\n",
    ")\n",
    "df = df.rename(columns={\n",
    "    \"in_both\":\"group_in_both\",\n",
    "    \"out\":\"group_out\",\n",
    "    \"neutral_unint\":\"group_neutral_unint\"\n",
    "})\n",
    "\n",
    "group_dict = {\n",
    "    0:\"in_both\",\n",
    "    1:\"out\",\n",
    "    2:\"neutral_unint\"\n",
    "}\n",
    "df[\"group\"] = df[\"group\"].replace(group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caead0f0-a761-4ec0-bb3c-15801636d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"group\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2528c16c-e751-4cca-987b-77b5182f5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"group\", \n",
    "        \"group_in_both\", \"group_out\", \"group_neutral_unint\"]\n",
    "tweets = pd.merge(\n",
    "    tweets, \n",
    "    df[cols], \n",
    "    how=\"left\", \n",
    "    left_on=\"tweet_id\", \n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e1fe2-7356-483f-befd-0e194ddaaa82",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21a88c66-8aa9-4c96-a843-16914c3bd9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 infer_goal.py ../best_models/model-twitter-xlm-roberta-base_germanhass_epochs-100_batchsize-64_data-confident_examples_goal_aug-trans-inferred2_condensed_split-4 /home/jlasse/counterspeech-strategies/data/inference/flat_trees.csv.gzip 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3be2502-502d-4d7d-9634-d66a6f56132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/inferred_goal_condensed_flat_trees.csv ../../../data/inference --progress\n",
    "#! xz inferred_goal_condensed_flat_trees.csv --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48ae9862-abd4-4f97-8a1f-d153923060be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_goal_condensed_flat_trees.csv.xz\"\n",
    "df = pd.read_csv(\n",
    "    Path(src, \"inference\", fname), \n",
    "    compression=\"xz\",\n",
    "    delimiter=\";\",\n",
    "    dtype={\n",
    "        \"tweet_id\":str, \n",
    "        \"goal\":int, \n",
    "        \"in_both_positive\":float, \n",
    "        \"out_negative\":float,\n",
    "        \"neutral_unint\":float\n",
    "    }\n",
    ")\n",
    "# old inference script hat the wrong labels ...\n",
    "df.columns = [\"tweet_id\", \"text\", \"goal\", \"in_both_positive\", \"out_negative\", \"neutral_unint\"]\n",
    "df = df.rename(columns={\n",
    "    \"in_both_positive\":\"goal_in_both_positive\",\n",
    "    \"out_negative\":\"goal_out_negative\",\n",
    "    \"neutral_unint\":\"goal_neutral_unint\"\n",
    "})\n",
    "\n",
    "goal_dict = {\n",
    "    0:\"in_both_positive\",\n",
    "    1:\"out_negative\",\n",
    "    2:\"neutral_unint\"\n",
    "}\n",
    "df[\"goal\"] = df[\"goal\"].replace(goal_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ca5b4f1-c12b-4089-91cb-72ff31d6d179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral_unint       608085\n",
       "out_negative        510024\n",
       "in_both_positive    193796\n",
       "Name: goal2, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"goal\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e929e093-96ec-4012-b475-61079ce6b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"goal\", \n",
    "        \"goal_in_both_positive\", \"goal_out_negative\", \"goal_neutral_unint\"]\n",
    "tweets = pd.merge(\n",
    "    tweets, \n",
    "    df[cols],\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98402a-76a7-4fe9-a12b-643b9a9c8faa",
   "metadata": {},
   "source": [
    "# Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7eacc46-159e-4028-83be-fca8edcbcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/inferred_hate_condensed_flat_trees.csv ../../../data/inference/ --progress\n",
    "#! xz inferred_hate_condensed_flat_trees.csv --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5eb9c73-9316-4f9b-9671-126e33c2b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_hate_condensed_flat_trees.csv.xz\"\n",
    "df = pd.read_csv(\n",
    "    Path(src, \"inference\", fname), \n",
    "    compression=\"xz\",\n",
    "    delimiter=\";\",\n",
    "    dtype={\n",
    "        \"tweet_id\":str,\n",
    "        \"hate\":int,\n",
    "        \"yes\":float,\n",
    "        \"no\":float,\n",
    "    }\n",
    ")\n",
    "hate_dict = {\n",
    "    0:\"yes\",\n",
    "    1:\"no\",\n",
    "}\n",
    "df[\"hate\"] = df[\"hate\"].replace(hate_dict)\n",
    "df = df.rename(columns={\n",
    "    \"yes\":\"hate_yes\",\n",
    "    \"no\":\"hate_no\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718103ff-b5c5-4514-8b6e-6631f262a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hate\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08aa9035-7095-4ada-b123-1357698eab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"hate\", \"hate_yes\", \"hate_no\"]\n",
    "tweets = pd.merge(\n",
    "    tweets, \n",
    "    df[cols],\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f61583-b45e-4f76-81a4-fab65154a009",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc01cf18-8000-4b87-8635-c9fc2ed40181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/inferred_target_condensed_flat_trees.csv ../../../data/inference/ --progress\n",
    "#! xz inferred_target_condensed_flat_trees.csv --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0bc769f-48fd-4d68-a035-df19d8694814",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_target_condensed_flat_trees.csv.xz\"\n",
    "df = pd.read_csv(\n",
    "    Path(src, \"inference\", fname), \n",
    "    compression=\"xz\",\n",
    "    delimiter=\";\",\n",
    "    dtype={\n",
    "        \"tweet_id\":str, \n",
    "        \"target\":int,\n",
    "        \"inst\":float,\n",
    "        \"right-wing\":float,\n",
    "        \"left-wing\":float,\n",
    "        \"vulnerable\":float,\n",
    "        \"other_new\":float\n",
    "    }\n",
    ")\n",
    "target_dict = {\n",
    "    0:\"inst\",\n",
    "    1:\"right-wing\",\n",
    "    2:\"left-wing\",\n",
    "    3:\"vulnerable\",\n",
    "    4:\"other_new\"\n",
    "}\n",
    "df[\"target\"] = df[\"target\"].replace(target_dict)\n",
    "df = df.rename(columns={\n",
    "    \"inst\":\"target_inst\",\n",
    "    \"right-wing\":\"target_right-wing\",\n",
    "    \"left-wing\":\"target_left-wing\",\n",
    "    \"vulnerable\":\"target_vulnerable\",\n",
    "    \"other_new\":\"target_other_new\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2072c0-7f10-4121-bbf8-06658faca4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6594923e-5969-4797-be2b-bfb027014794",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"target\", \"target_inst\", \n",
    "        \"target_right-wing\", \"target_left-wing\",\n",
    "        \"target_vulnerable\", \"target_other_new\"]\n",
    "tweets = pd.merge(\n",
    "    tweets, \n",
    "    df[cols], \n",
    "    how=\"left\", \n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da757a59-24e5-4ebf-8076-b898e822c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target is only defined for tweets where hate == \"yes\"\n",
    "idx = tweets[tweets[\"hate\"] == \"no\"].index\n",
    "tweets.loc[idx, \"target\"] = np.nan\n",
    "tweets.loc[idx, \"target_inst\"] = np.nan\n",
    "tweets.loc[idx, \"target_right-wing\"] = np.nan\n",
    "tweets.loc[idx, \"target_left-wing\"] = np.nan\n",
    "tweets.loc[idx, \"target_vulnerable\"] = np.nan\n",
    "tweets.loc[idx, \"target_other_new\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820a5e4-df1e-4972-9d1a-8abc7ec40a73",
   "metadata": {},
   "source": [
    "# Add human labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45532a-9659-41b5-afed-3b066e81f730",
   "metadata": {},
   "source": [
    "## Only confident labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7805b1aa-6f2e-4c10-aaba-d40022ec4e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_10_EM.csv     batch_2_AS.csv     batch_6_AH.csv\n",
      "batch_10_EM_LT.csv  batch_2_EM.csv     batch_6_AH_LT.csv\n",
      "batch_10_LT.csv     batch_2_LT.csv     batch_6_AS_AH.csv\n",
      "batch_10_LT_EM.csv  batch_3_AH.csv     batch_6_AS.csv\n",
      "batch_11_EM.csv     batch_3_AH_LT.csv  batch_6_LT_AH.csv\n",
      "batch_11_EM_LT.csv  batch_3_AS_AH.csv  batch_6_LT.csv\n",
      "batch_11_LT.csv     batch_3_AS.csv     batch_7a_LT_AH.csv\n",
      "batch_11_LT_EM.csv  batch_3_LT_AH.csv  batch_7a_LT.csv\n",
      "batch_12_EM.csv     batch_3_LT.csv     batch_7b_LT_AH.csv\n",
      "batch_12_EM_LT.csv  batch_4_AH_AS.csv  batch_7b_LT.csv\n",
      "batch_12_LT.csv     batch_4_AH.csv     batch_8_EM.csv\n",
      "batch_12_LT_EM.csv  batch_4_AS_AH.csv  batch_8_EM_LT.csv\n",
      "batch_13_EM.csv     batch_4_AS.csv     batch_8_LT.csv\n",
      "batch_13_EM_LT.csv  batch_4_LT_AH.csv  batch_8_LT_EM.csv\n",
      "batch_13_LT.csv     batch_4_LT.csv     batch_9_EM.csv\n",
      "batch_13_LT_EM.csv  batch_5_AH.csv     batch_9_EM_LT.csv\n",
      "batch_14_EM_AH.csv  batch_5_AH_LT.csv  batch_9_LT.csv\n",
      "batch_14_EM.csv     batch_5_AS_AH.csv  batch_9_LT_EM.csv\n",
      "batch_1_AS.csv\t    batch_5_AS.csv     goal2_validation_samples_AH.csv\n",
      "batch_1_LT.csv\t    batch_5_LT_AH.csv  goal2_validation_samples_JL.csv\n",
      "batch_2_AH.csv\t    batch_5_LT.csv     goal_minority_examples_AH.csv\n"
     ]
    }
   ],
   "source": [
    "# data sets with manually re-assigned tweet IDs. The original labelled data sets\n",
    "# are located in labelled_samples/\n",
    "! ls ../../../data/labelled_samples_with_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93224bb3-61e9-4fd0-b6c9-9d2d5ed2b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pairs = [\n",
    "    (\"batch_1_AS\", \"batch_1_LT\"),\n",
    "    (\"batch_3_AH\", \"batch_3_AH_LT\"),\n",
    "    (\"batch_3_AS\", \"batch_3_AS_AH\"),\n",
    "    (\"batch_3_LT\", \"batch_3_LT_AH\"),\n",
    "    (\"batch_4_AH\", \"batch_4_AH_AS\"),\n",
    "    (\"batch_4_AS\", \"batch_4_AS_AH\"),\n",
    "    (\"batch_4_LT\", \"batch_4_LT_AH\"),\n",
    "    (\"batch_5_LT\", \"batch_5_LT_AH\"),\n",
    "    (\"batch_5_AS\", \"batch_5_AS_AH\"),\n",
    "    (\"batch_5_AH\", \"batch_5_AH_LT\"),\n",
    "    (\"batch_6_AH\", \"batch_6_AH_LT\"),\n",
    "    (\"batch_6_AS\", \"batch_6_AS_AH\"),\n",
    "    (\"batch_6_LT\", \"batch_6_LT_AH\"),\n",
    "    (\"batch_7a_LT\", \"batch_7a_LT_AH\"),\n",
    "    (\"batch_7b_LT\", \"batch_7b_LT_AH\"),\n",
    "    (\"batch_8_LT\", \"batch_8_LT_EM\"),\n",
    "    (\"batch_8_EM\", \"batch_8_EM_LT\"),\n",
    "    (\"batch_9_LT\", \"batch_9_LT_EM\"),\n",
    "    (\"batch_9_EM\", \"batch_9_EM_LT\"),\n",
    "    (\"batch_10_LT\", \"batch_10_LT_EM\"),\n",
    "    (\"batch_10_EM\", \"batch_10_EM_LT\"),\n",
    "    (\"batch_11_LT\", \"batch_11_LT_EM\"),\n",
    "    (\"batch_11_EM\", \"batch_11_EM_LT\"),\n",
    "    (\"batch_12_LT\", \"batch_12_LT_EM\"),\n",
    "    (\"batch_12_EM\", \"batch_12_EM_LT\"),\n",
    "    (\"batch_13_LT\", \"batch_13_LT_EM\"),\n",
    "    (\"batch_13_EM\", \"batch_13_EM_LT\"),\n",
    "    (\"batch_14_EM\", \"batch_14_EM_AH\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fbb2029-7dee-4761-b2b3-3814274fa9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_group_values(df):    \n",
    "    # replace missing \"neutral\" and \"unint\" entries in the [GOAL]\n",
    "    # category with the corresponding entries in the [GROUP] category\n",
    "    df[\"[GROUP]\"] = df[\"[GROUP]\"]\\\n",
    "        .apply(lambda x: x if x in [\"neutral\", \"unint\"] else np.nan)\n",
    "    df.loc[df[df[\"[GOAL]\"].isna()].index, \"[GOAL]\"] = \\\n",
    "        df.loc[df[df[\"[GOAL]\"].isna()].index, \"[GROUP]\"].values\n",
    "    df = df.dropna(subset=[\"[GOAL]\"])\n",
    "    df = df.drop(columns=[\"[GROUP]\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2bb876d-35a2-49df-a468-a9d95392c94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_goal_labels(confident_examples):\n",
    "    # add additional minority class labels drawn from data sets with only a single\n",
    "    # label to be labelled with a second label\n",
    "    src = \"../../../data/labelled_samples_with_ids\"\n",
    "    fname = \"goal_minority_examples_AH.csv\"\n",
    "    df2 = pd.read_csv(\n",
    "        join(src, fname), \n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\"\n",
    "    ).dropna()\n",
    "    df2 = df2.drop(columns=\"[GROUP]\")\n",
    "    df2.columns = [\"tweet_id\", \"text\", \"label_2\"]\n",
    "    #df2[\"label_2\"] = df2[\"label_2\"].replace(label_to_condensed_id)\n",
    "\n",
    "    # load all data with only a single label\n",
    "    src = \"../../../data/labelled_samples_with_ids\"\n",
    "    #dimension = \"[GOAL]\"\n",
    "    #fname = \"confident_examples_goal{}\"\\\n",
    "    #    .format(condensation_dataset_names[label_condensation])\n",
    "    cols = [\"tweet_id\", \"text\", \"[GOAL]\", \"[GROUP]\"]\n",
    "    df1 = pd.DataFrame()\n",
    "    for pair in label_pairs:\n",
    "        tmp = pd.read_csv(\n",
    "            join(src, pair[0] + \".csv\"),\n",
    "            dtype={\"tweet_id\":str},\n",
    "            delimiter=\";\",\n",
    "            usecols=cols\n",
    "        )\n",
    "        tmp = add_group_values(tmp).rename(columns={\"[GOAL]\":\"label_1\"})\n",
    "\n",
    "        tmp = tmp[tmp[\"label_1\"] != \"foreign\"]\n",
    "        #tmp[\"label_1\"] = tmp[\"label_1\"].replace(label_to_condensed_id)\n",
    "        df1 = pd.concat([df1, tmp])\n",
    "\n",
    "    # create a subset of examples that now has two labels and look for confident\n",
    "    # examples where both labels agree\n",
    "    shared_ids = df1[df1[\"tweet_id\"].isin(df2[\"tweet_id\"])][\"tweet_id\"].values\n",
    "    df1 = df1[df1[\"tweet_id\"].isin(shared_ids)]\n",
    "    df2 = df2[df2[\"tweet_id\"].isin(shared_ids)]\n",
    "    df1 = df1.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    df2 = df2.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    df = pd.concat([df1, df2[[\"label_2\"]]], axis=1)[[\"tweet_id\", \"text\", \"label_1\", \"label_2\"]]\n",
    "    df = df[df[\"label_1\"] == df[\"label_2\"]]\n",
    "    df = df.drop(columns=[\"label_2\"]).rename(columns={\"label_1\":\"label\"})\n",
    "\n",
    "    # add the new confident examples to the existing ones\n",
    "    confident_examples = pd.concat([confident_examples, df])\n",
    "    confident_examples = confident_examples.reset_index(drop=True)\n",
    "    #confident_examples[\"label\"] = confident_examples[\"label\"].astype(int)\n",
    "    \n",
    "    return confident_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8c41274-c99f-42aa-99d2-9562d2c2a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confident_examples_traindata(dimension, pair):\n",
    "    src = \"../../../data/labelled_samples_with_ids\"\n",
    "    cols = [\"tweet_id\", dimension]\n",
    "    if dimension == \"[GOAL]\":\n",
    "        cols = [\"tweet_id\", dimension, \"[GROUP]\"]\n",
    "    df1 = pd.read_csv(\n",
    "        join(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    )\n",
    "    df2 = pd.read_csv(\n",
    "        join(src, pair[1] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    )\n",
    "    \n",
    "    if dimension == \"[GOAL]\":\n",
    "        df1 = add_group_values(df1).rename(columns={dimension:\"label_1\"})\n",
    "        df2 = add_group_values(df2).rename(columns={dimension:\"label_2\"})\n",
    "    else:\n",
    "        df1 = df1.rename(columns={dimension:\"label_1\"})\n",
    "        df2 = df2.rename(columns={dimension:\"label_2\"})\n",
    "\n",
    "    df1 = df1[df1[\"label_1\"] != \"foreign\"]\n",
    "    df2 = df2[df2[\"label_2\"] != \"foreign\"]\n",
    "\n",
    "    shared_ids = df1[df1[\"tweet_id\"].isin(df2[\"tweet_id\"])][\"tweet_id\"].values\n",
    "    df1 = df1[df1[\"tweet_id\"].isin(shared_ids)]\n",
    "    df2 = df2[df2[\"tweet_id\"].isin(shared_ids)]\n",
    "    df1 = df1.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    df2 = df2.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "\n",
    "    df = pd.concat([df1, df2[[\"label_2\"]]], axis=1)[[\"tweet_id\", \"label_1\", \"label_2\"]]\n",
    "    df = df[df[\"label_1\"] == df[\"label_2\"]]\n",
    "    df = df.drop(columns=[\"label_2\"]).rename(columns={\"label_1\":\"label\"})\n",
    "    df = df[df[\"label\"] != \"foreign\"]\n",
    "    \n",
    "    if dimension == \"[GOAL]\":\n",
    "        df = get_additional_goal_labels(df)\n",
    "    return df[[\"tweet_id\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cab4d55-85ac-4c50-bcad-2b5f77746ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence(row):\n",
    "    '''Determine the rater agreement across four ratings'''\n",
    "    labels = list(row[[f\"label_{i}\" for i in range(1, 5)]].values)\n",
    "    if len(set(labels)) == 1:\n",
    "        return \"unanimous\"\n",
    "    elif len(set(labels)) == 3:\n",
    "        return \"weak-majority\"\n",
    "    elif len(set(labels)) == 4:\n",
    "        return \"disagreement\"\n",
    "    elif len(set(labels)) == 2 and labels.count(labels[0]) == 2:\n",
    "        return \"split\"\n",
    "    else:\n",
    "        return \"majority\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c392836-e50f-4885-8227-3e5c7ebb2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_label(row, weak_majority=True):\n",
    "    '''Determine the majority label across four ratings'''\n",
    "    labels = list(row[[f\"label_{i}\" for i in range(1, 5)]].values)\n",
    "    # unanimous agreement\n",
    "    if len(set(labels)) == 1:\n",
    "        return labels[0]\n",
    "    # two raters have the same label, the others are different\n",
    "    elif len(set(labels)) == 3:\n",
    "        # if a weak majority is taken as agreement, return the majority label\n",
    "        if weak_majority:\n",
    "            return row[[f\"label_{i}\" for i in range(1, 5)]].value_counts().index[0]\n",
    "        # if weak majority is taken as disagreement, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "    # complete disagreement\n",
    "    elif len(set(labels)) == 4:\n",
    "        return np.nan\n",
    "    # split decision\n",
    "    elif len(set(labels)) == 2 and labels.count(labels[0]) == 2:\n",
    "        return np.nan\n",
    "    # strong majority agreement\n",
    "    else:\n",
    "        return row[[f\"label_{i}\" for i in range(1, 5)]].value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6cbda14e-bb71-47bf-b1c8-3dd332c55a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confident_examples_testdata(dimension):\n",
    "    src = \"../../../data/labelled_samples_with_ids\"\n",
    "    raters = [\"AH\", \"AS\", \"EM\", \"LT\"]\n",
    "    labels = {}\n",
    "    for i, rater in enumerate(raters):\n",
    "        tmp = pd.read_csv(\n",
    "            join(src, f\"batch_2_{rater}.csv\"),\n",
    "            dtype={\"tweet_id\":str},\n",
    "            delimiter=\";\",\n",
    "            usecols=[\"tweet_id\", \"text\", dimension]\n",
    "        )\n",
    "        tmp = tmp.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "        tmp = tmp.rename(columns={dimension:f\"label_{i+1}\"})\n",
    "        labels[rater] = tmp\n",
    "\n",
    "    df = labels[raters[0]]\n",
    "    for i, rater in enumerate(raters[1:]):\n",
    "        df[f\"label_{i+2}\"] = labels[rater][f\"label_{i+2}\"]\n",
    "    df = df[[\"tweet_id\", \"text\"] + [f\"label_{i}\" for i in range(1, 5)]].copy()\n",
    "    \n",
    "    df[\"confidence\"] = df.apply(calculate_confidence, axis=1)\n",
    "    df[\"label\"] = df.apply(get_confidence_label, weak_majority=False, axis=1)\n",
    "    df = df.dropna(subset=[\"label\"])\n",
    "    df = df[df[\"label\"] != \"foreign\"]\n",
    "    \n",
    "    return df[[\"tweet_id\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6294e013-b510-4d88-a133-708ec56166ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimensions = [\"[STRAGETY]\", \"[GROUP]\", \"[GOAL]\", \n",
    "              \"[TARGET]\", \"[HATE]\"]\n",
    "dimension_map = {\n",
    "    \"[STRATEGY]\":\"strategy\",\n",
    "    \"[GROUP]\":\"group\",\n",
    "    \"[GOAL]\":\"goal\",\n",
    "    \"[HATE]\":\"hate\",\n",
    "    \"[TARGET]\":\"target\"\n",
    "}\n",
    "\n",
    "for dimension in dimensions:\n",
    "    confident_examples = pd.DataFrame()\n",
    "    for pair in label_pairs:\n",
    "        # get confident examples from traindata\n",
    "        confident_examples = pd.concat([\n",
    "            confident_examples, \n",
    "            get_confident_examples_traindata(dimension, pair)\n",
    "        ])\n",
    "    # get confident examples from testdata\n",
    "    confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        get_confident_examples_testdata(dimension)\n",
    "    ])\n",
    "    \n",
    "    confident_examples = confident_examples.reset_index(drop=True)\n",
    "    confident_examples = confident_examples.\\\n",
    "        rename(columns={\"label\":\"{}_human_label_confident\"\\\n",
    "                        .format(dimension_map[dimension])})\n",
    "    confident_examples = confident_examples.drop_duplicates()\n",
    "    \n",
    "    assert len(confident_examples) == len(confident_examples[\"tweet_id\"].unique())\n",
    "    \n",
    "    tweets = pd.merge(\n",
    "        tweets, \n",
    "        confident_examples,\n",
    "        how=\"left\",\n",
    "        left_on=\"tweet_id\",\n",
    "        right_on=\"tweet_id\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4e05e-63cf-48e0-b11e-f46fd35bc97d",
   "metadata": {},
   "source": [
    "## All labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c56a3c55-9e2c-49b2-80a9-859490d04332",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../data/labelled_samples_with_ids\"\n",
    "raters = {\n",
    "    \"1\":[\"LT\"],\n",
    "    \"2\":[\"LT\"],\n",
    "    \"3\":[\"AH\", \"AS\", \"LT\"],\n",
    "    \"4\":[\"AH\", \"AS\", \"LT\"],\n",
    "    \"5\":[\"AH\", \"AS\", \"LT\"],\n",
    "    \"6\":[\"AH\", \"AS\", \"LT\"],\n",
    "    \"7a\":[\"LT\"],\n",
    "    \"7b\":[\"LT\"],\n",
    "    \"8\":[\"EM\", \"LT\"],\n",
    "    \"9\":[\"EM\", \"LT\"],\n",
    "    \"10\":[\"EM\", \"LT\"],\n",
    "    \"11\":[\"EM\", \"LT\"],\n",
    "    \"12\":[\"EM\", \"LT\"],\n",
    "    \"13\":[\"EM\", \"LT\"],\n",
    "    \"14\":[\"EM\"]\n",
    "}\n",
    "\n",
    "human_labels = pd.DataFrame()\n",
    "for batch in raters.keys():#range(1, N_batches + 1):\n",
    "    labelled = pd.concat([pd.read_csv(\n",
    "        join(src, f\"batch_{batch}_{rater}.csv\"),\n",
    "        delimiter=\";\", dtype={\"tweet_id\":str}\n",
    "    ) for rater in raters[batch]])\n",
    "\n",
    "    if batch in [\"7a\", \"7b\"]: batch = \"7\"\n",
    "    batch = int(batch)\n",
    "    labelled[\"labelling_batch\"] = batch\n",
    "    labelled = labelled.rename(columns={\n",
    "        \"[STRAGEGY]\":\"strategy_human_label\",\n",
    "        \"[GROUP]\":\"group_human_label\",\n",
    "        \"[GOAL]\":\"goal_human_label\",\n",
    "        \"[HATE]\":\"hate_human_label\",\n",
    "        \"[TARGET]\":\"target_human_label\"\n",
    "    })\n",
    "    \n",
    "    human_labels = pd.concat([human_labels, labelled])\n",
    "human_labels = human_labels.drop_duplicates(subset=[\"tweet_id\"])\n",
    "assert len(human_labels) == len(human_labels[\"tweet_id\"].unique())\n",
    "human_labels = human_labels.drop(columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74cf0602-024d-4353-90ee-f526e27a4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.merge(\n",
    "    tweets, \n",
    "    human_labels, \n",
    "    how=\"left\", \n",
    "    left_on=\"tweet_id\", \n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "686bf310-1440-4eb5-b11a-336f62348980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0     1536\n",
       "5.0     1520\n",
       "4.0     1508\n",
       "6.0     1393\n",
       "1.0     1108\n",
       "2.0     1070\n",
       "8.0     1014\n",
       "12.0    1012\n",
       "13.0    1009\n",
       "10.0    1008\n",
       "9.0     1006\n",
       "11.0    1005\n",
       "7.0     1004\n",
       "14.0     501\n",
       "Name: labelling_batch, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"labelling_batch\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80de2a2-d1f2-4057-9819-dfa3dfbfe77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817696e-5986-42e8-ac08-749d1b54061e",
   "metadata": {},
   "source": [
    "# Export validation data for the goal dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865d342-94aa-4d62-b1d7-86d4be033d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_candidates = tweets[(tweets[\"goal_human_label\"].isna()) &\\\n",
    "                           (tweets[\"goal_human_label_confident\"].isna())]\n",
    "export_candidates[\"goal\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76c43a53-e911-4ec6-b303-f64b38910519",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_in_both_positive = export_candidates[export_candidates[\"goal\"] == \"in_both_positive\"]\\\n",
    "    .sample(n=67, random_state=42)\n",
    "export_out_negative = export_candidates[export_candidates[\"goal\"] == \"out_negative\"]\\\n",
    "    .sample(n=67, random_state=42)\n",
    "export_neutral_unint = export_candidates[export_candidates[\"goal\"] == \"neutral_unint\"]\\\n",
    "    .sample(n=67, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "067bf259-6c94-4fff-bd17-39a49e32214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../data/additional_samples\"\n",
    "fname = \"goal_validation_samples_with_inferred_label.csv\"\n",
    "export = pd.concat([\n",
    "    export_in_both_positive,\n",
    "    export_out_negative,\n",
    "    export_neutral_unint\n",
    "]).sample(frac=1, random_state=42)\n",
    "export[[\"tweet_id\", \"text\", \"goal\"]].to_csv(join(src, fname), index=False)\n",
    "fname = \"goal_validation_samples.csv\"\n",
    "export[[\"tweet_id\", \"text\"]].to_csv(join(src, fname), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450aaee-9100-47ce-ba1e-5b9ff993e484",
   "metadata": {},
   "source": [
    "# Export inferred data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d365400-334f-4fda-85fb-1fa473f40be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opin            4646\n",
       "unint           1767\n",
       "sarc            1750\n",
       "insult-inst     1299\n",
       "insult-pers     1144\n",
       "insult-polit     839\n",
       "insult-ism       835\n",
       "inconsist        772\n",
       "info             685\n",
       "other            622\n",
       "conseq           385\n",
       "foreign          348\n",
       "quest            312\n",
       "correct          288\n",
       "Name: strategy_human_label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"strategy_human_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd218c9-e3da-4e30-b387-eddfa40c7744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15692"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"strategy_human_label\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "647943ef-7ba7-4a76-b3fb-68ddb38aa055",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_data.csv.gzip\"\n",
    "tweets.drop(columns=[\"text\"]).to_csv(\n",
    "    fname, \n",
    "    compression=\"gzip\", \n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
