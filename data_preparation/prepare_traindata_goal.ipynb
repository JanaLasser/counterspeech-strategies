{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ef6b7-50fa-4d94-a7ec-ac196c786ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import data_preparation_resources as dpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2752f2-649d-46d9-8410-6040bc82e525",
   "metadata": {},
   "source": [
    "# Label condensation stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e9ce6-e247-427e-a96d-daadb759b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../../data/traindata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea7fea7-e7f4-4f59-b356-9e7b69a26378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be \"none\", \"medium\" and \"full\"\n",
    "label_condensation = \"full2\"\n",
    "condensation_dataset_names = {\n",
    "    \"none\":\"\",\n",
    "    \"medium\":\"_halfcondensed\",\n",
    "    \"full\":\"_condensed\",\n",
    "    \"full2\":\"_condensed2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb3f77-3e47-42b5-935c-cfd3de823c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label dictionary after removing \"foreign\"\n",
    "label_to_id = {\n",
    "    \"strength\":0,\n",
    "    \"just\":1,\n",
    "    \"threat\":2,\n",
    "    \"weak\":3,\n",
    "    \"emph-ground\":4,\n",
    "    \"emph-prob\":5,\n",
    "    \"neutral\":6,\n",
    "    \"unint\":7\n",
    "    }\n",
    "\n",
    "# condensed label dictionaries\n",
    "if label_condensation == \"none\":\n",
    "    label_to_condensed_id = label_to_id\n",
    "    condensed_id_to_label = {val:key for key, val in label_to_id.items()}\n",
    "    id_to_condensed_id = {val:val for val in label_to_id.values()}\n",
    "elif label_condensation == \"medium\":\n",
    "    label_to_condensed_id = {\n",
    "         \"strength\": 0, # pose\n",
    "         \"just\": 0, \n",
    "         \"threat\": 1,\n",
    "         \"weak\": 2,\n",
    "         \"emph-ground\": 3, # emph\n",
    "         \"emph-prob\": 3,\n",
    "         \"neutral\": 4,\n",
    "         \"unint\": 5\n",
    "    }\n",
    "\n",
    "    condensed_id_to_label = {\n",
    "        0:\"pose\",\n",
    "        1:\"threat\",\n",
    "        2:\"weak\",\n",
    "        3:\"emph\",\n",
    "        4:\"neutral\",\n",
    "        5:\"unint\"\n",
    "    }\n",
    "\n",
    "    id_to_condensed_id = {\n",
    "         0: 0,\n",
    "         1: 0,\n",
    "         2: 1,\n",
    "         3: 2,\n",
    "         4: 3,\n",
    "         5: 3,\n",
    "         6: 4,\n",
    "         7: 5\n",
    "    }\n",
    "elif label_condensation == \"full\":\n",
    "    label_to_condensed_id = {\n",
    "         \"strength\": 2, # pose\n",
    "         \"just\": 2, \n",
    "         \"threat\": 2,\n",
    "         \"weak\": 0,\n",
    "         \"emph-ground\": 2, # emph\n",
    "         \"emph-prob\": 2,\n",
    "         \"neutral\": 1,\n",
    "         \"unint\": 0\n",
    "    }\n",
    "\n",
    "    condensed_id_to_label = {\n",
    "        0:\"weak\",\n",
    "        1:\"neutral\",\n",
    "        2:\"other\",\n",
    "    }\n",
    "\n",
    "    id_to_condensed_id = {\n",
    "         0: 2,\n",
    "         1: 2,\n",
    "         2: 2,\n",
    "         3: 0,\n",
    "         4: 2,\n",
    "         5: 2,\n",
    "         6: 1,\n",
    "         7: 0\n",
    "    }\n",
    "elif label_condensation == \"full2\":\n",
    "    label_to_condensed_id = {\n",
    "         \"strength\": 0, # pose\n",
    "         \"just\": 0, \n",
    "         \"threat\": 1,\n",
    "         \"weak\": 1,\n",
    "         \"emph-ground\": 0, # emph\n",
    "         \"emph-prob\": 0,\n",
    "         \"neutral\": 2,\n",
    "         \"unint\": 2\n",
    "    }\n",
    "\n",
    "    condensed_id_to_label = {\n",
    "        0:\"in_both_positive\",\n",
    "        1:\"out_negative\",\n",
    "        2:\"neutral_unint\",\n",
    "    }\n",
    "\n",
    "    id_to_condensed_id = {\n",
    "         0: 0,\n",
    "         1: 0,\n",
    "         2: 1,\n",
    "         3: 1,\n",
    "         4: 0,\n",
    "         5: 0,\n",
    "         6: 2,\n",
    "         7: 2\n",
    "    }\n",
    "else:\n",
    "    print(\"unknown condensation level!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ad79d-c640-4169-942d-351f0e5249a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V1: dataset with confident labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872e8be-0ae5-4d75-bb5a-1252dcc30da0",
   "metadata": {},
   "source": [
    "Note: the dimension \"group\" used to be called \"topic\" before. This is the reason why some traindata has this as the column name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4260de7-51a1-4790-bae7-4c667bcfd4e6",
   "metadata": {},
   "source": [
    "## Load confident human labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcedac-5e0d-4df9-9028-a1b7b33b3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_topic_values(df):    \n",
    "    # replace missing \"neutral\" and \"unint\" entries in the [GOAL]\n",
    "    # category with the corresponding entries in the [TOPIC] category\n",
    "    df[\"[TOPIC]\"] = df[\"[TOPIC]\"]\\\n",
    "        .apply(lambda x: x if x in [\"neutral\", \"unint\"] else np.nan)\n",
    "    df.loc[df[df[\"[GOAL]\"].isna()].index, \"[GOAL]\"] = \\\n",
    "        df.loc[df[df[\"[GOAL]\"].isna()].index, \"[TOPIC]\"].values\n",
    "    df = df.dropna(subset=[\"[GOAL]\"])\n",
    "    df = df.drop(columns=[\"[TOPIC]\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8bf47-f171-41f5-afbb-ec422d342acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[GOAL]\"\n",
    "fname = \"confident_examples_goal{}\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "cols = [\"tweet_id\", \"text\", dimension, \"[TOPIC]\"]\n",
    "confident_examples = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df1 = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    )\n",
    "    df1 = add_topic_values(df1).rename(columns={dimension:\"label_1\"})\n",
    "    \n",
    "    df2 = pd.read_csv(\n",
    "        Path(src, pair[1] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    )\n",
    "    df2 = add_topic_values(df2).rename(columns={dimension:\"label_2\"})\n",
    "    \n",
    "    df1 = df1[df1[\"label_1\"] != \"foreign\"]\n",
    "    df2 = df2[df2[\"label_2\"] != \"foreign\"]\n",
    "    df1[\"label_1\"] = df1[\"label_1\"].replace(label_to_condensed_id)\n",
    "    df2[\"label_2\"] = df2[\"label_2\"].replace(label_to_condensed_id)\n",
    "    #df2 = pd.concat([df2, minority_labels])\n",
    "    \n",
    "    shared_ids = df1[df1[\"tweet_id\"].isin(df2[\"tweet_id\"])][\"tweet_id\"].values\n",
    "    df1 = df1[df1[\"tweet_id\"].isin(shared_ids)]\n",
    "    df2 = df2[df2[\"tweet_id\"].isin(shared_ids)]\n",
    "    df1 = df1.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    df2 = df2.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    \n",
    "    df = pd.concat([df1, df2[[\"label_2\"]]], axis=1)[[\"tweet_id\", \"text\", \"label_1\", \"label_2\"]]\n",
    "    df = df[df[\"label_1\"] == df[\"label_2\"]]\n",
    "    df = df.drop(columns=[\"label_2\"]).rename(columns={\"label_1\":\"label\"})\n",
    "    confident_examples = pd.concat([confident_examples, df])\n",
    "confident_examples = confident_examples.reset_index(drop=True)\n",
    "confident_examples[\"label\"] = confident_examples[\"label\"].astype(int)\n",
    "confident_examples = dpr.clean_text(confident_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64bed4-51c7-4374-a77d-37c68e383964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add additional minority class labels drawn from data sets with only a single\n",
    "# label to be labelled with a second label\n",
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "fname = \"goal_minority_examples_AH.csv\"\n",
    "df2 = pd.read_csv(\n",
    "    Path(src, fname), \n",
    "    dtype={\"tweet_id\":str},\n",
    "    delimiter=\";\"\n",
    ").dropna()\n",
    "df2 = df2.drop(columns=\"[TOPIC]\")\n",
    "df2.columns = [\"tweet_id\", \"text\", \"label_2\"]\n",
    "df2[\"label_2\"] = df2[\"label_2\"].replace(label_to_condensed_id)\n",
    "\n",
    "# load all data with only a single label\n",
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[GOAL]\"\n",
    "fname = \"confident_examples_goal{}\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "cols = [\"tweet_id\", \"text\", dimension, \"[TOPIC]\"]\n",
    "df1 = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    tmp = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    )\n",
    "    tmp = add_topic_values(tmp).rename(columns={dimension:\"label_1\"})\n",
    "    \n",
    "    tmp = tmp[tmp[\"label_1\"] != \"foreign\"]\n",
    "    tmp[\"label_1\"] = tmp[\"label_1\"].replace(label_to_condensed_id)\n",
    "    df1 = pd.concat([df1, tmp])\n",
    "    \n",
    "# create a subset of examples that now has two labels and look for confident\n",
    "# examples where both labels agree\n",
    "shared_ids = df1[df1[\"tweet_id\"].isin(df2[\"tweet_id\"])][\"tweet_id\"].values\n",
    "df1 = df1[df1[\"tweet_id\"].isin(shared_ids)]\n",
    "df2 = df2[df2[\"tweet_id\"].isin(shared_ids)]\n",
    "df1 = df1.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "df2 = df2.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "df = pd.concat([df1, df2[[\"label_2\"]]], axis=1)[[\"tweet_id\", \"text\", \"label_1\", \"label_2\"]]\n",
    "df = df[df[\"label_1\"] == df[\"label_2\"]]\n",
    "df = df.drop(columns=[\"label_2\"]).rename(columns={\"label_1\":\"label\"})\n",
    "\n",
    "# add the new confident examples to the existing ones\n",
    "confident_examples = pd.concat([confident_examples, df])\n",
    "confident_examples = confident_examples.reset_index(drop=True)\n",
    "confident_examples[\"label\"] = confident_examples[\"label\"].astype(int)\n",
    "confident_examples = dpr.clean_text(confident_examples)\n",
    "confident_examples.to_csv(Path(dst, fname + \".csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52383e79-5f20-4aa9-9d12-36a958e8dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(confident_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2448fd-3391-4085-8a4b-3a18c25acf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full\n",
    "#        0:\"weak\"\n",
    "#        1:\"neutral\"\n",
    "#        2:\"other\"\n",
    "\n",
    "# full 2\n",
    "#        0:\"in_both_positive\",\n",
    "#        1:\"out_negative\",\n",
    "#        2:\"neutral_unint\",\n",
    "\n",
    "# medium\n",
    "#        0:\"pose\",\n",
    "#        1:\"threat\",\n",
    "#        2:\"weak\",\n",
    "#        3:\"emph\",\n",
    "#        4:\"neutral\",\n",
    "#        5:\"unint\"\n",
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c94fd-4df5-407f-809d-ad904c3e06b9",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344b4e6-1408-48eb-8efb-1b35161cda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_goal{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_goal{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef1349-6846-4662-8616-3a6114c843ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_goal* jlasse@nvcluster:/home/jlasse/GermanHass/goal_analysis/data_preparation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af09771-eedf-4f62-9067-160d0670e768",
   "metadata": {},
   "source": [
    "# V2: augmented minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48062463-de0c-4fae-ae90-32a6ac3c83f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full\n",
    "#        0:\"weak\"\n",
    "#        1:\"neutral\"\n",
    "#        2:\"other\"\n",
    "\n",
    "# full 2\n",
    "#        0:\"in_both_positive\",\n",
    "#        1:\"out_negative\",\n",
    "#        2:\"neutral_unint\",\n",
    "\n",
    "# medium\n",
    "#        0:\"pose\",\n",
    "#        1:\"threat\",\n",
    "#        2:\"weak\",\n",
    "#        3:\"emph\",\n",
    "#        4:\"neutral\",\n",
    "#        5:\"unint\"\n",
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c08a17-dcfc-4429-a8f8-f52926b6652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = dpr.select_translations(confident_examples, [0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7eef1-fb5e-4c03-a8e9-cb4f410bbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30fe17-8069-4a74-9675-1896b3267225",
   "metadata": {},
   "source": [
    "## Add minority example translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b199a-7eae-4e39-9c7c-65b237e66574",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "if label_condensation == \"none\":\n",
    "    #translations = dpr.select_translations(confident_examples, [0, 2, 3])\n",
    "    #translations_in = translations[translations[\"label\"] == 0].sample(n=300, random_state=42)\n",
    "    #translations_both = translations[translations[\"label\"] == 2]\n",
    "    #translations_neutral = translations[translations[\"label\"] == 3].sample(n=300, random_state=42)\n",
    "    #new_confident_examples = pd.concat([\n",
    "    #    confident_examples, \n",
    "    #    translations_in[cols],\n",
    "    #    translations_both[cols],\n",
    "    #    translations_neutral[cols],\n",
    "    #])\n",
    "    pass\n",
    "elif label_condensation == \"medium\":\n",
    "    translations = dpr.select_translations(confident_examples, [0, 1, 3, 4, 5])\n",
    "    translations_pose = translations[translations[\"label\"] == 0]\n",
    "    translations_threat = translations[translations[\"label\"] == 1]\n",
    "    translations_emph = translations[translations[\"label\"] == 3]\n",
    "    translations_neutral = translations[translations[\"label\"] == 4]\n",
    "    translations_unint = translations[translations[\"label\"] == 5]\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        translations_pose[cols],\n",
    "        translations_threat[cols],\n",
    "        translations_emph[cols],\n",
    "        translations_neutral[cols],\n",
    "        translations_unint[cols]\n",
    "    ])\n",
    "elif label_condensation == \"full\":\n",
    "    translations = dpr.select_translations(confident_examples, [1, 2])\n",
    "    translations_neutral = translations[translations[\"label\"] == 1]\n",
    "    translations_other = translations[translations[\"label\"] == 2]\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        translations_neutral[cols],\n",
    "        translations_other[cols],\n",
    "    ])\n",
    "elif label_condensation == \"full2\":\n",
    "    translations = dpr.select_translations(confident_examples, [0, 2])\n",
    "    translations_in_both_positive = translations[translations[\"label\"] == 0]\n",
    "    translations_neutral_unint = translations[translations[\"label\"] == 2]\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        translations_in_both_positive[cols],\n",
    "        translations_neutral_unint[cols],\n",
    "    ])\n",
    "    \n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b72a6-b737-48fc-b6d2-f347e6a509a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_goal_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c8b03-b279-47f5-9e58-38e3fe361980",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213fd3e-22b6-4e72-8100-4496c30eb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_goal_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ").dropna().reset_index(drop=True)\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_goal_aug-trans{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab136f7e-1a51-4eca-9dd9-57d4f96b4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_goal_aug-trans* jlasse@nvcluster:/home/jlasse/GermanHass/goal_analysis/data_preparation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e9ea3-a092-4880-b260-71d35e26558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac89fe6-e291-4aa3-ba22-7791fae738f0",
   "metadata": {},
   "source": [
    "# Create a new minority class data set for labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df83e8d-e589-4757-997e-d1b8f7abedf4",
   "metadata": {},
   "source": [
    "**Important**: this code needs to run with `label_condensation=halfcondensed`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677958e-69c0-460d-baff-896493aac467",
   "metadata": {},
   "source": [
    "## Load remaining human annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbf921-de33-4695-91ff-5fc18828d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[GOAL]\"\n",
    "cols = [\"tweet_id\", \"text\", dimension, \"[TOPIC]\"]\n",
    "labelled = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    )\n",
    "    df = add_topic_values(df).rename(columns={dimension:\"label_1\"})\n",
    "    df = df[df[\"label_1\"] != \"foreign\"]\n",
    "    df = df.dropna(subset=[\"label_1\"])\n",
    "    df[\"label_1\"] = df[\"label_1\"].apply(lambda x: x.strip(\" \"))\n",
    "    df[\"label_1\"] = df[\"label_1\"].replace(label_to_condensed_id)\n",
    "    labelled = pd.concat([labelled, df])\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "labelled[\"label_1\"] = labelled[\"label_1\"].astype(int)\n",
    "labelled = labelled.rename(columns={\"label_1\":\"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed469b74-5cd5-4a7d-aa21-f4fb3183bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = labelled[~labelled[\"tweet_id\"].isin(confident_examples[\"tweet_id\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5dbe8b-6289-4235-a3dd-855fad1fbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remaining_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d525959-1f9d-421e-8e07-96d8c0b134f6",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7ba55-7e88-4595-951a-5b883d8e6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#        0:\"pose\", -> minority\n",
    "#        1:\"threat\", -> minority\n",
    "#        2:\"weak\",\n",
    "#        3:\"emph\", -> minority\n",
    "#        4:\"neutral\",\n",
    "#        5:\"unint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48d49b-4092-425f-b602-ea39f3bc5204",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = remaining_examples[remaining_examples[\"label\"] == 0].sample(n=300, random_state=42)\n",
    "threat = remaining_examples[remaining_examples[\"label\"] == 1].sample(n=300, random_state=42)\n",
    "emph = remaining_examples[remaining_examples[\"label\"] == 3].sample(n=300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394fa3a-3e3b-44e5-aa35-61065c6f6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_labelling = pd.concat([pose, threat, emph]).sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40feb99f-3142-4dfd-a941-6fe292512fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../../../data/additional_samples\"\n",
    "fname = \"goal_minority_examples.csv\"\n",
    "data_for_labelling[[\"tweet_id\", \"text\"]].to_csv(\n",
    "    Path(dst, fname),\n",
    "    sep=\";\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c3c6f-4851-4d27-8683-331204ac4163",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V3: human + inferred labels (round 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e818dc-a2ae-4154-945a-9343d77f035c",
   "metadata": {},
   "source": [
    "## Load existing confident examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f56f1f-3561-424f-808e-cfa559a8c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_goal_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "confident_examples = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a7720-ad05-4f94-995e-dde79fab091f",
   "metadata": {},
   "source": [
    "## Load remaining human annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd4b17-5c19-4f46-8b91-4a8110247c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[GOAL]\"\n",
    "cols = [\"tweet_id\", \"text\", dimension, \"[TOPIC]\"]\n",
    "labelled = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    )\n",
    "    df = add_topic_values(df).rename(columns={dimension:\"label_1\"})\n",
    "    df = df[df[\"label_1\"] != \"foreign\"]\n",
    "    df = df.dropna(subset=[\"label_1\"])\n",
    "    df[\"label_1\"] = df[\"label_1\"].apply(lambda x: x.strip(\" \"))\n",
    "    df[\"label_1\"] = df[\"label_1\"].replace(label_to_condensed_id)\n",
    "    labelled = pd.concat([labelled, df])\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "labelled[\"label_1\"] = labelled[\"label_1\"].astype(int)\n",
    "labelled = dpr.clean_text(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0058d-0f68-402b-b754-f1f83c0f75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = labelled[~labelled[\"tweet_id\"].isin(confident_examples[\"tweet_id\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614339e-321f-4b03-b748-2b1e96b7bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remaining_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401991e-dbe5-46b4-8a6f-5c3c38141253",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"remaining_examples{}_1.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "remaining_examples.rename(columns={\"label_1\":\"label\"}).to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81972a-d725-4446-9c20-41d5817a8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh remaining_examples_goal* jlasse@nvcluster:/home/jlasse/GermanHass/goal_analysis/data_preparation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec472e7-868a-4019-84dc-13219594b1f0",
   "metadata": {},
   "source": [
    "## Load inferred labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1db26-2cfd-417a-ac81-92d3367af782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 infer_goal.py ../best_models/model-twitter-xlm-roberta-base_germanhass_epochs-100_batchsize-64_data-confident_examples_goal_aug-trans_condensed2_split-3 ../data_preparation/remaining_examples_goal_condensed2_1.csv full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff7299-a884-4efa-ba3e-0deefc9fa864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download inferred data\n",
    "! rsync -avze ssh jlasse@nvcluster:/home/jlasse/GermanHass/goal_analysis/inference/inferred_goal* ../../../data/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e5909-8947-4e60-9bd7-655ac82c33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: manually change condensed2 -> condensed!!\n",
    "fname = \"inferred_goal_condensed_remaining_examples{}_1.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "inferred_labels = pd.read_csv(\n",
    "    Path(\"../../../data/inference\", fname),\n",
    "    delimiter=\";\",\n",
    "    dtype={\"tweet_id\":str, \"goal\":int},\n",
    "    usecols=[\"tweet_id\", \"goal\"]\n",
    ").rename(columns={\"goal\":\"label_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33b76a-7b14-4f52-b819-30bab92c1200",
   "metadata": {},
   "source": [
    "## Determine label agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b35ab4-6f50-4d86-bb8d-f8666a47ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = pd.merge(\n",
    "    remaining_examples,\n",
    "    inferred_labels,\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8ef0c-0ad7-435e-8f24-58c39187fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = remaining_examples.dropna(subset=[\"label_2\"])\n",
    "remaining_examples[\"label_2\"] = remaining_examples[\"label_2\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d716877-21de-4796-bb9a-fe5e932cea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confident_examples = remaining_examples[remaining_examples[\"label_1\"] == remaining_examples[\"label_2\"]]\n",
    "new_confident_examples = new_confident_examples[[\"tweet_id\", \"text\", \"label_1\"]].rename(columns={\"label_1\":\"label\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7b3a5-82ec-4735-bd1d-148b25bb7bd0",
   "metadata": {},
   "source": [
    "## Add augmented minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d445e088-91ca-46c3-9862-39ab5238a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcf165-da2d-4168-961a-7e679b5f1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0a30d-cc66-4288-97b1-3705e519e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "if label_condensation == \"none\":\n",
    "    #translations = dpr.select_translations(confident_examples, [0, 2])\n",
    "    #translations_in = translations[translations[\"label\"] == 0].sample(n=300, random_state=42)\n",
    "    #translations_both = translations[translations[\"label\"] == 2]\n",
    "    #new_confident_examples = pd.concat([\n",
    "    #    confident_examples, \n",
    "    #    new_confident_examples,\n",
    "    #    translations_in[cols],\n",
    "    #    translations_both[cols],\n",
    "    #])\n",
    "    pass\n",
    "elif label_condensation == \"medium\":\n",
    "    # since we have an overabundance of examples from the \"weak\" category,\n",
    "    # we subsample the new confident examples for \"weak\"\n",
    "    new_confident_examples_weak = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 2].sample(n=492+444, random_state=42)\n",
    "    new_confident_examples_rest = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] != 2]\n",
    "\n",
    "    translations = dpr.select_translations(new_confident_examples, [0, 1, 3, 4, 5])\n",
    "    translations_pose = translations[translations[\"label\"] == 0]\n",
    "    translations_threat = translations[translations[\"label\"] == 1]\n",
    "    translations_emph = translations[translations[\"label\"] == 3]\n",
    "    translations_neutral = translations[translations[\"label\"] == 4]\n",
    "    translations_unint = translations[translations[\"label\"] == 5]\n",
    "    \n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples_weak,\n",
    "        new_confident_examples_rest,\n",
    "        translations_pose[cols],\n",
    "        translations_threat[cols],\n",
    "        translations_emph[cols],\n",
    "        translations_neutral[cols],\n",
    "        translations_unint[cols],\n",
    "    ])\n",
    "elif label_condensation == \"full\":\n",
    "    new_confident_examples_other = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 0].sample(n=288, random_state=42)\n",
    "    new_confident_examples_neutral = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 1]\n",
    "    new_confident_examples_weak = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 2].sample(n=256, random_state=42)\n",
    "\n",
    "    translations_neutral = dpr.select_translations(new_confident_examples, [1])\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples_weak,\n",
    "        new_confident_examples_neutral,\n",
    "        new_confident_examples_other,\n",
    "        translations_neutral[cols],\n",
    "    ])\n",
    "elif label_condensation == \"full2\":\n",
    "    new_confident_examples_neutral_unint = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 2].sample(n=324, random_state=42)\n",
    "    new_confident_examples_out_negative = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 1].sample(n=187, random_state=42)\n",
    "    new_confident_examples_in_both_positive = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 0]\n",
    "    translations_in_both_positive = dpr.select_translations(new_confident_examples, [0])\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples_in_both_positive,\n",
    "        new_confident_examples_out_negative,\n",
    "        new_confident_examples_neutral_unint,\n",
    "        translations_in_both_positive[cols],\n",
    "    ])\n",
    "else:\n",
    "    print(\"unknown label condensation\")\n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cae93-517c-44ae-8846-55e168cd1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_goal_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2bd6fd-2e04-4d09-b709-34f2d0161e6e",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f8b94-f361-4b34-a96f-18579c6bc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_goal_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ").dropna().reset_index(drop=True)\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_goal_aug-trans-inferred{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ed731-4051-44c8-99ba-ab548d3ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_goal_aug-trans-inferred* jlasse@nvcluster:/home/jlasse/GermanHass/goal_analysis/data_preparation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45ea23-b04a-40a0-8ec8-e68331611783",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V4: human + inferred labels (round 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e600f7c-f74b-43c2-aa31-31f6e904d60d",
   "metadata": {},
   "source": [
    "## Load existing confident examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696cdc58-2424-4f77-895a-1da8812117fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_goal_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "confident_examples = pd.read_csv(\n",
    "    fname,\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426072be-a527-4fc7-b3d5-65fd0db31c8b",
   "metadata": {},
   "source": [
    "## Load remaining human annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab86bf1-341f-4544-b77a-b524c52d3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[GOAL]\"\n",
    "cols = [\"tweet_id\", \"text\", dimension, \"[TOPIC]\"]\n",
    "labelled = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    )\n",
    "    df = add_topic_values(df).rename(columns={dimension:\"label_1\"})\n",
    "    df = df[df[\"label_1\"] != \"foreign\"]\n",
    "    df = df.dropna(subset=[\"label_1\"])\n",
    "    df[\"label_1\"] = df[\"label_1\"].apply(lambda x: x.strip(\" \"))\n",
    "    df[\"label_1\"] = df[\"label_1\"].replace(label_to_condensed_id)\n",
    "    labelled = pd.concat([labelled, df])\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "labelled[\"label_1\"] = labelled[\"label_1\"].astype(int)\n",
    "labelled = dpr.clean_text(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5e432-2f8f-4373-8d8a-3d4c3369b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = labelled[~labelled[\"tweet_id\"].isin(confident_examples[\"tweet_id\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c0bf2-85a7-443d-8dc3-c59a949948b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remaining_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f90191c-3b81-4962-a067-d540b8555a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"remaining_examples_goal{}_2.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "remaining_examples.rename(columns={\"label_1\":\"label\"}).to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e158f8-321f-46d9-8d70-644b287b6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh remaining_examples_goal* jlasse@nvcluster:/home/jlasse/GermanHass/goal_analysis/data_preparation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb9ae3-8a11-4f6f-8f93-68387c452fc7",
   "metadata": {},
   "source": [
    "## Load inferred labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246e815-75bb-4dab-a397-1c49d2dc159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 infer_goal.py ../best_models/model-twitter-xlm-roberta-base_germanhass_epochs-100_batchsize-64_data-confident_examples_goal_aug-trans-inferred_condensed_split-4 ../data_preparation/remaining_examples_goal_condensed_2.csv full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23309dee-28e2-4578-8d8a-655adced2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download inferred data\n",
    "! rsync -avze ssh jlasse@nvcluster:/home/jlasse/GermanHass/goal_analysis/inference/inferred_goal* ../../../data/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7cc95c-4d2e-4745-b9d8-9388e9fd894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_goal{}_remaining_examples{}_2.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation],\n",
    "            condensation_dataset_names[label_condensation])\n",
    "inferred_labels = pd.read_csv(\n",
    "    Path(\"../../../data/inference\", fname),\n",
    "    delimiter=\";\",\n",
    "    dtype={\"tweet_id\":str, \"goal\":int},\n",
    "    usecols=[\"tweet_id\", \"goal\"]\n",
    ").rename(columns={\"goal\":\"label_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b640a6-a9d0-43e4-9f7b-c4548f5ed219",
   "metadata": {},
   "source": [
    "## Determine label agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf37c3-478b-4bd7-8eda-99edcb3439f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = pd.merge(\n",
    "    remaining_examples,\n",
    "    inferred_labels,\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024511cf-f309-446e-8aa1-99f33cd0287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = remaining_examples.dropna(subset=[\"label_2\"])\n",
    "remaining_examples[\"label_2\"] = remaining_examples[\"label_2\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30831da-24ca-486c-87da-58a137e0f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confident_examples = remaining_examples[remaining_examples[\"label_1\"] == remaining_examples[\"label_2\"]]\n",
    "new_confident_examples = new_confident_examples[[\"tweet_id\", \"text\", \"label_1\"]].rename(columns={\"label_1\":\"label\"})\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8785c8-e4b8-40a8-8420-73f9ff1beb04",
   "metadata": {},
   "source": [
    "## Add augmented minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b82307-ca29-448f-bed5-7f05539cf977",
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ac284-9032-43aa-9752-af9023fd654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = dpr.select_translations(new_confident_examples, [1])\n",
    "translations[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbb196-e101-4548-95ec-099e9ab4a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "if label_condensation == \"none\":\n",
    "    # TODO\n",
    "    pass\n",
    "elif label_condensation == \"medium\":\n",
    "    # since we have an overabundance of examples from the \"weak\" category,\n",
    "    # we subsample the new confident examples for \"weak\"\n",
    "    new_confident_examples_weak = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 2].sample(n=223+217, random_state=42)\n",
    "    new_confident_examples_rest = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] != 2]\n",
    "    \n",
    "    translations = dpr.select_translations(new_confident_examples, [0, 1, 3, 4, 5])\n",
    "    translations_pose = translations[translations[\"label\"] == 0]\n",
    "    translations_threat = translations[translations[\"label\"] == 1]\n",
    "    translations_emph = translations[translations[\"label\"] == 3]\n",
    "    translations_neutral = translations[translations[\"label\"] == 4]\n",
    "    translations_unint = translations[translations[\"label\"] == 5]\n",
    "\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples_weak,\n",
    "        new_confident_examples_rest,\n",
    "        translations_pose[cols],\n",
    "        translations_threat[cols],\n",
    "        translations_emph[cols],\n",
    "        translations_neutral[cols],\n",
    "        translations_unint[cols],\n",
    "    ])\n",
    "elif label_condensation == \"full\":\n",
    "    translations_neutral = dpr.select_translations(new_confident_examples, [1])\n",
    "    new_confident_examples_weak = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 0].sample(n=287, random_state=42)\n",
    "    new_confident_examples_neutral = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 1]\n",
    "    new_confident_examples_other = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 2].sample(n=287, random_state=42)\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples_weak,\n",
    "        new_confident_examples_neutral,\n",
    "        new_confident_examples_other,\n",
    "        translations_neutral[cols],\n",
    "    ])\n",
    "elif label_condensation == \"full2\":\n",
    "    translations_in_both_positive = dpr.select_translations(new_confident_examples, [0])\n",
    "    new_confident_examples_in_both_positive = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 0]\n",
    "    new_confident_examples_out_negative = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 1].sample(n=172+157, random_state=42)\n",
    "    new_confident_examples_neutral_unint = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 2].sample(n=172+157, random_state=42)\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples_in_both_positive,\n",
    "        new_confident_examples_out_negative,\n",
    "        new_confident_examples_neutral_unint,\n",
    "        translations_in_both_positive[cols],\n",
    "    ])\n",
    "else:\n",
    "    print(\"unknown label condensation\")\n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac8361-b879-4f02-a23f-52fb6c5c3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_goal_aug-trans-inferred2{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bfd878-f139-44be-99f5-ed5009caf3dd",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43fbff2-9fa0-46a7-b9ae-fa7f5083b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_goal_aug-trans-inferred2{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_goal_aug-trans-inferred2{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df29aa-9d3b-49f4-98e6-777570c6a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_goal_aug-trans-inferred* jlasse@nvcluster:/home/jlasse/GermanHass/goal_analysis/data_preparation/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
