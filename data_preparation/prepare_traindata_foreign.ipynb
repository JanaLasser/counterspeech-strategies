{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the training data to fine-tune the foreign language prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"[STYLE]\": {\n",
    "        \"info\":0,\n",
    "        \"opin\":0,\n",
    "        \"quest\":0,\n",
    "        \"conseq\":0,\n",
    "        \"correct\":0,\n",
    "        \"inconsist\":0,\n",
    "        \"sarc\":0,\n",
    "        \"insult-pers\":0,\n",
    "        \"insult-ism\":0,\n",
    "        \"insult-polit\":0,\n",
    "        \"insult-inst\":0,\n",
    "        \"other\":0,\n",
    "        \"unint\":0,\n",
    "        \"foreign\":1\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial rater: AS\n",
      "initial batch dropped 0 NA entries\n",
      "additional batch 3 rater AH: dropped 0 NA entries\n",
      "additional batch 3 rater LT: dropped 0 NA entries\n",
      "additional batch 4 rater AH: dropped 0 NA entries\n",
      "additional batch 4 rater LT: dropped 0 NA entries\n",
      "additional batch 5 rater AH: dropped 0 NA entries\n",
      "additional batch 5 rater LT: dropped 0 NA entries\n",
      "initial rater: LT\n",
      "initial batch dropped 0 NA entries\n",
      "additional batch 3 rater AH: dropped 0 NA entries\n",
      "additional batch 3 rater LT: dropped 0 NA entries\n",
      "additional batch 4 rater AH: dropped 0 NA entries\n",
      "additional batch 4 rater LT: dropped 0 NA entries\n",
      "additional batch 5 rater AH: dropped 0 NA entries\n",
      "additional batch 5 rater LT: dropped 0 NA entries\n"
     ]
    }
   ],
   "source": [
    "src = \"labelled_samples_with_ids\"\n",
    "fname = \"batch_{}_{}.csv\"\n",
    "category = \"[STYLE]\"\n",
    "raters_initial_batch = [\"AS\", \"LT\"]\n",
    "initial_batch = 1\n",
    "raters_additional_batches = [\"AH\", \"LT\"]\n",
    "additional_batches = [3, 4, 5]\n",
    "\n",
    "data = {}\n",
    "for initial_rater in raters_initial_batch:\n",
    "    print(f\"initial rater: {initial_rater}\")\n",
    "    # load the initial batch labelled by two raters\n",
    "    df = pd.read_csv(join(data_dir, src, fname.format(initial_batch, initial_rater)),\n",
    "                     delimiter=\";\")\n",
    "    N = len(df)\n",
    "    df = df[[\"text\", category]]\n",
    "    df = df.dropna()\n",
    "    print(f\"initial batch dropped {N - len(df)} NA entries\")\n",
    "\n",
    "    # load additional batches and add them to the datasets\n",
    "    for additional_batch in additional_batches:\n",
    "        for rater in raters_additional_batches:\n",
    "            tmp = pd.read_csv(join(data_dir, src, fname.format(additional_batch, rater)),\n",
    "                              delimiter=\";\")\n",
    "            N = len(tmp)\n",
    "            tmp = tmp[[\"text\", category]]\n",
    "            tmp = tmp.dropna()\n",
    "            print(f\"additional batch {additional_batch} rater {rater}: dropped {N - len(tmp)} NA entries\")\n",
    "            df = pd.concat([df, tmp])\n",
    "            \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # hardcoded label conversion\n",
    "    label_to_id = label_map[category]\n",
    "    id_to_label = {val:key for key, val in label_to_id.items()}\n",
    "    df[\"label\"] = [label_to_id[label] for label in df[category]]\n",
    "    df = df.drop(columns=[category])\n",
    "    data[initial_rater] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "for rater in [\"AS\", \"LT\"]:\n",
    "    # remove only URLs\n",
    "    data[rater][\"text\"] = data[rater][\"text\"]\\\n",
    "        .apply(lambda x: re.sub(r\"https?:\\/\\/\\S*\", \"\", x, flags=re.MULTILINE))\n",
    "\n",
    "    # lowercase all text\n",
    "    data[rater][\"text\"] = data[rater][\"text\"]\\\n",
    "        .apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for rater in raters_initial_batch:\n",
    "    for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "        # get the eval data\n",
    "        sss = StratifiedShuffleSplit(\n",
    "            n_splits=1, \n",
    "            test_size=eval_frac, \n",
    "            random_state=seed\n",
    "        )\n",
    "        sss.get_n_splits(data[rater][\"text\"], data[rater][\"label\"])\n",
    "        for tmp_index, eval_index in sss.split(data[rater][\"text\"],\n",
    "                                                 data[rater][\"label\"]):\n",
    "            X_tmp, X_eval = data[rater][\"text\"].loc[tmp_index],\\\n",
    "                              data[rater][\"text\"].loc[eval_index]\n",
    "            y_tmp, y_eval = data[rater][\"label\"].loc[tmp_index],\\\n",
    "                              data[rater][\"label\"].loc[eval_index]\n",
    "\n",
    "        tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "        evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "        # get the test data\n",
    "        sss = StratifiedShuffleSplit(\n",
    "            n_splits=1, \n",
    "            test_size=test_frac, \n",
    "            random_state=s + 10\n",
    "        )\n",
    "        sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "        for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "            X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                              tmp[\"text\"].loc[test_index]\n",
    "            y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                              tmp[\"label\"].loc[test_index]\n",
    "\n",
    "            traindata = pd.concat([X_train, y_train], axis=1)\n",
    "            testdata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "            tmp_fname = fname.split(\".\")[0]\n",
    "            batch = '1'\n",
    "            for b in additional_batches:\n",
    "                batch += f'+{b}'\n",
    "\n",
    "        dst = join(data_dir, \"traindata\", \"foreign\")\n",
    "        traindata.to_csv(join(dst, tmp_fname.format(batch, rater) + f\"_train_{s+1}.csv\"), index=False)\n",
    "        testdata.to_csv(join(dst, tmp_fname.format(batch, rater) + f\"_test_{s+1}.csv\"), index=False)\n",
    "        evaldata.to_csv(join(dst, tmp_fname.format(batch, rater) + f\"_eval_{s+1}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
