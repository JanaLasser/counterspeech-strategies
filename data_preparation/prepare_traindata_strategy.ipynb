{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ef6b7-50fa-4d94-a7ec-ac196c786ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import data_preparation_resources as dpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2752f2-649d-46d9-8410-6040bc82e525",
   "metadata": {},
   "source": [
    "# Label condensation stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d6be9-669c-4c49-a446-188fa9b28317",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../../data/traindata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277bf74-d59a-44cf-bb3d-4eb5a91ad9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_condensation = \"full\"\n",
    "condensation_dataset_names = {\n",
    "    \"medium\":\"_halfcondensed\",\n",
    "    \"full\":\"_condensed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459a68a-d131-46d3-b0bc-c6b9afdc63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full label dictionary after removing \"foreign\"\n",
    "label_to_id = {\n",
    "    \"info\":0,\n",
    "    \"opin\":1,\n",
    "    \"quest\":2,\n",
    "    \"conseq\":3,\n",
    "    \"correct\":4,\n",
    "    \"inconsist\":5,\n",
    "    \"sarc\":6,\n",
    "    \"insult-pers\":7,\n",
    "    \"insult-ism\":8,\n",
    "    \"insult-polit\":9,\n",
    "    \"insult-inst\":10,\n",
    "    \"other\":11,\n",
    "    \"unint\":12,\n",
    "}\n",
    "id_to_label = {val:key for key, val in label_to_id.items()}\n",
    "\n",
    "# condensed label dictionaries\n",
    "if label_condensation == \"medium\":\n",
    "    label_to_condensed_id = {\n",
    "         'info': 0,\n",
    "         'opin': 1,\n",
    "         'quest': 2,\n",
    "         'conseq': 2,\n",
    "         'correct': 2,\n",
    "         'inconsist': 3,\n",
    "         'sarc': 4,\n",
    "         'insult-pers': 5,\n",
    "         'insult-ism': 5,\n",
    "         'insult-polit': 5,\n",
    "         'insult-inst': 5,\n",
    "         'other': 6,\n",
    "         'unint': 6\n",
    "    }\n",
    "    \n",
    "    condensed_id_to_label = {\n",
    "        0:\"info\",\n",
    "        1:\"opin\",\n",
    "        2:\"construct\",\n",
    "        3:\"inconsist\",\n",
    "        4:\"sarc\",\n",
    "        5:\"insult\",\n",
    "        6:\"other_new\",\n",
    "    }\n",
    "    \n",
    "    id_to_condensed_id = {\n",
    "         0: 0,\n",
    "         1: 1,\n",
    "         2: 2,\n",
    "         3: 2,\n",
    "         4: 2,\n",
    "         5: 3,\n",
    "         6: 4,\n",
    "         7: 5,\n",
    "         8: 5,\n",
    "         9: 5,\n",
    "         10: 5,\n",
    "         11: 6,\n",
    "         12: 6}\n",
    "    label_to_condensed_label = {\n",
    "         'info': \"info\",\n",
    "         'opin': \"opin\",\n",
    "         'quest': \"construct\",\n",
    "         'conseq': \"construct\",\n",
    "         'correct': \"construct\",\n",
    "         'inconsist': \"inconsist\",\n",
    "         'sarc': \"sarc\",\n",
    "         'insult-pers': \"insult\",\n",
    "         'insult-ism': \"insult\",\n",
    "         'insult-polit': \"insult\",\n",
    "         'insult-inst': \"insult\",\n",
    "         'other': \"other_new\",\n",
    "         'unint': \"other_new\"    \n",
    "    }\n",
    "\n",
    "    condensed_label_to_condensed_id = {\n",
    "        \"info\":0,\n",
    "        \"opin\":1,\n",
    "        \"construct\":2,\n",
    "        \"inconsust\":3,\n",
    "        \"sarc\":4,\n",
    "        \"insult\":5,\n",
    "        \"other_new\":6\n",
    "    }\n",
    "\n",
    "elif label_condensation == \"full\":\n",
    "    label_to_condensed_id = {\n",
    "         'info': 0,\n",
    "         'opin': 1,\n",
    "         'quest': 0,\n",
    "         'conseq': 0,\n",
    "         'correct': 0,\n",
    "         'inconsist': 0,\n",
    "         'sarc': 2,\n",
    "         'insult-pers': 3,\n",
    "         'insult-ism': 3,\n",
    "         'insult-polit': 3,\n",
    "         'insult-inst': 3,\n",
    "         'other': 4,\n",
    "         'unint': 4}\n",
    "\n",
    "    condensed_id_to_label = {\n",
    "        0:\"construct\",\n",
    "        1:\"opin\",\n",
    "        2:\"sarc\",\n",
    "        3:\"leave_fact\",\n",
    "        4:\"other_new\",\n",
    "    }\n",
    "\n",
    "    id_to_condensed_id = {\n",
    "         0: 0,\n",
    "         1: 1,\n",
    "         2: 0,\n",
    "         3: 0,\n",
    "         4: 0,\n",
    "         5: 0,\n",
    "         6: 2,\n",
    "         7: 3,\n",
    "         8: 3,\n",
    "         9: 3,\n",
    "         10: 3,\n",
    "         11: 4,\n",
    "         12: 4}\n",
    "    \n",
    "    label_to_condensed_label = {\n",
    "         'info': \"construct\",\n",
    "         'opin': \"opin\",\n",
    "         'quest': \"construct\",\n",
    "         'conseq': \"construct\",\n",
    "         'correct': \"construct\",\n",
    "         'inconsist': \"construct\",\n",
    "         'sarc': \"sarc\",\n",
    "         'insult-pers': \"leave_fact\",\n",
    "         'insult-ism': \"leave_fact\",\n",
    "         'insult-polit': \"leave_fact\",\n",
    "         'insult-inst': \"leave_fact\",\n",
    "         'other': \"other_new\",\n",
    "         'unint': \"other_new\"    \n",
    "    }\n",
    "\n",
    "    condensed_label_to_condensed_id = {\n",
    "        \"construct\":0,\n",
    "        \"opin\":1,\n",
    "        \"sarc\":2,\n",
    "        \"leave_fact\":3,\n",
    "        \"other_new\":4,\n",
    "    }\n",
    "else:\n",
    "    print(\"unknown condensation level!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2d73e-f42a-43f2-8294-fb11874fcd57",
   "metadata": {},
   "source": [
    "# Backtranslations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ad79d-c640-4169-942d-351f0e5249a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V1: dataset with confident labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4260de7-51a1-4790-bae7-4c667bcfd4e6",
   "metadata": {},
   "source": [
    "## Load confident human labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8bf47-f171-41f5-afbb-ec422d342acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[STRATEGY]\"\n",
    "fname = \"confident_examples_strategy{}\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "cols = [\"tweet_id\", \"text\", dimension]\n",
    "confident_examples = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df1 = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_1\"})\n",
    "    df2 = pd.read_csv(\n",
    "        Path(src, pair[1] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_2\"})\n",
    "    \n",
    "    df1 = df1[df1[\"label_1\"] != \"foreign\"]\n",
    "    df2 = df2[df2[\"label_2\"] != \"foreign\"]\n",
    "    df1[\"label_1\"] = df1[\"label_1\"].replace(label_to_condensed_id)\n",
    "    df2[\"label_2\"] = df2[\"label_2\"].replace(label_to_condensed_id)\n",
    "    \n",
    "    shared_ids = df1[df1[\"tweet_id\"].isin(df2[\"tweet_id\"])][\"tweet_id\"].values\n",
    "    df1 = df1[df1[\"tweet_id\"].isin(shared_ids)]\n",
    "    df2 = df2[df2[\"tweet_id\"].isin(shared_ids)]\n",
    "    df1 = df1.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    df2 = df2.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    \n",
    "    df = pd.concat([df1, df2[[\"label_2\"]]], axis=1)[[\"tweet_id\", \"text\", \"label_1\", \"label_2\"]]\n",
    "    df = df[df[\"label_1\"] == df[\"label_2\"]]\n",
    "    df = df.drop(columns=[\"label_2\"]).rename(columns={\"label_1\":\"label\"})\n",
    "    confident_examples = pd.concat([confident_examples, df])\n",
    "confident_examples = confident_examples.reset_index(drop=True)\n",
    "confident_examples[\"label\"] = confident_examples[\"label\"].astype(int)\n",
    "\n",
    "# remove URLs\n",
    "confident_examples = dpr.clean_text(confident_examples)\n",
    "\n",
    "confident_examples.to_csv(Path(dst,fname + \".csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed077db-3149-4ace-b355-3ae282e49d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#        0:\"construct\",\n",
    "#        1:\"opin\",\n",
    "#        2:\"sarc\",\n",
    "#        3:\"leave_fact\",\n",
    "#        4:\"other_new\",\n",
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23290239-2bbe-4b84-8c35-a51d51a23694",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(confident_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c94fd-4df5-407f-809d-ad904c3e06b9",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344b4e6-1408-48eb-8efb-1b35161cda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_strategy{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_strategy{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef1349-6846-4662-8616-3a6114c843ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload traindata to GPU cluster\n",
    "! rsync -avze ssh ../../../data/traindata/confident_examples_strategy* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af09771-eedf-4f62-9067-160d0670e768",
   "metadata": {},
   "source": [
    "# V2: augmented minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612c59e-0a4d-4e55-b238-bf842b9e16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#        0:\"construct\",\n",
    "#        1:\"opin\",\n",
    "#        2:\"sarc\",\n",
    "#        3:\"leave_fact\",\n",
    "#        4:\"other_new\",\n",
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30fe17-8069-4a74-9675-1896b3267225",
   "metadata": {},
   "source": [
    "## Add minority example translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4671caed-a82b-44e5-85d3-9e86b2a5d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = dpr.select_translations(confident_examples, [0, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be86537-891a-46cf-b6c6-98729b942930",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b199a-7eae-4e39-9c7c-65b237e66574",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "\n",
    "if label_condensation == \"medium\":\n",
    "    translations = dpr.select_translations(confident_examples, [0, 2, 3, 4, 5, 6])\n",
    "    translations_info = translations[translations[\"label\"] == 0]\n",
    "    translations_construct = translations[translations[\"label\"] == 2]\n",
    "    translations_inconsist = translations[translations[\"label\"] == 3]\n",
    "    translations_sarc = translations[translations[\"label\"] == 4]\n",
    "    translations_insult = translations[translations[\"label\"] == 5].sample(n=45, random_state=42)\n",
    "    translations_other_new = translations[translations[\"label\"] == 6].sample(n=140, random_state=42)\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        translations_info[cols],\n",
    "        translations_construct[cols],\n",
    "        translations_inconsist[cols],\n",
    "        translations_sarc[cols],\n",
    "        translations_insult[cols],\n",
    "        translations_other_new[cols],\n",
    "    ])\n",
    "else:\n",
    "    translations_construct = translations[translations[\"label\"] == 0]\n",
    "    translations_sarc = translations[translations[\"label\"] == 2]\n",
    "    translations_leave_fact = translations[translations[\"label\"] == 3].sample(n=45, random_state=42)\n",
    "    translations_other_new = translations[translations[\"label\"] == 4].sample(n=140, random_state=42)\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        translations_construct[cols],\n",
    "        translations_sarc[cols],\n",
    "        translations_leave_fact[cols],\n",
    "        translations_other_new[cols],\n",
    "    ])\n",
    "    pass\n",
    "\n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b72a6-b737-48fc-b6d2-f347e6a509a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_strategy_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c8b03-b279-47f5-9e58-38e3fe361980",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213fd3e-22b6-4e72-8100-4496c30eb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_strategy_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_strategy_aug-trans{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab136f7e-1a51-4eca-9dd9-57d4f96b4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh -./../../data/traindata/confident_examples_strategy_aug-trans* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e9ea3-a092-4880-b260-71d35e26558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c3c6f-4851-4d27-8683-331204ac4163",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V3: human + inferred labels (round 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e818dc-a2ae-4154-945a-9343d77f035c",
   "metadata": {},
   "source": [
    "## Load existing confident examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f56f1f-3561-424f-808e-cfa559a8c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_strategy_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "confident_examples = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a7720-ad05-4f94-995e-dde79fab091f",
   "metadata": {},
   "source": [
    "## Load remaining human annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd4b17-5c19-4f46-8b91-4a8110247c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[STRATEGY]\"\n",
    "cols = [\"tweet_id\", \"text\", dimension]\n",
    "labelled = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_1\"})\n",
    "    \n",
    "    df = df[df[\"label_1\"] != \"foreign\"]\n",
    "    df = df.dropna(subset=[\"label_1\"])\n",
    "    df[\"label_1\"] = df[\"label_1\"].apply(lambda x: x.strip(\" \"))\n",
    "    df[\"label_1\"] = df[\"label_1\"].replace(label_to_condensed_id)\n",
    "    labelled = pd.concat([labelled, df])\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "labelled[\"label_1\"] = labelled[\"label_1\"].astype(int)\n",
    "labelled = dpr.clean_text(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50e52b-8b94-4e43-8fce-868000d0c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled[\"label_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0058d-0f68-402b-b754-f1f83c0f75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = labelled[~labelled[\"tweet_id\"].isin(confident_examples[\"tweet_id\"])].copy()\n",
    "len(remaining_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401991e-dbe5-46b4-8a6f-5c3c38141253",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"remaining_examples_strategy{}_1.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "remaining_examples.rename(columns={\"label_1\":\"label\"}).to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81972a-d725-4446-9c20-41d5817a8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/remaining_examples_strategy* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec472e7-868a-4019-84dc-13219594b1f0",
   "metadata": {},
   "source": [
    "## Load inferred labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1db26-2cfd-417a-ac81-92d3367af782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 infer_strategy.py ../best_models/model-twitter-xlm-roberta-base_germanhass_epochs-100_batchsize-64_data-confident_examples_strategy_aug-trans_halfcondensed_split-3 ../data/traindata/remaining_examples_strategy_condensed_1.csv 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff7299-a884-4efa-ba3e-0deefc9fa864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download inferred data\n",
    "! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/inferred_strategy* ../../../data/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e5909-8947-4e60-9bd7-655ac82c33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_strategy{}_remaining_examples{}_1.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation],\n",
    "            condensation_dataset_names[label_condensation])\n",
    "inferred_labels = pd.read_csv(\n",
    "    Path(\"../../../data/inference\", fname),\n",
    "    delimiter=\";\",\n",
    "    dtype={\"tweet_id\":str, \"strategy\":int},\n",
    "    usecols=[\"tweet_id\", \"strategy\"]\n",
    ").rename(columns={\"strategy\":\"label_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33b76a-7b14-4f52-b819-30bab92c1200",
   "metadata": {},
   "source": [
    "## Determine label agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b35ab4-6f50-4d86-bb8d-f8666a47ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = pd.merge(\n",
    "    remaining_examples,\n",
    "    inferred_labels,\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8ef0c-0ad7-435e-8f24-58c39187fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = remaining_examples.dropna(subset=[\"label_2\"])\n",
    "remaining_examples[\"label_2\"] = remaining_examples[\"label_2\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d716877-21de-4796-bb9a-fe5e932cea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confident_examples = remaining_examples[remaining_examples[\"label_1\"] == remaining_examples[\"label_2\"]]\n",
    "new_confident_examples = new_confident_examples[[\"tweet_id\", \"text\", \"label_1\"]].rename(columns={\"label_1\":\"label\"})\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7b3a5-82ec-4735-bd1d-148b25bb7bd0",
   "metadata": {},
   "source": [
    "## Add augmented minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035ee4f-0f7f-4539-82fd-b66b796446fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0a30d-cc66-4288-97b1-3705e519e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "\n",
    "if label_condensation == \"medium\":\n",
    "    translations = dpr.select_translations(new_confident_examples, [0, 1, 2, 3, 4, 6])\n",
    "    translations_info = translations[translations[\"label\"] == 0]\n",
    "    translations_opin = translations[translations[\"label\"] == 1].sample(n=413, random_state=42)\n",
    "    translations_construct = translations[translations[\"label\"] == 2]\n",
    "    translations_inconsist = translations[translations[\"label\"] == 3]\n",
    "    translations_sarc = translations[translations[\"label\"] == 4]\n",
    "    translations_other_new = translations[translations[\"label\"] == 6]\n",
    "    \n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples,\n",
    "        translations_info[cols],\n",
    "        translations_opin[cols],\n",
    "        translations_construct[cols],\n",
    "        translations_inconsist[cols],\n",
    "        translations_sarc[cols],\n",
    "        translations_other_new[cols],\n",
    "    ])\n",
    "else:\n",
    "    # subsample the opin examples to have less class imbalance\n",
    "    new_confident_examples_construct = new_confident_examples[new_confident_examples[\"label\"] == 0]\n",
    "    new_confident_examples_opin = new_confident_examples[new_confident_examples[\"label\"] == 1].sample(n=1000, random_state=42)\n",
    "    new_confident_examples_sarc = new_confident_examples[new_confident_examples[\"label\"] == 2]\n",
    "    new_confident_examples_leave_fact = new_confident_examples[new_confident_examples[\"label\"] == 3]\n",
    "    new_confident_examples_other_new = new_confident_examples[new_confident_examples[\"label\"] == 4]\n",
    "\n",
    "    translations = dpr.select_translations(new_confident_examples, [0, 2, 3, 4])\n",
    "    translations_construct = translations[translations[\"label\"] == 0]\n",
    "    translations_sarc = translations[translations[\"label\"] == 2]\n",
    "    translations_leave_fact = translations[translations[\"label\"] == 3]\n",
    "    translations_other_new = translations[translations[\"label\"] == 4]\n",
    "    \n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples_construct,\n",
    "        new_confident_examples_opin,\n",
    "        new_confident_examples_sarc,\n",
    "        new_confident_examples_leave_fact,\n",
    "        new_confident_examples_other_new,\n",
    "        translations_construct[cols],\n",
    "        translations_sarc[cols],\n",
    "        translations_leave_fact[cols],\n",
    "        translations_other_new[cols],\n",
    "    ])\n",
    "    \n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cae93-517c-44ae-8846-55e168cd1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_strategy_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2bd6fd-2e04-4d09-b709-34f2d0161e6e",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f8b94-f361-4b34-a96f-18579c6bc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_strategy_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_strategy_aug-trans-inferred{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ed731-4051-44c8-99ba-ab548d3ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_strategy_aug-trans-inferred* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f042148e-f7ef-41be-883b-85394141ec19",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V4: human + inferred labels (round 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d511b-7e0c-48e7-bd59-47d5f4b7c36a",
   "metadata": {},
   "source": [
    "## Load existing confident examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc0e52-c223-4696-aa21-d91c0b1c7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_strategy_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "confident_examples = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68504429-7ef8-4355-a894-ad424413dd2f",
   "metadata": {},
   "source": [
    "## Load remaining human annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d0893-dc3d-4169-9805-02e54b567d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[STRATEGY]\"\n",
    "cols = [\"tweet_id\", \"text\", dimension]\n",
    "labelled = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_1\"})\n",
    "    \n",
    "    df = df[df[\"label_1\"] != \"foreign\"]\n",
    "    df = df.dropna(subset=[\"label_1\"])\n",
    "    df[\"label_1\"] = df[\"label_1\"].apply(lambda x: x.strip(\" \"))\n",
    "    df[\"label_1\"] = df[\"label_1\"].replace(label_to_condensed_id)\n",
    "    labelled = pd.concat([labelled, df])\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "labelled[\"label_1\"] = labelled[\"label_1\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f57509-d16b-4bf5-af13-7135fe29a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled[\"label_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c692dfe2-a92c-47f5-81e1-3dd4457d2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = labelled[~labelled[\"tweet_id\"].isin(confident_examples[\"tweet_id\"])].copy()\n",
    "len(remaining_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adcc02c-b900-4af4-9937-1162669fcd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"remaining_examples_strategy{}_2.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "remaining_examples[\"label_1\"] = remaining_examples[\"label_1\"]\\\n",
    "    .replace(id_to_condensed_id)\n",
    "remaining_examples.rename(columns={\"label_1\":\"label\"}).to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be61f0-11a7-403e-933a-c2e212551cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/remaining_examples_strategy* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b1a6b-198b-4c6d-be60-1405398d5689",
   "metadata": {},
   "source": [
    "## Load inferred labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d68a6c-44a4-4d96-bfca-47249eaab4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 infer_strategy.py ../best_models/model-twitter-xlm-roberta-base_germanhass_epochs-100_batchsize-64_data-confident_examples_strategy_aug-trans-inferred_halfcondensed_split-1 ../data/traindata/remaining_examples_strategy_halfcondensed_2.csv 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05ee82-67c2-481d-b692-4dc34e91757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download inferred data\n",
    "! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/inferred_strategy* ../../../data/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235abff4-0ecc-4512-9b6d-b407b41ee14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_strategy{}_remaining_examples{}_2.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation],\n",
    "            condensation_dataset_names[label_condensation])\n",
    "inferred_labels = pd.read_csv(\n",
    "    Path(\"../../../data/inference\", fname),\n",
    "    delimiter=\";\",\n",
    "    dtype={\"tweet_id\":str, \"strategy\":int},\n",
    "    usecols=[\"tweet_id\", \"strategy\"]\n",
    ").rename(columns={\"strategy\":\"label_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f5229-9100-41f8-8fee-f1be5a27e94c",
   "metadata": {},
   "source": [
    "## Determine label agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae47ae7-a724-44d0-accb-705434a7abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = pd.merge(\n",
    "    remaining_examples,\n",
    "    inferred_labels,\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2512c-bbe0-4242-aad6-68301248ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = remaining_examples.dropna(subset=[\"label_2\"])\n",
    "remaining_examples[\"label_2\"] = remaining_examples[\"label_2\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed48b4e-4657-4a3f-98a9-caec8b1858b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confident_examples = remaining_examples[remaining_examples[\"label_1\"] == remaining_examples[\"label_2\"]]\n",
    "new_confident_examples = new_confident_examples[[\"tweet_id\", \"text\", \"label_1\"]].rename(columns={\"label_1\":\"label\"})\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e753326-702a-48db-b093-81826376c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9ff76-a483-46e3-bb7d-f5c6aa7943d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = dpr.select_translations(new_confident_examples, [3, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c0025-72f9-4f61-b708-231e04a8e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(translations[translations[\"label\"] == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49b237-485a-4bc3-a668-9a8fb1b109c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = select_translations(confident_examples, [3])\n",
    "translations[\"translation_id\"] = [f\"t{i}\" for i in range(len(translations))]\n",
    "translations_inconsist_old = translations.sample(n=2140, random_state=42)\n",
    "translations_inconsist_new = translations[~translations[\"translation_id\"].isin(translations_inconsist_old[\"translation_id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee239ed-9e0c-47cf-b2ca-83246ae761e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(translations_inconsist_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36999e2-3d9a-49c5-91cf-c4541504dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccb9045-8c28-4c19-b967-ffe1cbe683df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(translations.drop_duplicates(subset=[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d66ff-3639-4f02-aa23-89e664ac053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(translations_inconsist_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095124c-885a-4f0f-a721-b735ceef89bd",
   "metadata": {},
   "source": [
    "## Add augmented minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c12a87-52bf-44ab-b3c2-ae257b534a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "\n",
    "if label_condensation == \"medium\":\n",
    "    translations = select_translations(new_confident_examples, [0, 2, 3, 4])\n",
    "    translations_info = translations[translations[\"label\"] == 0].sample(n=500, random_state=42)\n",
    "    translations_construct = translations[translations[\"label\"] == 2].sample(n=429, random_state=42)\n",
    "    translations_inconsist = translations[translations[\"label\"] == 3]\n",
    "    translations_sarc = translations[translations[\"label\"] == 4].sample(n=361, random_state=42)\n",
    "    #translations_insult = translations[translations[\"label\"] == 5].sample(n=289, random_state=42)\n",
    "    #translations_other_new = translations[translations[\"label\"] == 6].sample(n=1751, random_state=42)\n",
    "    \n",
    "    # since we don't generate enough new translations for classes 3, 4 and 5\n",
    "    # but have some translations remaining from the initial confident samples,\n",
    "    # we get the translations that were not used in the last traning pass and\n",
    "    # add them here as well\n",
    "    translations = select_translations(confident_examples, [3, 4, 5])\n",
    "    translations_inconsistent_old = translations[translations[\"label\"] == 3].sample(n=373, random_state=42)\n",
    "    translations_inconsistent_new = translations[~translations[\"text\"].isin(translations_neutral_old[\"text\"])]\n",
    "    \n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples,\n",
    "        translations_info[cols],\n",
    "        translations_construct[cols],\n",
    "        translations_inconsist[cols],\n",
    "        translations_sarc[cols],\n",
    "        translations_insult[cols],\n",
    "        translations_other_new[cols],\n",
    "    ])\n",
    "else:\n",
    "    pass\n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860a820-1d62-4742-b294-20ea0cb5e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_strategy_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43c933-01fd-49b6-a61c-ca7e2636c515",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c297f21-c6e8-499e-bac7-d13e39fdacc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_strategy_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_strategy_aug-trans-inferred{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660148a-3c74-4b42-95c6-9b251ff6bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_strategy_aug-trans-inferred* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
