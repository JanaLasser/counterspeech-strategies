{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ef6b7-50fa-4d94-a7ec-ac196c786ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import data_preparation_resources as dpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2752f2-649d-46d9-8410-6040bc82e525",
   "metadata": {},
   "source": [
    "# Label condensation stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80c01f-77a8-4cfd-af0e-3ecc04e7b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../../data/traindata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea7fea7-e7f4-4f59-b356-9e7b69a26378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be \"none\", \"medium\" and \"full\"\n",
    "label_condensation = \"full\"\n",
    "# only here to be consistent with other scripts that do have label condensation\n",
    "condensation_dataset_names = {\"none\":\"\", \"full\":\"_condensed\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb3f77-3e47-42b5-935c-cfd3de823c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label dictionary after removing \"foreign\"\n",
    "label_to_id = {\n",
    "    \"yes\":0,\n",
    "    \"no\":1,\n",
    "    \"unint\":2,\n",
    "    }\n",
    "\n",
    "# condensed label dictionaries\n",
    "if label_condensation == \"none\":\n",
    "    label_to_condensed_id = label_to_id\n",
    "    condensed_id_to_label = {val:key for key, val in label_to_id.items()}\n",
    "elif label_condensation == \"full\":\n",
    "    label_to_condensed_id = {\n",
    "        \"yes\":0,\n",
    "        \"no\":1,\n",
    "        \"unint\":1\n",
    "    }\n",
    "    condensed_id_to_label = {\n",
    "        0:\"yes\",\n",
    "        1:\"no\"\n",
    "    }\n",
    "else:\n",
    "    print(\"unknown condensation level!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ad79d-c640-4169-942d-351f0e5249a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V1: dataset with confident labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4260de7-51a1-4790-bae7-4c667bcfd4e6",
   "metadata": {},
   "source": [
    "## Load confident human labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8bf47-f171-41f5-afbb-ec422d342acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[SPEECH][hate]\"\n",
    "fname = \"confident_examples_hate{}\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "cols = [\"tweet_id\", \"text\", dimension]\n",
    "confident_examples = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df1 = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_1\"})\n",
    "    df2 = pd.read_csv(\n",
    "        Path(src, pair[1] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_2\"})\n",
    "    \n",
    "    df1 = df1[df1[\"label_1\"] != \"foreign\"]\n",
    "    df2 = df2[df2[\"label_2\"] != \"foreign\"]\n",
    "    df1[\"label_1\"] = df1[\"label_1\"].replace(label_to_condensed_id)\n",
    "    df2[\"label_2\"] = df2[\"label_2\"].replace(label_to_condensed_id)\n",
    "    \n",
    "    shared_ids = df1[df1[\"tweet_id\"].isin(df2[\"tweet_id\"])][\"tweet_id\"].values\n",
    "    df1 = df1[df1[\"tweet_id\"].isin(shared_ids)]\n",
    "    df2 = df2[df2[\"tweet_id\"].isin(shared_ids)]\n",
    "    df1 = df1.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    df2 = df2.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    \n",
    "    df = pd.concat([df1, df2[[\"label_2\"]]], axis=1)[[\"tweet_id\", \"text\", \"label_1\", \"label_2\"]]\n",
    "    df = df[df[\"label_1\"] == df[\"label_2\"]]\n",
    "    df = df.drop(columns=[\"label_2\"]).rename(columns={\"label_1\":\"label\"})\n",
    "    confident_examples = pd.concat([confident_examples, df])\n",
    "confident_examples = confident_examples.reset_index(drop=True)\n",
    "confident_examples[\"label\"] = confident_examples[\"label\"].astype(int)\n",
    "confident_examples = dpr.clean_text(confident_examples)\n",
    "confident_examples.to_csv(Path(dst,fname + \".csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed077db-3149-4ace-b355-3ae282e49d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    \"yes\":0,\n",
    "#    \"no\":1,\n",
    "#    \"unint\":2,\n",
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493b94b-d203-41a0-ba65-400c85d052d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(confident_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c94fd-4df5-407f-809d-ad904c3e06b9",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344b4e6-1408-48eb-8efb-1b35161cda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_hate{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_hate{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef1349-6846-4662-8616-3a6114c843ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_hate* jlasse@nvcluster:/home/jlasse/GermanHass/speech-hate_analysis/data_preparation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af09771-eedf-4f62-9067-160d0670e768",
   "metadata": {},
   "source": [
    "# V2: augmented minority class examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30fe17-8069-4a74-9675-1896b3267225",
   "metadata": {},
   "source": [
    "## Add minority example translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41868f17-26d6-40d3-bb29-514de9cb3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = dpr.select_translations(confident_examples, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b149ef-11fb-45db-ae3a-4e1654e1d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b199a-7eae-4e39-9c7c-65b237e66574",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "if label_condensation == \"none\":\n",
    "    translations = dpr.select_translations(confident_examples, [0, 2])\n",
    "    translations_yes = translations[translations[\"label\"] == 0]\n",
    "    translations_unint = translations[translations[\"label\"] == 2]\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        translations_yes[cols],\n",
    "        translations_unint[cols],\n",
    "    ])\n",
    "elif label_condensation == \"full\":\n",
    "    translations = dpr.select_translations(confident_examples, [0])\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        translations[cols],\n",
    "    ])\n",
    "else:\n",
    "    print(\"unknown condensation level\")\n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b72a6-b737-48fc-b6d2-f347e6a509a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_hate_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c8b03-b279-47f5-9e58-38e3fe361980",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213fd3e-22b6-4e72-8100-4496c30eb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_hate_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_hate_aug-trans{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab136f7e-1a51-4eca-9dd9-57d4f96b4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_hate_aug-trans* jlasse@nvcluster:/home/jlasse/GermanHass/speech-hate_analysis/data_preparation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e9ea3-a092-4880-b260-71d35e26558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c3c6f-4851-4d27-8683-331204ac4163",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V3: human + inferred labels (round 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e818dc-a2ae-4154-945a-9343d77f035c",
   "metadata": {},
   "source": [
    "## Load existing confident examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f56f1f-3561-424f-808e-cfa559a8c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_hate_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "confident_examples = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a7720-ad05-4f94-995e-dde79fab091f",
   "metadata": {},
   "source": [
    "## Load remaining human annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd4b17-5c19-4f46-8b91-4a8110247c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[SPEECH][hate]\"\n",
    "cols = [\"tweet_id\", \"text\", dimension]\n",
    "labelled = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_1\"})\n",
    "    \n",
    "    df = df[df[\"label_1\"] != \"foreign\"]\n",
    "    df = df.dropna(subset=[\"label_1\"])\n",
    "    df[\"label_1\"] = df[\"label_1\"].apply(lambda x: x.strip(\" \"))\n",
    "    df[\"label_1\"] = df[\"label_1\"].replace(label_to_condensed_id)\n",
    "    labelled = pd.concat([labelled, df])\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "labelled[\"label_1\"] = labelled[\"label_1\"].astype(int)\n",
    "labelled = dpr.clean_text(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50e52b-8b94-4e43-8fce-868000d0c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled[\"label_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0058d-0f68-402b-b754-f1f83c0f75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = labelled[~labelled[\"tweet_id\"].isin(confident_examples[\"tweet_id\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614339e-321f-4b03-b748-2b1e96b7bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remaining_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401991e-dbe5-46b4-8a6f-5c3c38141253",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"remaining_examples_hate{}_1.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "remaining_examples.rename(columns={\"label_1\":\"label\"}).to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81972a-d725-4446-9c20-41d5817a8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/remaining_examples_hate* jlasse@nvcluster:/home/jlasse/GermanHass/speech-hate_analysis/data_preparation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec472e7-868a-4019-84dc-13219594b1f0",
   "metadata": {},
   "source": [
    "## Load inferred labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1db26-2cfd-417a-ac81-92d3367af782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 infer_hate.py ../best_models/model-twitter-xlm-roberta-base_germanhass_epochs-100_batchsize-64_data-confident_examples_hate_aug-trans_condensed_split-1 ../data_preparation/remaining_examples_hate_condensed_1.csv 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff7299-a884-4efa-ba3e-0deefc9fa864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download inferred data\n",
    "! rsync -avze ssh jlasse@nvcluster:/home/jlasse/GermanHass/speech-hate_analysis/inference/inferred_speech-hate* ../../../data/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e5909-8947-4e60-9bd7-655ac82c33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_hate{}_remaining_examples{}_1.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation],\n",
    "            condensation_dataset_names[label_condensation])\n",
    "inferred_labels = pd.read_csv(\n",
    "    Path(\"../../../data/inference\", fname),\n",
    "    delimiter=\";\",\n",
    "    dtype={\"tweet_id\":str, \"speech-hate\":int},\n",
    "    usecols=[\"tweet_id\", \"speech-hate\"]\n",
    ").rename(columns={\"speech-hate\":\"label_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33b76a-7b14-4f52-b819-30bab92c1200",
   "metadata": {},
   "source": [
    "## Determine label agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b35ab4-6f50-4d86-bb8d-f8666a47ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = pd.merge(\n",
    "    remaining_examples,\n",
    "    inferred_labels,\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8ef0c-0ad7-435e-8f24-58c39187fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = remaining_examples.dropna(subset=[\"label_2\"])\n",
    "remaining_examples[\"label_2\"] = remaining_examples[\"label_2\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d716877-21de-4796-bb9a-fe5e932cea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confident_examples = remaining_examples[remaining_examples[\"label_1\"] == remaining_examples[\"label_2\"]]\n",
    "new_confident_examples = new_confident_examples[[\"tweet_id\", \"text\", \"label_1\"]].rename(columns={\"label_1\":\"label\"})\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7b3a5-82ec-4735-bd1d-148b25bb7bd0",
   "metadata": {},
   "source": [
    "## Add augmented minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0a30d-cc66-4288-97b1-3705e519e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "if label_condensation == \"none\":\n",
    "    translations = dpr.select_translations(new_confident_examples, [0])\n",
    "    translations_yes = translations[translations[\"label\"] == 0]\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        new_confident_examples,\n",
    "        translations_yes[cols],\n",
    "    ])\n",
    "elif label_condensation == \"full\":\n",
    "    translations = dpr.select_translations(new_confident_examples, [0])\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        new_confident_examples,\n",
    "        translations[cols],\n",
    "    ])\n",
    "else:\n",
    "    print(\"unknown label condensation!\")\n",
    "\n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cae93-517c-44ae-8846-55e168cd1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_hate_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2bd6fd-2e04-4d09-b709-34f2d0161e6e",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f8b94-f361-4b34-a96f-18579c6bc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_hate_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_hate_aug-trans-inferred{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ed731-4051-44c8-99ba-ab548d3ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_hate_aug-trans-inferred* jlasse@nvcluster:/home/jlasse/GermanHass/speech-hate_analysis/data_preparation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd73bef-71b2-4953-a71e-2db2390e82c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V4: human + inferred labels (round 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc4879-9bce-4d48-81f4-e1e694e4cba0",
   "metadata": {},
   "source": [
    "## Load existing confident examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a24e0ec-4b73-4c5c-9a8a-66748ec8b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_hate_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "confident_examples = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02f9d0-b591-4b54-9e11-af4897fbac33",
   "metadata": {},
   "source": [
    "## Load remaining human annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490b944-8545-433f-b27d-0b78781818f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[SPEECH][hate]\"\n",
    "cols = [\"tweet_id\", \"text\", dimension]\n",
    "labelled = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_1\"})\n",
    "    \n",
    "    df = df[df[\"label_1\"] != \"foreign\"]\n",
    "    df = df.dropna(subset=[\"label_1\"])\n",
    "    df[\"label_1\"] = df[\"label_1\"].apply(lambda x: x.strip(\" \"))\n",
    "    df[\"label_1\"] = df[\"label_1\"].replace(label_to_condensed_id)\n",
    "    labelled = pd.concat([labelled, df])\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "labelled[\"label_1\"] = labelled[\"label_1\"].astype(int)\n",
    "labelled = dpr.clean_text(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a24763-6269-4960-8a95-6af7fe61660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled[\"label_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83cd60c-314e-4cd8-a263-846d7e8aef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = labelled[~labelled[\"tweet_id\"].isin(confident_examples[\"tweet_id\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9ac6c-5270-4cb3-9d24-35fac42f4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remaining_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151b085-a8b6-48f2-b2d0-cb35504b066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"remaining_examples_hate{}_2.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "remaining_examples.rename(columns={\"label_1\":\"label\"}).to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82f8e4-c5e7-45b3-8f29-952b50363bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/remaining_examples_hate* jlasse@nvcluster:/home/jlasse/GermanHass/hate_analysis/data_preparation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6b98e-67b2-4fe2-ad1a-b15425d16666",
   "metadata": {},
   "source": [
    "## Load inferred labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5fd72-6a89-493a-a2f2-a2542168cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 infer_hate.py ../best_models/model-twitter-xlm-roberta-base_germanhass_epochs-100_batchsize-64_data-confident_examples_hate_aug-trans-inferred_condensed_split-5 ../data_preparation/remaining_examples_hate_condensed_2.csv 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546be047-ec3b-4d00-a9aa-b6487b1ded82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download inferred data\n",
    "! rsync -avze ssh jlasse@nvcluster:/home/jlasse/GermanHass/speech-hate_analysis/inference/inferred_speech-hate* ../../../data/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e8345-195a-4246-941c-f2181a8a9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_hate{}_remaining_examples{}_2.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation],\n",
    "            condensation_dataset_names[label_condensation])\n",
    "inferred_labels = pd.read_csv(\n",
    "    Path(\"../../../data/inference\", fname),\n",
    "    delimiter=\";\",\n",
    "    dtype={\"tweet_id\":str, \"speech-hate\":int},\n",
    "    usecols=[\"tweet_id\", \"speech-hate\"]\n",
    ").rename(columns={\"speech-hate\":\"label_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670cc82-6ad4-4850-9657-19b35f3f65d1",
   "metadata": {},
   "source": [
    "## Determine label agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50121f41-dbc7-43b8-8e1e-208811a22bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = pd.merge(\n",
    "    remaining_examples,\n",
    "    inferred_labels,\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aba195-545c-4b74-8473-66097ece420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = remaining_examples.dropna(subset=[\"label_2\"])\n",
    "remaining_examples[\"label_2\"] = remaining_examples[\"label_2\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94842fa4-22a5-40b3-b95f-83e04241fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confident_examples = remaining_examples[remaining_examples[\"label_1\"] == remaining_examples[\"label_2\"]]\n",
    "new_confident_examples = new_confident_examples[[\"tweet_id\", \"text\", \"label_1\"]].rename(columns={\"label_1\":\"label\"})\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940b789-1590-416e-9c1f-38c7e1ba57e8",
   "metadata": {},
   "source": [
    "## Add augmented minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1bfa05-273f-4cfb-bfcc-e3ade74b5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "if label_condensation == \"none\":\n",
    "    translations = dpr.select_translations(new_confident_examples, [0])\n",
    "    translations_yes = translations[translations[\"label\"] == 0]\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        new_confident_examples,\n",
    "        translations_yes[cols],\n",
    "    ])\n",
    "elif label_condensation == \"full\":\n",
    "    translations = dpr.select_translations(new_confident_examples, [0])\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        new_confident_examples,\n",
    "        translations[cols],\n",
    "    ])\n",
    "else:\n",
    "    print(\"unknown label condensation!\")\n",
    "\n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef9ba22-2bd8-4ed0-bdc7-ac6b669c1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_hate_aug-trans-inferred2{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e409c257-a593-4942-93e1-7ed8512948f8",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46f7ef-c024-4382-b7a7-40bcd6ea935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_hate_aug-trans-inferred2{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_hate_aug-trans-inferred2{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5119dd97-81b6-4c74-bdcc-1ea820e1900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_hate_aug-trans-inferred2* jlasse@nvcluster:/home/jlasse/GermanHass/speech-hate_analysis/data_preparation/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
