{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb4ef6b7-50fa-4d94-a7ec-ac196c786ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import data_preparation_resources as dpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2752f2-649d-46d9-8410-6040bc82e525",
   "metadata": {},
   "source": [
    "# Label condensation stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd2f4d7-5085-4cdc-bdb9-dce74dea5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"../../../data/traindata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea7fea7-e7f4-4f59-b356-9e7b69a26378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be \"none\", \"medium\" and \"full\"\n",
    "label_condensation = \"full\"\n",
    "condensation_dataset_names = {\n",
    "    \"none\":\"\",\n",
    "    \"medium\":\"_halfcondensed\",\n",
    "    \"full\":\"_condensed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54cb3f77-3e47-42b5-935c-cfd3de823c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label dictionary after removing \"foreign\"\n",
    "label_to_id = {\n",
    "        \"in\":0,\n",
    "        \"out\":1,\n",
    "        \"both\":2,\n",
    "        \"neutral\":3,\n",
    "        \"unint\":4,\n",
    "    }\n",
    "\n",
    "# condensed label dictionaries\n",
    "if label_condensation == \"none\":\n",
    "    label_to_condensed_id = label_to_id\n",
    "    condensed_id_to_label = {val:key for key, val in label_to_id.items()}\n",
    "elif label_condensation == \"medium\":\n",
    "    label_to_condensed_id = {\n",
    "         'in': 0,\n",
    "         'out': 1,\n",
    "         'both': 0,\n",
    "         'neutral': 2,\n",
    "         'unint':2\n",
    "    }\n",
    "\n",
    "    condensed_id_to_label = {\n",
    "        0:\"in_both\",\n",
    "        1:\"out\",\n",
    "        2:\"neutral_unint\"\n",
    "    }    \n",
    "elif label_condensation == \"full\":\n",
    "    label_to_condensed_id = {\n",
    "         'in': 0,\n",
    "         'out': 1,\n",
    "         'both': 0,\n",
    "         'neutral': 0,\n",
    "         'unint':0\n",
    "    }\n",
    "\n",
    "    condensed_id_to_label = {\n",
    "        0:\"not_out\",\n",
    "        1:\"out\",\n",
    "    }\n",
    "else:\n",
    "    print(\"unknown condensation level!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ad79d-c640-4169-942d-351f0e5249a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V1: dataset with confident labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4260de7-51a1-4790-bae7-4c667bcfd4e6",
   "metadata": {},
   "source": [
    "## Load confident human labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcc8bf47-f171-41f5-afbb-ec422d342acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[GROUP]\"\n",
    "fname = \"confident_examples_group{}\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "cols = [\"tweet_id\", \"text\", dimension]\n",
    "confident_examples = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df1 = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_1\"})\n",
    "    df2 = pd.read_csv(\n",
    "        Path(src, pair[1] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_2\"})\n",
    "    \n",
    "    df1 = df1[df1[\"label_1\"] != \"foreign\"]\n",
    "    df2 = df2[df2[\"label_2\"] != \"foreign\"]\n",
    "    df1[\"label_1\"] = df1[\"label_1\"].replace(label_to_condensed_id)\n",
    "    df2[\"label_2\"] = df2[\"label_2\"].replace(label_to_condensed_id)\n",
    "    \n",
    "    shared_ids = df1[df1[\"tweet_id\"].isin(df2[\"tweet_id\"])][\"tweet_id\"].values\n",
    "    df1 = df1[df1[\"tweet_id\"].isin(shared_ids)]\n",
    "    df2 = df2[df2[\"tweet_id\"].isin(shared_ids)]\n",
    "    df1 = df1.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    df2 = df2.sort_values(by=\"tweet_id\").reset_index(drop=True)\n",
    "    \n",
    "    df = pd.concat([df1, df2[[\"label_2\"]]], axis=1)[[\"tweet_id\", \"text\", \"label_1\", \"label_2\"]]\n",
    "    df = df[df[\"label_1\"] == df[\"label_2\"]]\n",
    "    df = df.drop(columns=[\"label_2\"]).rename(columns={\"label_1\":\"label\"})\n",
    "    confident_examples = pd.concat([confident_examples, df])\n",
    "confident_examples = confident_examples.reset_index(drop=True)\n",
    "confident_examples[\"label\"] = confident_examples[\"label\"].astype(int)\n",
    "confident_examples = dpr.clean_text(confident_examples)\n",
    "confident_examples.to_csv(Path(dst, fname + \".csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed077db-3149-4ace-b355-3ae282e49d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    985\n",
       "0    679\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#    \"in_both\":0,\n",
    "#    \"out\":1,\n",
    "#    \"neutral_unint\":2\n",
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ccc5706-e83a-45f1-8800-ab3763451382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1664"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(confident_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c94fd-4df5-407f-809d-ad904c3e06b9",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4344b4e6-1408-48eb-8efb-1b35161cda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_group{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_group{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef1349-6846-4662-8616-3a6114c843ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_group* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af09771-eedf-4f62-9067-160d0670e768",
   "metadata": {},
   "source": [
    "# V2: augmented minority class examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a30fe17-8069-4a74-9675-1896b3267225",
   "metadata": {},
   "source": [
    "## Add minority example translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bc6e805-9214-4b77-9dee-561a5ef32729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    985\n",
       "0    679\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1baf9676-4650-4ec6-9958-913b07b2ca33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    645\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations = dpr.select_translations(confident_examples, [0, 2])\n",
    "translations[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be7b199a-7eae-4e39-9c7c-65b237e66574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    985\n",
       "0    985\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "if label_condensation == \"none\":\n",
    "    translations = dpr.select_translations(confident_examples, [0, 2])\n",
    "    translations_in = translations[translations[\"label\"] == 0].sample(n=300, random_state=42)\n",
    "    translations_both = translations[translations[\"label\"] == 2]\n",
    "    translations_neutral = translations[translations[\"label\"] == 3].sample(n=300, random_state=42)\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        translations_in[cols],\n",
    "        translations_both[cols],\n",
    "        translations_neutral[cols],\n",
    "    ])\n",
    "elif label_condensation == \"medium\":\n",
    "    translations = dpr.select_translations(confident_examples, [0])\n",
    "    translations_in_both = translations[translations[\"label\"] == 0]\n",
    "    # subsample confident examples to reduce class imbalance\n",
    "    confident_examples_out = confident_examples[confident_examples[\"label\"] == 1].sample(n=485, random_state=42)\n",
    "    confident_examples_other = confident_examples[confident_examples[\"label\"].isin([0, 2])]\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples_out,\n",
    "        confident_examples_other,\n",
    "        translations_in_both[cols],\n",
    "    ])    \n",
    "else:\n",
    "    translations = dpr.select_translations(confident_examples, [0]).sample(n=306, random_state=42)\n",
    "    translations_not_out = translations[translations[\"label\"] == 0]\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        translations_not_out[cols],\n",
    "    ])\n",
    "\n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "508b72a6-b737-48fc-b6d2-f347e6a509a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_group_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c8b03-b279-47f5-9e58-38e3fe361980",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2213fd3e-22b6-4e72-8100-4496c30eb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_groupc_aug-trans{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_group_aug-trans{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab136f7e-1a51-4eca-9dd9-57d4f96b4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_group_aug-trans* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "836e9ea3-a092-4880-b260-71d35e26558a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    689\n",
       "0    689\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c3c6f-4851-4d27-8683-331204ac4163",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V3: human + inferred labels (round 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e818dc-a2ae-4154-945a-9343d77f035c",
   "metadata": {},
   "source": [
    "## Load existing confident examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54f56f1f-3561-424f-808e-cfa559a8c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_group_aug-trans{}_full.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "confident_examples = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a7720-ad05-4f94-995e-dde79fab091f",
   "metadata": {},
   "source": [
    "## Load remaining human annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6bd4b17-5c19-4f46-8b91-4a8110247c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[GROUP]\"\n",
    "cols = [\"tweet_id\", \"text\", dimension]\n",
    "labelled = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_1\"})\n",
    "    \n",
    "    df = df[df[\"label_1\"] != \"foreign\"]\n",
    "    df = df[df[\"label_1\"] != \"unint\"]\n",
    "    df = df.dropna(subset=[\"label_1\"])\n",
    "    df[\"label_1\"] = df[\"label_1\"].apply(lambda x: x.strip(\" \"))\n",
    "    df[\"label_1\"] = df[\"label_1\"].replace(label_to_condensed_id)\n",
    "    labelled = pd.concat([labelled, df])\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "labelled[\"label_1\"] = labelled[\"label_1\"].astype(int)\n",
    "labelled = dpr.clean_text(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed50e52b-8b94-4e43-8fce-868000d0c1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8535\n",
       "0    3368\n",
       "Name: label_1, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled[\"label_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5a0058d-0f68-402b-b754-f1f83c0f75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = labelled[~labelled[\"tweet_id\"].isin(confident_examples[\"tweet_id\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1614339e-321f-4b03-b748-2b1e96b7bb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10464"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2401991e-dbe5-46b4-8a6f-5c3c38141253",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"remaining_examples_group{}_1.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "remaining_examples.rename(columns={\"label_1\":\"label\"}).to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81972a-d725-4446-9c20-41d5817a8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh remaining_examples_group* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec472e7-868a-4019-84dc-13219594b1f0",
   "metadata": {},
   "source": [
    "## Load inferred labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cca1db26-2cfd-417a-ac81-92d3367af782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 infer_group.py ../best_models/model-twitter-xlm-roberta-base_germanhass_epochs-100_batchsize-64_data-confident_examples_group_aug-trans_condensed_split-3 ../data/inference/remaining_examples_group_condensed_1.csv 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff7299-a884-4efa-ba3e-0deefc9fa864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download inferred data\n",
    "! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/* ../../../data/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "489e5909-8947-4e60-9bd7-655ac82c33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_group{}_remaining_examples{}_1.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation],\n",
    "            condensation_dataset_names[label_condensation])\n",
    "inferred_labels = pd.read_csv(\n",
    "    Path(\"../../../data/inference\", fname),\n",
    "    delimiter=\";\",\n",
    "    dtype={\"tweet_id\":str, \"group\":int},\n",
    "    usecols=[\"tweet_id\", \"group\"]\n",
    ").rename(columns={\"group\":\"label_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33b76a-7b14-4f52-b819-30bab92c1200",
   "metadata": {},
   "source": [
    "## Determine label agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5b35ab4-6f50-4d86-bb8d-f8666a47ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = pd.merge(\n",
    "    remaining_examples,\n",
    "    inferred_labels,\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ac8ef0c-0ad7-435e-8f24-58c39187fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = remaining_examples.dropna(subset=[\"label_2\"])\n",
    "remaining_examples[\"label_2\"] = remaining_examples[\"label_2\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d716877-21de-4796-bb9a-fe5e932cea47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5977\n",
       "0    1535\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_confident_examples = remaining_examples[remaining_examples[\"label_1\"] == remaining_examples[\"label_2\"]]\n",
    "new_confident_examples = new_confident_examples[[\"tweet_id\", \"text\", \"label_1\"]].rename(columns={\"label_1\":\"label\"})\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7b3a5-82ec-4735-bd1d-148b25bb7bd0",
   "metadata": {},
   "source": [
    "## Add augmented minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c35874b7-efb0-483c-97a9-753fbd33e3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1422"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations = dpr.select_translations(new_confident_examples, [0])\n",
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "125dc43e-7d84-405c-8a0d-7b076090663f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    985\n",
       "0    985\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03d0a30d-cc66-4288-97b1-3705e519e864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3942\n",
       "1    3942\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "if label_condensation == \"none\":\n",
    "    translations = dpr.select_translations(new_confident_examples, [0, 2])\n",
    "    translations_in = translations[translations[\"label\"] == 0].sample(n=300, random_state=42)\n",
    "    translations_both = translations[translations[\"label\"] == 2]\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        new_confident_examples,\n",
    "        translations_in[cols],\n",
    "        translations_both[cols],\n",
    "    ])\n",
    "elif label_condensation == \"medium\":\n",
    "    # use only the translated in_both examples to reduce class imbalance\n",
    "    translations_in_both = dpr.select_translations(new_confident_examples, [0])\n",
    "    new_confident_examples_in_both = new_confident_examples[new_confident_examples[\"label\"] == 0]\n",
    "     \n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        translations_in_both[cols],\n",
    "        new_confident_examples_in_both\n",
    "    ])\n",
    "elif label_condensation == \"full\":\n",
    "    # subsample the outgroup examples to have less class imbalance\n",
    "    new_confident_examples_not_out = new_confident_examples[new_confident_examples[\"label\"] == 0]\n",
    "    new_confident_examples_out = new_confident_examples[new_confident_examples[\"label\"] == 1].sample(n=1535+1422, random_state=42)\n",
    "    \n",
    "    translations = dpr.select_translations(new_confident_examples, [0])\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples_not_out,\n",
    "        new_confident_examples_out,\n",
    "        translations[cols],\n",
    "    ])\n",
    "else:\n",
    "    print(\"unknown label condensation\")\n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "471cae93-517c-44ae-8846-55e168cd1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_group_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2bd6fd-2e04-4d09-b709-34f2d0161e6e",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b1f8b94-f361-4b34-a96f-18579c6bc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_group_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_group_aug-trans-inferred{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ed731-4051-44c8-99ba-ab548d3ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_group_aug-trans-inferred* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc768485-0d53-49d2-b0b8-5033b3ee9027",
   "metadata": {
    "tags": []
   },
   "source": [
    "# V4: human + inferred labels (round 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c332607-dd88-4ef2-a4cc-c3f86c1e532d",
   "metadata": {},
   "source": [
    "## Load existing confident examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0026416-2ec6-4728-87c5-e82b9ae56779",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_group_aug-trans-inferred{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "confident_examples = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f1fab-66c2-471a-9a96-1e0315ae8084",
   "metadata": {},
   "source": [
    "## Load remaining human annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca8192a0-2101-4636-850e-c8566f2c3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../../../data/labelled_samples_with_ids\"\n",
    "dimension = \"[GROUP]\"\n",
    "cols = [\"tweet_id\", \"text\", dimension]\n",
    "labelled = pd.DataFrame()\n",
    "for pair in dpr.label_pairs:\n",
    "    df = pd.read_csv(\n",
    "        Path(src, pair[0] + \".csv\"),\n",
    "        dtype={\"tweet_id\":str},\n",
    "        delimiter=\";\",\n",
    "        usecols=cols\n",
    "    ).rename(columns={dimension:\"label_1\"})\n",
    "    \n",
    "    df = df[df[\"label_1\"] != \"foreign\"]\n",
    "    df = df[df[\"label_1\"] != \"unint\"]\n",
    "    df = df.dropna(subset=[\"label_1\"])\n",
    "    df[\"label_1\"] = df[\"label_1\"].apply(lambda x: x.strip(\" \"))\n",
    "    df[\"label_1\"] = df[\"label_1\"].replace(label_to_condensed_id)\n",
    "    labelled = pd.concat([labelled, df])\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "labelled[\"label_1\"] = labelled[\"label_1\"].astype(int)\n",
    "labelled = dpr.clean_text(labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddc37fcb-cd53-4828-8b60-310aea0e9766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8535\n",
       "0    3368\n",
       "Name: label_1, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled[\"label_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0bf7685-668f-4212-b07c-2bc3e24d4038",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = labelled[~labelled[\"tweet_id\"].isin(confident_examples[\"tweet_id\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52885df3-5aa4-41d0-b6b4-12a309ab961d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5972"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3fe0598b-9524-497e-9b29-604d8f0d5cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"remaining_examples_group{}_2.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "remaining_examples.rename(columns={\"label_1\":\"label\"}).to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e1a5f-2953-4b77-8720-fcfbc49215c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/remaining_examples_group* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775029cc-533d-4a9e-9838-9cbf6588f25a",
   "metadata": {},
   "source": [
    "## Load inferred labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "090a6985-9670-42ab-bc91-2990cce78aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 infer_group.py ../best_models/model-twitter-xlm-roberta-base_germanhass_epochs-100_batchsize-64_data-confident_examples_group_aug-trans-inferred_condensed_split-4 ../data/traindata/remaining_examples_condensed_2.csv 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c95cf2-8c17-4a3d-9dda-63c718cc4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download inferred data\n",
    "! rsync -avze ssh jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/inference/inferred_group* ../../../data/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c4ce83e-e8c3-4d29-a59a-eebbc553b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"inferred_group{}_remaining_examples{}_2.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation],\n",
    "            condensation_dataset_names[label_condensation])\n",
    "inferred_labels = pd.read_csv(\n",
    "    Path(\"../../../data/inference\", fname),\n",
    "    delimiter=\";\",\n",
    "    dtype={\"tweet_id\":str, \"group\":int},\n",
    "    usecols=[\"tweet_id\", \"group\"]\n",
    ").rename(columns={\"group\":\"label_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4eab2-9205-4a35-ad7b-3bb8b4adc379",
   "metadata": {},
   "source": [
    "## Determine label agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b5bdecc-3373-43cd-8914-1b23672bf42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = pd.merge(\n",
    "    remaining_examples,\n",
    "    inferred_labels,\n",
    "    how=\"left\",\n",
    "    left_on=\"tweet_id\",\n",
    "    right_on=\"tweet_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9cd45037-3d30-4bef-b6da-8ba9772eb0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples = remaining_examples.dropna(subset=[\"label_2\"])\n",
    "remaining_examples[\"label_2\"] = remaining_examples[\"label_2\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1d6bc647-4ed9-44c3-997b-09a9495ab2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3369\n",
       "0     123\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_confident_examples = remaining_examples[remaining_examples[\"label_1\"] == remaining_examples[\"label_2\"]]\n",
    "new_confident_examples = new_confident_examples[[\"tweet_id\", \"text\", \"label_1\"]].rename(columns={\"label_1\":\"label\"})\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd970d-263c-40a8-be17-756e48e8b863",
   "metadata": {},
   "source": [
    "## Add augmented minority class examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ecc39e26-0571-4860-9494-1e6ae5ff5afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3942\n",
       "1    3942\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75e0bc7d-9505-4d0b-85f7-833f698bb133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations = dpr.select_translations(new_confident_examples, [0])\n",
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f6eaf61-b1e0-42ce-b140-cf7b54560741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4178\n",
       "0    4178\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"tweet_id\", \"text\", \"label\"]\n",
    "if label_condensation == \"none\":\n",
    "    translations = dpr.select_translations(new_confident_examples, [0, 2])\n",
    "    translations_in = translations[translations[\"label\"] == 0].sample(n=300, random_state=42)\n",
    "    translations_both = translations[translations[\"label\"] == 2]\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples, \n",
    "        new_confident_examples,\n",
    "        translations_in[cols],\n",
    "        translations_both[cols],\n",
    "    ])\n",
    "elif label_condensation == \"medium\":\n",
    "    # use only the translated in_both examples to reduce class imbalance\n",
    "    translations_in_both = dpr.select_translations(new_confident_examples, [0])\n",
    "    new_confident_examples_in_both = new_confident_examples[new_confident_examples[\"label\"] == 0]\n",
    "    new_confident_examples_out = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 1].sample(n=712, random_state=42)\n",
    "    new_confident_examples_neutral_unint = new_confident_examples[\\\n",
    "        new_confident_examples[\"label\"] == 2].sample(n=712, random_state=42)\n",
    "     \n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        translations_in_both[cols],\n",
    "        new_confident_examples_in_both,\n",
    "        new_confident_examples_out,\n",
    "        new_confident_examples_neutral_unint\n",
    "    ])\n",
    "    \n",
    "elif label_condensation == \"full\":\n",
    "    translations_not_out = dpr.select_translations(new_confident_examples, [0])\n",
    "    \n",
    "    # subsample the outgroup examples to have less class imbalance\n",
    "    new_confident_examples_not_out = new_confident_examples[new_confident_examples[\"label\"] == 0]\n",
    "    new_confident_examples_out = new_confident_examples[new_confident_examples[\"label\"] == 1].sample(n=236, random_state=42)\n",
    "    new_confident_examples = pd.concat([\n",
    "        confident_examples,\n",
    "        new_confident_examples_not_out,\n",
    "        new_confident_examples_out,\n",
    "        translations_not_out[cols]\n",
    "    ])\n",
    "else:\n",
    "    print(\"unknown label condensation\")\n",
    "    \n",
    "new_confident_examples = new_confident_examples.reset_index(drop=True)\n",
    "new_confident_examples = new_confident_examples.sample(frac=1, replace=False)\n",
    "new_confident_examples[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dbd814a9-a33f-4741-9d66-33f2d2f693dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_group_aug-trans-inferred2{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "new_confident_examples.to_csv(\n",
    "    Path(dst, fname),\n",
    "    index=False,\n",
    "    sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffd095-c5c1-49a8-b7e4-f2c0f84285b9",
   "metadata": {},
   "source": [
    "## Create training splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56228d09-408b-4239-9a3a-a60ef0e89291",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"confident_examples_group_aug-trans-inferred2{}.csv\"\\\n",
    "    .format(condensation_dataset_names[label_condensation])\n",
    "data = pd.read_csv(\n",
    "    Path(dst, fname),\n",
    "    dtype={\"tweet_id\":str, \"label\":int, \"text\":str},\n",
    "    delimiter=\";\"\n",
    ")\n",
    "test_frac = 0.15\n",
    "eval_frac = 0.15\n",
    "\n",
    "for s, seed in enumerate([42, 43, 44, 45, 46]):\n",
    "    # get the eval data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=eval_frac, \n",
    "        random_state=seed\n",
    "    )\n",
    "    sss.get_n_splits(data[\"text\"], data[\"label\"])\n",
    "    for tmp_index, eval_index in sss.split(data[\"text\"], data[\"label\"]):\n",
    "        X_tmp, X_eval = data[\"text\"].loc[tmp_index], data[\"text\"].loc[eval_index]\n",
    "        y_tmp, y_eval = data[\"label\"].loc[tmp_index], data[\"label\"].loc[eval_index]\n",
    "\n",
    "    tmp = pd.concat([X_tmp, y_tmp], axis=1).reset_index(drop=True)\n",
    "    evaldata = pd.concat([X_eval, y_eval], axis=1)\n",
    "\n",
    "    # get the test data\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=test_frac / (1 - eval_frac), \n",
    "        random_state=s + 10\n",
    "    )\n",
    "    sss.get_n_splits(tmp[\"text\"], tmp[\"label\"])\n",
    "    for train_index, test_index in sss.split(tmp[\"text\"], tmp[\"label\"]):\n",
    "        X_train, X_test = tmp[\"text\"].loc[train_index],\\\n",
    "                          tmp[\"text\"].loc[test_index]\n",
    "        y_train, y_test = tmp[\"label\"].loc[train_index],\\\n",
    "                          tmp[\"label\"].loc[test_index]\n",
    "\n",
    "        traindata = pd.concat([X_train, y_train], axis=1)\n",
    "        testdata = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    tmp_fname = \"confident_examples_group_aug-trans-inferred2{}\"\\\n",
    "        .format(condensation_dataset_names[label_condensation])\n",
    "    traindata.to_csv(Path(dst, tmp_fname + f\"_train_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    testdata.to_csv(Path(dst, tmp_fname + f\"_test_{s+1}.csv\"), index=False, sep=\";\")\n",
    "    evaldata.to_csv(Path(dst, tmp_fname + f\"_eval_{s+1}.csv\"), index=False, sep=\";\")\n",
    "data.to_csv(Path(dst, tmp_fname + \"_full.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba6429-30b4-446b-9b8f-77125f205efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rsync -avze ssh ../../../data/traindata/confident_examples_group_aug-trans-inferred2* jlasse@nvcluster:/home/jlasse/counterspeech-strategies/data/traindata/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
