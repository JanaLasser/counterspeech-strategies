{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "676e7995-c5d9-4e09-8f55-aaea897adc1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocess import Pool\n",
    "import torch\n",
    "from os.path import join\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "from transformers import XLMRobertaForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861426d-c545-41ba-9fb9-50d0acb0e330",
   "metadata": {},
   "source": [
    "See\n",
    "* https://towardsdatascience.com/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403\n",
    "* https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e\n",
    "* https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6073c589-d835-4d42-a224-bff9dd8abc65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't recognize argument -f\n"
     ]
    }
   ],
   "source": [
    "testing = False\n",
    "try:\n",
    "    arg = sys.argv[1]\n",
    "    if arg == \"test\":\n",
    "        testing = True\n",
    "    else:\n",
    "        print(\"couldn't recognize argument {}\".format(arg))\n",
    "except IndexError:\n",
    "    print('running in normal mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faa53d0e-9db4-46e8-a369-2fbba3fabc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RIDataset(torch.utils.data.Dataset):\n",
    "   \n",
    "    # only 6.8% of texts (Tweets) have more than 60 non-pad tokens\n",
    "    def __init__(self, texts, labels, tokenizer, seq_len=90):        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of dataset.\"\"\"\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])         \n",
    "        tokenized = self.tokenizer(\n",
    "            text,            \n",
    "            max_length = self.seq_len,                                \n",
    "            padding = \"max_length\",     # Pad to the specified max_length. \n",
    "            truncation = True,          # Truncate to the specified max_length. \n",
    "            add_special_tokens = True,  # Whether to insert [CLS], [SEP], <s>, etc.   \n",
    "            return_attention_mask = True            \n",
    "        )         \n",
    "        return {\"ids\": torch.tensor(tokenized[\"input_ids\"], dtype=torch.long),\n",
    "                \"masks\": torch.tensor(tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "                \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8228ad1f-aefc-403d-9bf2-0603dd612731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(predictions, labels):\n",
    "    return torch.nn.CrossEntropyLoss()(predictions, labels)\n",
    "\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):    \n",
    "    \n",
    "    model.train()                                # Put the model in training mode.              \n",
    "    \n",
    "    lr_list = []\n",
    "    train_losses = []         \n",
    "    \n",
    "    for batch in data_loader:                    # Loop over all batches.\n",
    "        \n",
    "        ids = batch[\"ids\"].to(device, dtype=torch.long)\n",
    "        masks = batch[\"masks\"].to(device, dtype=torch.long)\n",
    "        labels = batch[\"label\"].to(device, dtype=torch.long) \n",
    "        \n",
    "        optimizer.zero_grad()                    # To zero out the gradients.\n",
    "\n",
    "        outputs = model(ids, masks).logits       # Predictions from 1 batch of data.\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)         # Get the training loss.\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        loss.backward()                          # To backpropagate the error (gradients are computed).\n",
    "        optimizer.step()                         # To update parameters based on current gradients.\n",
    "        lr_list.append(optimizer.param_groups[0][\"lr\"])\n",
    "        scheduler.step()                         # To update learning rate.\n",
    "        \n",
    "    return train_losses, lr_list\n",
    "\n",
    "\n",
    "def validate_fn(data_loader, model, device):\n",
    "        \n",
    "    model.eval()                                    # Put model in evaluation mode.\n",
    "    \n",
    "    val_losses = []\n",
    "        \n",
    "    with torch.no_grad():                           # Disable gradient calculation.\n",
    "        \n",
    "        for batch in data_loader:                   # Loop over all batches.\n",
    "            \n",
    "            ids = batch[\"ids\"].to(device, dtype=torch.long)\n",
    "            masks = batch[\"masks\"].to(device, dtype=torch.long)\n",
    "            labels = batch[\"label\"].to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(ids, masks).logits      # Predictions from 1 batch of data.\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)        # Get the validation loss.\n",
    "            val_losses.append(loss.item())\n",
    "            \n",
    "    return val_losses \n",
    "\n",
    "\n",
    "def plot_train_val_losses(all_train_losses, all_val_losses, fold):\n",
    "    epochs = range(1, len(all_train_losses) + 1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs, all_train_losses, label='training loss')\n",
    "    ax.plot(epochs, all_val_losses, label='validation loss')\n",
    "    ax.set_title('Fold: {}, {}'.format(fold, model_name))\n",
    "    plt.savefig('losses_fold_{}.pdf'.format(fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0703e9e9-dd5e-46dd-b8a4-a8db1da2ee0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training(df, model_name):\n",
    "    \n",
    "    cv = []\n",
    "\n",
    "    for fold in FOLDS:\n",
    "\n",
    "        tokenizer =  XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "        \n",
    "        # Fetch training data\n",
    "        df_train = df[df[\"fold\"] != fold].reset_index(drop=True)\n",
    "\n",
    "        # Fetch validation data\n",
    "        df_val = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "        # Initialize training dataset\n",
    "        train_dataset = RIDataset(texts = df_train[\"text\"].values,\n",
    "                                  labels = df_train[\"label\"].values,\n",
    "                                  tokenizer = tokenizer)\n",
    "\n",
    "        # Initialize validation dataset\n",
    "        val_dataset = RIDataset(texts = df_val[\"text\"].values,\n",
    "                                labels = df_val[\"label\"].values,\n",
    "                                tokenizer = tokenizer)\n",
    "\n",
    "        # Create training dataloader\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size = TRAIN_BS,\n",
    "                                       shuffle = True, num_workers = 2)\n",
    "\n",
    "        # Create validation dataloader\n",
    "        val_data_loader = DataLoader(val_dataset, batch_size = VAL_BS,\n",
    "                                     shuffle = False, num_workers = 2)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = XLMRobertaForSequenceClassification\\\n",
    "                .from_pretrained(model_name, num_labels=2)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = transformers.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "        train_steps = int(len(df_train) / TRAIN_BS * EPOCHS) \n",
    "\n",
    "        scheduler = transformers.get_scheduler(\n",
    "            \"linear\",    # Create a schedule with a learning rate that decreases linearly \n",
    "                         # from the initial learning rate set in the optimizer to 0.\n",
    "            optimizer = optimizer,\n",
    "            num_warmup_steps = 0,\n",
    "            num_training_steps = train_steps)\n",
    "\n",
    "\n",
    "        print(f\"===== FOLD: {fold} =====\")    \n",
    "        #best_rmse = 999\n",
    "        #early_stopping_counter = 0       \n",
    "        all_train_losses = []\n",
    "        all_val_losses = []\n",
    "        all_lr = []\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "\n",
    "            # Call the train function and get the training loss\n",
    "            train_losses, lr_list = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "            train_loss = np.mean(train_losses)   \n",
    "            all_train_losses.append(train_loss)\n",
    "            all_lr.extend(lr_list)\n",
    "\n",
    "            # Perform validation and get the validation loss\n",
    "            val_losses = validate_fn(val_data_loader, model, device)\n",
    "            val_loss = np.mean(val_losses)\n",
    "            all_val_losses.append(val_loss) \n",
    "\n",
    "\n",
    "        # Plot the losses and learning rate schedule.\n",
    "        plot_train_val_losses(all_train_losses, all_val_losses, fold)\n",
    "\n",
    "                \n",
    "    # Print the cross validation scores and their average.\n",
    "    cv_rounded = [ round(elem, 4) for elem in cv ] \n",
    "    print(f\"CV: {cv_rounded}\") \n",
    "    print(f\"Average CV: {round(np.mean(cv), 4)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f11512f3-1d04-42a2-ae31-1fc21d77822a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at models/twitter-xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FOLD: 0 =====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_261876/1151481342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N rows: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_261876/1277582107.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(df, model_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# Call the train function and get the training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mall_train_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_261876/1144551531.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                          \u001b[0;31m# To backpropagate the error (gradients are computed).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                         \u001b[0;31m# To update parameters based on current gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlr_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ri/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ri/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FOLDS = [0, 1, 2, 3, 4]\n",
    "TRAIN_BS = 10\n",
    "VAL_BS = 100\n",
    "EPOCHS = 5\n",
    "data_frac = 1\n",
    "model_name = \"models/twitter-xlm-roberta-base\"\n",
    "\n",
    "#testing=True\n",
    "if testing:\n",
    "    FOLDS = FOLDS[0:1]\n",
    "    EPOCHS = 1\n",
    "    data_frac = 0.0004\n",
    "    \n",
    "src = '../../data/traindata'\n",
    "df = pd.read_csv(join(src, 'dataset_DE_train.csv'))\n",
    "df['label'] = df['label'].replace({'hate':1, 'counter':0})\n",
    "df = df.drop(columns=['id'])\n",
    "df = df.sample(frac=data_frac, random_state=42).reset_index(drop=True)\n",
    "k = 5\n",
    "N = len(df)\n",
    "fold_size = int(N / k)\n",
    "for fold in range(k):\n",
    "    df.loc[fold_size * fold:fold_size * (fold + 1), \"fold\"] = fold\n",
    "df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "print('N rows: {}'.format(len(df)))\n",
    "\n",
    "run_training(df, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5e125-0370-450a-b657-89c41fc72a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
