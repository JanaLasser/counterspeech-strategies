{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "676e7995-c5d9-4e09-8f55-aaea897adc1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocess import Pool\n",
    "import torch\n",
    "from os.path import join\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "from transformers import XLMRobertaForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861426d-c545-41ba-9fb9-50d0acb0e330",
   "metadata": {},
   "source": [
    "See\n",
    "* https://towardsdatascience.com/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403\n",
    "* https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e\n",
    "* https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6073c589-d835-4d42-a224-bff9dd8abc65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = \"test\"\n",
    "try:\n",
    "    mode = sys.argv[1]\n",
    "except IndexError:\n",
    "    print(\"no mode supplied!\")\n",
    "\n",
    "batch_size = 512\n",
    "try:\n",
    "    batch_size = int(sys.argv[2])\n",
    "except IndexError:\n",
    "    print(\"no batch size supplied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faa53d0e-9db4-46e8-a369-2fbba3fabc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RIDataset(torch.utils.data.Dataset):\n",
    "   \n",
    "    # only 6.8% of texts (Tweets) have more than 60 non-pad tokens\n",
    "    def __init__(self, texts, labels, tokenizer, seq_len=90):        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of dataset.\"\"\"\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])         \n",
    "        tokenized = self.tokenizer(\n",
    "            text,            \n",
    "            max_length = self.seq_len,                                \n",
    "            padding = \"max_length\",     # Pad to the specified max_length. \n",
    "            truncation = True,          # Truncate to the specified max_length. \n",
    "            add_special_tokens = True,  # Whether to insert [CLS], [SEP], <s>, etc.   \n",
    "            return_attention_mask = True            \n",
    "        )         \n",
    "        return {\"ids\": torch.tensor(tokenized[\"input_ids\"], dtype=torch.long),\n",
    "                \"masks\": torch.tensor(tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "                \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8228ad1f-aefc-403d-9bf2-0603dd612731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(predictions, labels):\n",
    "    return torch.nn.CrossEntropyLoss()(predictions, labels)\n",
    "\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):    \n",
    "    \n",
    "    model.train()                                # Put the model in training mode.              \n",
    "    \n",
    "    lr_list = []\n",
    "    train_losses = []         \n",
    "    \n",
    "    for batch in tqdm(data_loader, total=len(data_loader)):                    # Loop over all batches.\n",
    "        \n",
    "        ids = batch[\"ids\"].to(device, dtype=torch.long)\n",
    "        masks = batch[\"masks\"].to(device, dtype=torch.long)\n",
    "        labels = batch[\"label\"].to(device, dtype=torch.long) \n",
    "        \n",
    "        optimizer.zero_grad()                    # To zero out the gradients.\n",
    "\n",
    "        outputs = model(ids, masks).logits       # Predictions from 1 batch of data.\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)         # Get the training loss.\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        loss.backward()                          # To backpropagate the error (gradients are computed).\n",
    "        optimizer.step()                         # To update parameters based on current gradients.\n",
    "        lr_list.append(optimizer.param_groups[0][\"lr\"])\n",
    "        scheduler.step()                         # To update learning rate.\n",
    "        \n",
    "    return train_losses, lr_list\n",
    "\n",
    "\n",
    "def validate_fn(data_loader, model, device):\n",
    "        \n",
    "    model.eval()                                    # Put model in evaluation mode.\n",
    "    \n",
    "    val_losses = []\n",
    "        \n",
    "    with torch.no_grad():                           # Disable gradient calculation.\n",
    "        \n",
    "        for batch in tqdm(data_loader, total=len(data_loader)):                   # Loop over all batches.\n",
    "            \n",
    "            ids = batch[\"ids\"].to(device, dtype=torch.long)\n",
    "            masks = batch[\"masks\"].to(device, dtype=torch.long)\n",
    "            labels = batch[\"label\"].to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(ids, masks).logits      # Predictions from 1 batch of data.\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)        # Get the validation loss.\n",
    "            val_losses.append(loss.item())\n",
    "            \n",
    "    return val_losses \n",
    "\n",
    "\n",
    "def plot_train_val_losses(all_train_losses, all_val_losses, fold=None):\n",
    "    epochs = range(1, len(all_train_losses) + 1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs, all_train_losses, label='training loss')\n",
    "    ax.plot(epochs, all_val_losses, label='validation loss')\n",
    "    ax.legend()\n",
    "    if fold != None:\n",
    "        ax.set_title('Fold: {}, {}'.format(fold, model_name))\n",
    "        plt.savefig('losses_fold_{}.pdf'.format(fold))\n",
    "    else:\n",
    "        plt.savefig('losses.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0703e9e9-dd5e-46dd-b8a4-a8db1da2ee0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training_crossval(df, model_name):\n",
    "    \n",
    "    cv = []\n",
    "\n",
    "    for fold in FOLDS:\n",
    "\n",
    "        tokenizer =  XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "        \n",
    "        # Fetch training data\n",
    "        df_train = df[df[\"fold\"] != fold].reset_index(drop=True)\n",
    "\n",
    "        # Fetch validation data\n",
    "        df_val = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "        # Initialize training dataset\n",
    "        train_dataset = RIDataset(texts = df_train[\"text\"].values,\n",
    "                                  labels = df_train[\"label\"].values,\n",
    "                                  tokenizer = tokenizer)\n",
    "\n",
    "        # Initialize validation dataset\n",
    "        val_dataset = RIDataset(texts = df_val[\"text\"].values,\n",
    "                                labels = df_val[\"label\"].values,\n",
    "                                tokenizer = tokenizer)\n",
    "\n",
    "        # Create training dataloader\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size = TRAIN_BS,\n",
    "                                       shuffle = True, num_workers = 2)\n",
    "\n",
    "        # Create validation dataloader\n",
    "        val_data_loader = DataLoader(val_dataset, batch_size = VAL_BS,\n",
    "                                     shuffle = False, num_workers = 2)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = XLMRobertaForSequenceClassification\\\n",
    "                .from_pretrained(model_name, num_labels=2)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = transformers.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "        train_steps = int(len(df_train) / TRAIN_BS * EPOCHS) \n",
    "\n",
    "        scheduler = transformers.get_scheduler(\n",
    "            \"linear\",    # Create a schedule with a learning rate that decreases linearly \n",
    "                         # from the initial learning rate set in the optimizer to 0.\n",
    "            optimizer = optimizer,\n",
    "            num_warmup_steps = 0,\n",
    "            num_training_steps = train_steps)\n",
    "\n",
    "\n",
    "        print(f\"===== FOLD: {fold} =====\")    \n",
    "        #best_rmse = 999\n",
    "        #early_stopping_counter = 0       \n",
    "        all_train_losses = []\n",
    "        all_val_losses = []\n",
    "        all_lr = []\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            print(f\"\\t===== EPOCH: {epoch} =====\")   \n",
    "\n",
    "            # Call the train function and get the training loss\n",
    "            train_losses, lr_list = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "            train_loss = np.mean(train_losses)   \n",
    "            all_train_losses.append(train_loss)\n",
    "            all_lr.extend(lr_list)\n",
    "\n",
    "            # Perform validation and get the validation loss\n",
    "            val_losses = validate_fn(val_data_loader, model, device)\n",
    "            val_loss = np.mean(val_losses)\n",
    "            all_val_losses.append(val_loss) \n",
    "\n",
    "\n",
    "        # Plot the losses and learning rate schedule.\n",
    "        plot_train_val_losses(all_train_losses, all_val_losses, fold)\n",
    "\n",
    "                \n",
    "    # Print the cross validation scores and their average.\n",
    "    cv_rounded = [ round(elem, 4) for elem in cv ] \n",
    "    print(f\"CV: {cv_rounded}\") \n",
    "    print(f\"Average CV: {round(np.mean(cv), 4)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a405c873-2e94-4376-bb86-00cf6f640e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training(df, model_name):\n",
    "\n",
    "    tokenizer =  XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "    # Fetch training data\n",
    "    df_train = df[df[\"train\"] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Fetch validation data\n",
    "    df_val = df[df[\"train\"] == 0].reset_index(drop=True)\n",
    "\n",
    "    # Initialize training dataset\n",
    "    train_dataset = RIDataset(texts = df_train[\"text\"].values,\n",
    "                              labels = df_train[\"label\"].values,\n",
    "                              tokenizer = tokenizer)\n",
    "\n",
    "    # Initialize validation dataset\n",
    "    val_dataset = RIDataset(texts = df_val[\"text\"].values,\n",
    "                            labels = df_val[\"label\"].values,\n",
    "                            tokenizer = tokenizer)\n",
    "\n",
    "    # Create training dataloader\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size = TRAIN_BS,\n",
    "                                   shuffle = True, num_workers = 2)\n",
    "\n",
    "    # Create validation dataloader\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size = VAL_BS,\n",
    "                                 shuffle = False, num_workers = 2)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = XLMRobertaForSequenceClassification\\\n",
    "            .from_pretrained(model_name, num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = transformers.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "    train_steps = int(len(df_train) / TRAIN_BS * EPOCHS) \n",
    "\n",
    "    scheduler = transformers.get_scheduler(\n",
    "        \"linear\",    # Create a schedule with a learning rate that decreases linearly \n",
    "                     # from the initial learning rate set in the optimizer to 0.\n",
    "        optimizer = optimizer,\n",
    "        num_warmup_steps = 0,\n",
    "        num_training_steps = train_steps)\n",
    "   \n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_lr = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\t===== EPOCH: {epoch} =====\")   \n",
    "\n",
    "        # Call the train function and get the training loss\n",
    "        train_losses, lr_list = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        train_loss = np.mean(train_losses)   \n",
    "        all_train_losses.append(train_loss)\n",
    "        all_lr.extend(lr_list)\n",
    "\n",
    "        # Perform validation and get the validation loss\n",
    "        val_losses = validate_fn(val_data_loader, model, device)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        all_val_losses.append(val_loss) \n",
    "\n",
    "\n",
    "    # Plot the losses and learning rate schedule.\n",
    "    plot_train_val_losses(all_train_losses, all_val_losses, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f11512f3-1d04-42a2-ae31-1fc21d77822a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows: 100\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_261876/1897963277.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mN_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ri/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ri/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtake_split_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m             \u001b[0;31m# We have to operate column-wise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ri/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1815\u001b[0m             \u001b[0;31m# scalar value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0milocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_with_indexer_2d_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ri/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_single_column\u001b[0;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[1;32m   1896\u001b[0m         \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplane_indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m         \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0;31m# perform the equivalent of a setitem on the info axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ri/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3392\u001b[0m         \u001b[0;31m# icol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3394\u001b[0;31m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3396\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ri/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4603\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4604\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "FOLDS = [0, 1, 2, 3, 4]\n",
    "EPOCHS = 5\n",
    "data_frac = 1\n",
    "model_name = \"models/twitter-xlm-roberta-base\"\n",
    "\n",
    "#mode = \"test\"\n",
    "#batch_size = 10\n",
    "TRAIN_BS = batch_size\n",
    "VAL_BS = batch_size\n",
    "\n",
    "src = '../../data/traindata'\n",
    "df = pd.read_csv(join(src, 'dataset_DE_train.csv'))\n",
    "df['label'] = df['label'].replace({'hate':1, 'counter':0})\n",
    "df = df.drop(columns=['id'])\n",
    "\n",
    "if mode == \"test\":\n",
    "    FOLDS = FOLDS[0:1]\n",
    "    EPOCHS = 1\n",
    "    data_frac = 0.0001\n",
    "    df = df.sample(frac=data_frac, random_state=42).reset_index(drop=True)\n",
    "    print('N rows: {}'.format(len(df)))\n",
    "    \n",
    "    N_val = int(len(df)/5)\n",
    "    df[\"train\"] = 1\n",
    "    df.iloc[slice(0,N_val), \"train\"] = 0\n",
    "    \n",
    "    run_training(df, model_name)\n",
    "    \n",
    "if mode == \"prototype\":\n",
    "    data_frac = 0.01\n",
    "    df = df.sample(frac=data_frac, random_state=42).reset_index(drop=True)\n",
    "    print('N rows: {}'.format(len(df)))\n",
    "    N_val = int(len(df)/5)\n",
    "    df[\"train\"] = 1\n",
    "    df.iloc[0:N_val, \"train\"] = 0\n",
    "    \n",
    "    run_training(df, model_name)\n",
    "    \n",
    "if mode == \"train\":\n",
    "    k = len(FOLDS)\n",
    "    N = len(df)\n",
    "    fold_size = int(N / k)\n",
    "    for fold in range(k):\n",
    "        df.loc[fold_size * fold:fold_size * (fold + 1), \"fold\"] = fold\n",
    "    df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "    print('N rows: {}'.format(len(df)))\n",
    "    \n",
    "    run_training_crossval(df, model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71274679-6aee-4c7e-b672-5c5ca7d32ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138220b-8dd1-4966-86f3-228d9ee27240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
