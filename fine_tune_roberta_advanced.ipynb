{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "676e7995-c5d9-4e09-8f55-aaea897adc1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from multiprocess import Pool\n",
    "import torch\n",
    "from os.path import join\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "from transformers import XLMRobertaForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861426d-c545-41ba-9fb9-50d0acb0e330",
   "metadata": {},
   "source": [
    "See\n",
    "* https://towardsdatascience.com/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403\n",
    "* https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e\n",
    "* https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6073c589-d835-4d42-a224-bff9dd8abc65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = \"test\"\n",
    "try:\n",
    "    mode = sys.argv[1]\n",
    "except IndexError:\n",
    "    print(\"no mode supplied!\")\n",
    "\n",
    "batch_size = 512\n",
    "try:\n",
    "    batch_size = int(sys.argv[2])\n",
    "except IndexError:\n",
    "    print(\"no batch size supplied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faa53d0e-9db4-46e8-a369-2fbba3fabc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RIDataset(torch.utils.data.Dataset):\n",
    "   \n",
    "    # only 6.8% of texts (Tweets) have more than 60 non-pad tokens\n",
    "    def __init__(self, texts, labels, tokenizer, seq_len=90):        \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of dataset.\"\"\"\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])         \n",
    "        tokenized = self.tokenizer(\n",
    "            text,            \n",
    "            max_length = self.seq_len,                                \n",
    "            padding = \"max_length\",     # Pad to the specified max_length. \n",
    "            truncation = True,          # Truncate to the specified max_length. \n",
    "            add_special_tokens = True,  # Whether to insert [CLS], [SEP], <s>, etc.   \n",
    "            return_attention_mask = True            \n",
    "        )         \n",
    "        return {\"ids\": torch.tensor(tokenized[\"input_ids\"], dtype=torch.long),\n",
    "                \"masks\": torch.tensor(tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "                \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8228ad1f-aefc-403d-9bf2-0603dd612731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(predictions, labels):\n",
    "    return torch.nn.CrossEntropyLoss()(predictions, labels)\n",
    "\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):    \n",
    "    \n",
    "    model.train()                                # Put the model in training mode.              \n",
    "    \n",
    "    lr_list = []\n",
    "    train_losses = []         \n",
    "    \n",
    "    for batch in tqdm(data_loader, total=len(data_loader)):                    # Loop over all batches.\n",
    "        \n",
    "        ids = batch[\"ids\"].to(device, dtype=torch.long)\n",
    "        masks = batch[\"masks\"].to(device, dtype=torch.long)\n",
    "        labels = batch[\"label\"].to(device, dtype=torch.long) \n",
    "        \n",
    "        optimizer.zero_grad()                    # To zero out the gradients.\n",
    "\n",
    "        outputs = model(ids, masks).logits       # Predictions from 1 batch of data.\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)         # Get the training loss.\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        loss.backward()                          # To backpropagate the error (gradients are computed).\n",
    "        optimizer.step()                         # To update parameters based on current gradients.\n",
    "        lr_list.append(optimizer.param_groups[0][\"lr\"])\n",
    "        scheduler.step()                         # To update learning rate.\n",
    "        \n",
    "    return train_losses, lr_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(logits, labels):\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    performance = metric.compute(predictions=predictions, references=labels)\n",
    "    print(performance)\n",
    "    return performance[METRIC]\n",
    "\n",
    "\n",
    "def validate_fn(data_loader, model, device):\n",
    "        \n",
    "    model.eval()                                    # Put model in evaluation mode.\n",
    "    \n",
    "    val_losses = []\n",
    "    performance_metric = []\n",
    "        \n",
    "    with torch.no_grad():                           # Disable gradient calculation.\n",
    "        \n",
    "        for batch in tqdm(data_loader, total=len(data_loader)):                   # Loop over all batches.\n",
    "            \n",
    "            ids = batch[\"ids\"].to(device, dtype=torch.long)\n",
    "            masks = batch[\"masks\"].to(device, dtype=torch.long)\n",
    "            labels = batch[\"label\"].to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(ids, masks).logits      # Predictions from 1 batch of data.\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)        # Get the validation loss.\n",
    "            val_losses.append(loss.item())\n",
    "            \n",
    "            performance = compute_metrics(outputs, labels)\n",
    "            performance_metric.append(performance)\n",
    "            \n",
    "    return val_losses, performance_metric\n",
    "\n",
    "\n",
    "def plot_train_val_losses(all_train_losses, all_val_losses, fold=None):\n",
    "    epochs = range(1, len(all_train_losses) + 1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(epochs, all_train_losses, label='training loss')\n",
    "    ax.plot(epochs, all_val_losses, label='validation loss')\n",
    "    ax.legend()\n",
    "    if fold != None:\n",
    "        ax.set_title('Fold: {}, {}'.format(fold, model_name))\n",
    "        plt.savefig('losses_fold_{}.pdf'.format(fold))\n",
    "    else:\n",
    "        plt.savefig('losses.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703e9e9-dd5e-46dd-b8a4-a8db1da2ee0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training_crossval(df, model_name):\n",
    "    \n",
    "    cv = []\n",
    "\n",
    "    for fold in FOLDS:\n",
    "\n",
    "        tokenizer =  XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "        \n",
    "        # Fetch training data\n",
    "        df_train = df[df[\"fold\"] != fold].reset_index(drop=True)\n",
    "\n",
    "        # Fetch validation data\n",
    "        df_val = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "        # Initialize training dataset\n",
    "        train_dataset = RIDataset(texts = df_train[\"text\"].values,\n",
    "                                  labels = df_train[\"label\"].values,\n",
    "                                  tokenizer = tokenizer)\n",
    "\n",
    "        # Initialize validation dataset\n",
    "        val_dataset = RIDataset(texts = df_val[\"text\"].values,\n",
    "                                labels = df_val[\"label\"].values,\n",
    "                                tokenizer = tokenizer)\n",
    "\n",
    "        # Create training dataloader\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size = TRAIN_BS,\n",
    "                                       shuffle = True, num_workers = 2)\n",
    "\n",
    "        # Create validation dataloader\n",
    "        val_data_loader = DataLoader(val_dataset, batch_size = VAL_BS,\n",
    "                                     shuffle = False, num_workers = 2)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = XLMRobertaForSequenceClassification\\\n",
    "                .from_pretrained(model_name, num_labels=2)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = transformers.AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "        train_steps = int(len(df_train) / TRAIN_BS * EPOCHS) \n",
    "\n",
    "        scheduler = transformers.get_scheduler(\n",
    "            \"linear\",    # Create a schedule with a learning rate that decreases linearly \n",
    "                         # from the initial learning rate set in the optimizer to 0.\n",
    "            optimizer = optimizer,\n",
    "            num_warmup_steps = 0,\n",
    "            num_training_steps = train_steps)\n",
    "\n",
    "\n",
    "        print(f\"===== FOLD: {fold} =====\")    \n",
    "        #best_rmse = 999\n",
    "        #early_stopping_counter = 0       \n",
    "        all_train_losses = []\n",
    "        all_val_losses = []\n",
    "        all_lr = []\n",
    "        all_performances = []\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            print(f\"\\t===== EPOCH: {epoch} =====\")   \n",
    "\n",
    "            # Call the train function and get the training loss\n",
    "            train_losses, lr_list = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "            train_loss = np.mean(train_losses)   \n",
    "            all_train_losses.append(train_loss)\n",
    "            all_lr.extend(lr_list)\n",
    "\n",
    "            # Perform validation and get the validation loss\n",
    "            val_losses, performance_metric = \\\n",
    "                validate_fn(val_data_loader, model, device)\n",
    "            val_loss = np.mean(val_losses)\n",
    "            performance = np.mean(performance_metric)\n",
    "            all_val_losses.append(val_loss) \n",
    "            all_performances.append(performance)\n",
    "            print('{}: {}'.format(METRIC, performance))\n",
    "\n",
    "\n",
    "        # Plot the losses and learning rate schedule.\n",
    "        plot_train_val_losses(all_train_losses, all_val_losses, fold)\n",
    "        torch.save(model.state_dict(),\n",
    "                       'trained_models/twitter-xlm-roberta-base_nt-{}_nv-{}_e-{}_f-{}_bs-{}_lr-{}.pth'\\\n",
    "                       .format(len(train_dataset), len(val_dataset), len(EPOCHS),\n",
    "                               len(FOLDS), TRAIN_BS, LEARNING_RATE))\n",
    "\n",
    "                \n",
    "    # Print the cross validation scores and their average.\n",
    "    cv_rounded = [ round(elem, 4) for elem in cv ] \n",
    "    print(f\"CV: {cv_rounded}\") \n",
    "    print(f\"Average CV: {round(np.mean(cv), 4)}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a405c873-2e94-4376-bb86-00cf6f640e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training(df, model_name):\n",
    "\n",
    "    tokenizer =  XLMRobertaTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "    # Fetch training data\n",
    "    df_train = df[df[\"train\"] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Fetch validation data\n",
    "    df_val = df[df[\"train\"] == 0].reset_index(drop=True)\n",
    "\n",
    "    # Initialize training dataset\n",
    "    train_dataset = RIDataset(texts = df_train[\"text\"].values,\n",
    "                              labels = df_train[\"label\"].values,\n",
    "                              tokenizer = tokenizer)\n",
    "\n",
    "    # Initialize validation dataset\n",
    "    val_dataset = RIDataset(texts = df_val[\"text\"].values,\n",
    "                            labels = df_val[\"label\"].values,\n",
    "                            tokenizer = tokenizer)\n",
    "\n",
    "    # Create training dataloader\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size = TRAIN_BS,\n",
    "                                   shuffle = True, num_workers = 2)\n",
    "\n",
    "    # Create validation dataloader\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size = VAL_BS,\n",
    "                                 shuffle = False, num_workers = 2)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = XLMRobertaForSequenceClassification\\\n",
    "            .from_pretrained(model_name, num_labels=2)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = transformers.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_steps = int(len(df_train) / TRAIN_BS * EPOCHS) \n",
    "\n",
    "    scheduler = transformers.get_scheduler(\n",
    "        \"linear\",    # Create a schedule with a learning rate that decreases linearly \n",
    "                     # from the initial learning rate set in the optimizer to 0.\n",
    "        optimizer = optimizer,\n",
    "        num_warmup_steps = 0,\n",
    "        num_training_steps = train_steps)\n",
    "   \n",
    "    all_train_losses = []\n",
    "    all_val_losses = []\n",
    "    all_lr = []\n",
    "    all_performances = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\t===== EPOCH: {epoch} =====\")   \n",
    "\n",
    "        # Call the train function and get the training loss\n",
    "        train_losses, lr_list = train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        train_loss = np.mean(train_losses)   \n",
    "        all_train_losses.append(train_loss)\n",
    "        all_lr.extend(lr_list)\n",
    "\n",
    "        # Perform validation and get the validation loss\n",
    "        val_losses, performance_metric = \\\n",
    "            validate_fn(val_data_loader, model, device)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        performance = np.mean(performance_metric)\n",
    "        all_val_losses.append(val_loss) \n",
    "        all_performances.append(performance)\n",
    "        \n",
    "        print('{}: {}'.format(METRIC, performance))\n",
    "\n",
    "\n",
    "    # Plot the losses and learning rate schedule.\n",
    "    plot_train_val_losses(all_train_losses, all_val_losses)\n",
    "    torch.save(model.state_dict(),\n",
    "               'trained_models/twitter-xlm-roberta-base_nt-{}_nv-{}_e-{}_f-{}_bs-{}_lr-{}.pth'\\\n",
    "               .format(len(train_dataset), len(val_dataset), EPOCHS,\n",
    "                       len(FOLDS), TRAIN_BS, LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f11512f3-1d04-42a2-ae31-1fc21d77822a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at models/twitter-xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_261876/1012158438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"prototype\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_261876/3861989332.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(df, model_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrain_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mTRAIN_BS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     scheduler = transformers.get_scheduler(\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "FOLDS = [0, 1, 2, 3, 4]\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-6\n",
    "data_frac = 1\n",
    "model_name = \"models/twitter-xlm-roberta-base\"\n",
    "METRIC = \"f1\"\n",
    "metric = load_metric(METRIC)\n",
    "\n",
    "mode = \"test\"\n",
    "batch_size = 10\n",
    "TRAIN_BS = batch_size\n",
    "VAL_BS = batch_size\n",
    "\n",
    "src = '../../data/traindata'\n",
    "df = pd.read_csv(join(src, 'dataset_DE_train.csv'))\n",
    "df['label'] = df['label'].replace({'hate':1, 'counter':0})\n",
    "df = df.drop(columns=['id'])\n",
    "\n",
    "if mode == \"test\":\n",
    "    FOLDS = FOLDS[0:1]\n",
    "    EPOCHS = 1\n",
    "    data_frac = 0.0001\n",
    "    df = df.sample(frac=data_frac, random_state=42).reset_index(drop=True)\n",
    "    print('N rows: {}'.format(len(df)))\n",
    "    \n",
    "    N_val = int(len(df)/5)\n",
    "    df[\"train\"] = 1\n",
    "    df.loc[0:N_val, (\"train\")] = 0\n",
    "    \n",
    "    run_training(df, model_name)\n",
    "    \n",
    "if mode == \"prototype\":\n",
    "    FOLDS = FOLDS[0:1]\n",
    "    data_frac = 0.01\n",
    "    df = df.sample(frac=data_frac, random_state=42).reset_index(drop=True)\n",
    "    print('N rows: {}'.format(len(df)))\n",
    "    N_val = int(len(df)/5)\n",
    "    df[\"train\"] = 1\n",
    "    df.loc[0:N_val, (\"train\")] = 0\n",
    "    \n",
    "    run_training(df, model_name)\n",
    "    \n",
    "if mode == \"train\":\n",
    "    k = len(FOLDS)\n",
    "    N = len(df)\n",
    "    fold_size = int(N / k)\n",
    "    for fold in range(k):\n",
    "        df.loc[fold_size * fold:fold_size * (fold + 1), \"fold\"] = fold\n",
    "    df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "    print('N rows: {}'.format(len(df)))\n",
    "    \n",
    "    run_training_crossval(df, model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71274679-6aee-4c7e-b672-5c5ca7d32ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138220b-8dd1-4966-86f3-228d9ee27240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
