{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4f0224-4a49-4081-b533-56771e84e38c",
   "metadata": {},
   "source": [
    "# Fine-tune an XLM RoBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97adbb8e-4ace-418c-b2a3-cc188c5fd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from torch import cuda\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4abcb3-9f32-4733-aba0-49c2ee8645d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-5\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "MODEL = \"models/twitter-xlm-roberta-base\"\n",
    "MAX_TRAINING_EXAMPLES = 100\n",
    "MAX_TEST_EXAMPLES = int(MAX_TRAINING_EXAMPLES/2)\n",
    "MAX_VAL_EXAMPLES = int(MAX_TEST_EXAMPLES/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b086c6-a9e8-448e-aeaf-e039aa183851",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "for i in ['train','val','test']:\n",
    "    dataset_dict[i] = {}\n",
    "    for j in ['text','labels']:\n",
    "        dataset_dict[i][j] = open(f\"../../data/traindata/{i}_{j}.txt\")\\\n",
    "            .read().strip('\\n').split('\\n')\n",
    "        if j == 'labels':\n",
    "            dataset_dict[i][j] = [int(x) for x in dataset_dict[i][j]]\n",
    "\n",
    "if MAX_TRAINING_EXAMPLES > 0:\n",
    "    dataset_dict['train']['text'] = \\\n",
    "        dataset_dict['train']['text'][:MAX_TRAINING_EXAMPLES]\n",
    "    dataset_dict['train']['labels'] = \\\n",
    "        dataset_dict['train']['labels'][:MAX_TRAINING_EXAMPLES]\n",
    "    \n",
    "if MAX_TEST_EXAMPLES > 0:\n",
    "    dataset_dict['test']['text'] = \\\n",
    "        dataset_dict['test']['text'][:MAX_TEST_EXAMPLES]\n",
    "    dataset_dict['test']['labels'] = \\\n",
    "        dataset_dict['test']['labels'][:MAX_TEST_EXAMPLES]\n",
    "\n",
    "if MAX_VAL_EXAMPLES > 0:\n",
    "    dataset_dict['val']['text'] = \\\n",
    "        dataset_dict['val']['text'][:MAX_VAL_EXAMPLES]\n",
    "    dataset_dict['val']['labels'] = \\\n",
    "        dataset_dict['val']['labels'][:MAX_VAL_EXAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117a35dc-e1b1-47ac-a149-535bd9110054",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce94cc17-3d0f-4c76-804b-0ac4114af612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(dataset_dict['train']['text'], truncation=True, padding=True)\n",
    "val_encodings = tokenizer(dataset_dict['val']['text'], truncation=True, padding=True)\n",
    "test_encodings = tokenizer(dataset_dict['test']['text'], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b40f0b4-e775-4952-a020-af5ea4c5d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MyDataset(train_encodings, dataset_dict['train']['labels'])\n",
    "val_dataset = MyDataset(val_encodings, dataset_dict['val']['labels'])\n",
    "test_dataset = MyDataset(test_encodings, dataset_dict['test']['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e59f074-39b2-4924-88cd-173df9826d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file models/twitter-xlm-roberta-base/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"/home/jupyter/misc/tweeteval/TweetEval_models/xlm-twitter/twitter-xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file models/twitter-xlm-roberta-base/pytorch_model.bin\n",
      "Some weights of the model checkpoint at models/twitter-xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at models/twitter-xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                   # output directory\n",
    "    num_train_epochs=EPOCHS,                  # total number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # batch size for evaluation\n",
    "    warmup_steps=100,                         # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                        # strength of weight decay\n",
    "    logging_dir='./logs',                     # directory for storing logs\n",
    "    logging_steps=10,                         # when to print log\n",
    "    load_best_model_at_end=True,              # load or not best model at the end\n",
    "    save_strategy='no',\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "num_labels = len(set(dataset_dict[\"train\"][\"labels\"]))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00a2d360-7057-454b-9dd9-c1ae70095c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"f1\")\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbbcb9ea-1380-4c13-823d-9a5086ae92eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=0.6951256990432739, metrics={'train_runtime': 29.1294, 'train_samples_per_second': 3.433, 'train_steps_per_second': 0.137, 'total_flos': 5447221068000.0, 'train_loss': 0.6951256990432739, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        #device = device,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = val_dataset,\n",
    "        #data_collator = data_collator,\n",
    "        #tokenizer = tokenizer,\n",
    "        compute_metrics = compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c485b50-770b-4c2e-a795-e7d35ff013ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/best_model\n",
      "Configuration saved in ./results/best_model/config.json\n",
      "Model weights saved in ./results/best_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./results/best_model\") # save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc3efc3e-707a-4a1e-bb13-31124eb45e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 50\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.360     1.000     0.529        18\n",
      "           1      0.000     0.000     0.000        32\n",
      "\n",
      "    accuracy                          0.360        50\n",
      "   macro avg      0.180     0.500     0.265        50\n",
      "weighted avg      0.130     0.360     0.191        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jana/anaconda3/envs/ri/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jana/anaconda3/envs/ri/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jana/anaconda3/envs/ri/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_preds_raw, test_labels , _ = trainer.predict(test_dataset)\n",
    "test_preds = np.argmax(test_preds_raw, axis=-1)\n",
    "print(classification_report(test_labels, test_preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d037c53-737e-4a57-b13e-b06825b2f2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
