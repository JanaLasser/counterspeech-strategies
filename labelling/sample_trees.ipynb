{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e791908e-3a28-4dbd-b498-a5ed00e7a818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jana/anaconda3/envs/nlp/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "sys.path.append(\"../\") \n",
    "from germanhass.ReplyTrees.ReplyTreeWalker import ReplyTreeWalker\n",
    "from germanhass.DBCode.HassDBAdapter import HassDBAdapter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac20969a-c12e-4e27-8fd9-13ae144676ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = HassDBAdapter()\n",
    "node_cut_off = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cdeef-71d0-46f0-ac9a-4fd79a22354d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37984d83-5777-48fe-9388-9a854c6c5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nodes(node,count):\n",
    "    count +=1\n",
    "    return count\n",
    "\n",
    "def print_nodes(node):\n",
    "    print(node)\n",
    "\n",
    "def build_dict(node,parameters):\n",
    "    parameters[1][\"year\"].append(node[\"created_at\"][-4:])\n",
    "    parameters[1][\"tree_id\"].append(parameters[0])\n",
    "    parameters[1][\"tweet_id\"].append(node[\"_id\"])\n",
    "    parameters[1][\"hate_score\"].append(node['scores']['hate'])\n",
    "    parameters[1][\"counter_score\"].append(node['scores']['counter'])\n",
    "    parameters[1][\"text\"].append(node[\"full_text\"].replace(\"\\n\",\"\").replace(\",\",\"\"))\n",
    "    return parameters\n",
    "\n",
    "def remove_root_nodes(all_tweets):\n",
    "    df = pd.DataFrame(all_tweets)\n",
    "    #Remove Root Nodes \n",
    "    df=df[df.tree_id!=df.tweet_id]\n",
    "    return df\n",
    "\n",
    "def stable_user(username):\n",
    "    if username.lower() in [\"diezeit\",\"derspiegel\",\"spiegelonline\",\n",
    "                             \"faznet\", \"morgenmagazin\",\"tagesschau\",\n",
    "                             \"tagesthemen\",\"zdfheute\",\"annewilltalk\",\n",
    "                            \"augstein\",\"dunjahayali\",\"janboehm\",\n",
    "                            \"jkasek\", \"maischberger\",\"nicolediekmann\",\n",
    "                            \"cem_oezdemir\", \"c_lindner\", \"goeringeckardt\",\n",
    "                            \"heikomaas\", \"olafscholz\", \"regsprecher\", \n",
    "                            \"renatekuenast\"]:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "\n",
    "def create_sample(hate_df, counter_df, neutral_df, sample_size=10, \n",
    "                  neutral_sample_size=1, seed=None):\n",
    "    frames = []\n",
    "    for year in [2015, 2016, 2017, 2018]:\n",
    "        frames += [\n",
    "            get_tweets_by_year(hate_df, year=year, sample_size=sample_size, seed=seed),\n",
    "            get_tweets_by_year(counter_df, year=year, sample_size=sample_size, seed=seed),\n",
    "            get_tweets_by_year(neutral_df, year=year, sample_size=neutral_sample_size, seed=seed)\n",
    "        ]\n",
    "    \n",
    "    df = pd.concat(frames)\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def get_tweets_by_year(df, year=2015, sample_size=None, seed=None):\n",
    "    df = df[df.year==year]\n",
    "    if sample_size:\n",
    "        return df.sample(n=sample_size, random_state=seed)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def get_hate_tweets_df(df,sample_size=None, threshold=0.8):\n",
    "    hate_df = df[df.hate_score >= threshold]\n",
    "    return hate_df\n",
    "def get_counter_tweets_df(df,sample_size=None, threshold=0.8):\n",
    "    counter_df = df[df.counter_score >= threshold]\n",
    "    return counter_df\n",
    "def get_neutral_tweets_df(df, sample_size=None, \n",
    "                          threshold1=0.44, threshold2=0.55):\n",
    "    neutral_df = df[df.hate_score >= threshold1]\n",
    "    neutral_df = neutral_df[neutral_df.hate_score <= threshold2]\n",
    "\n",
    "    return neutral_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e4e33-c83f-41d1-966f-20a3774447fd",
   "metadata": {},
   "source": [
    "# Build dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "290f56fe-0d48-43dc-b068-f6ce1b7f2688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 204544/204544 [00:29<00:00, 6969.98it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tweets = {\"year\":[],\n",
    "              \"tree_id\":[],\n",
    "              \"tweet_id\":[],\n",
    "              \"hate_score\":[],\n",
    "              \"counter_score\":[],\n",
    "              \"text\":[]\n",
    "              }\n",
    "\n",
    "total = db.ReplyTree_coll.count_documents({})\n",
    "for tree in tqdm(db.ReplyTree_coll.find({}),total=total):\n",
    "    _id  = tree[\"_id\"]\n",
    "    RT = ReplyTreeWalker(tree)\n",
    "    num_nodes =RT.walk_w_parameters(count_nodes,parameters=0)\n",
    "    if num_nodes >= node_cut_off:\n",
    "        if stable_user(tree[\"screen_name\"]):\n",
    "            \n",
    "            #If you want to save the tree. \n",
    "            #with open(\"../results/colored_trees/\"+_id+\".json\", 'w') as outfile:\n",
    "            #    json.dump(RT.doc, outfile)\n",
    "            #RT.walk(print_nodes)\n",
    "            #print(parameters[0])\n",
    "            parameters = RT.walk_w_parameters(build_dict,None,parameters=[_id,all_tweets])\n",
    "            all_tweets=parameters[1]\n",
    "        #else:\n",
    "        #    print(\"Skipping\",tree[\"screen_name\"])\n",
    "\n",
    "#Remove the root nodes from the sample\n",
    "df = remove_root_nodes(all_tweets)\n",
    "\n",
    "#Split the dataframes by tweet type\n",
    "hate_df = get_hate_tweets_df(df)\n",
    "counter_df = get_counter_tweets_df(df)\n",
    "neutral_df = get_neutral_tweets_df(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea34a47-f429-42f8-9b93-2c6e51b319c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hate       80537\n",
       "neutral    59149\n",
       "counter    22237\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only select the most extreme Tweets for the sampling\n",
    "# see also thresholds in the get_hate(), get_counter() and get_neutral()\n",
    "# functions\n",
    "df['label'] = np.nan\n",
    "df.loc[df['hate_score'] >= 0.8, 'label'] = 'hate'\n",
    "df.loc[df['counter_score'] >= 0.8, 'label'] = 'counter'\n",
    "df.loc[(df['hate_score'] >= 0.44) & (df['hate_score'] <= 0.55), 'label'] = 'neutral'\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492f2eaa-23fa-432f-9751-dc984fc22d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of labelled tweets:     0.45488716523908385\n"
     ]
    }
   ],
   "source": [
    "print(f\"fraction of labelled tweets: \\\n",
    "    {(len(hate_df) + len(counter_df) + len(neutral_df)) / len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdc0bc20-a93c-4e73-8f79-a6ad31eeda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"/home/jana/Projects/CSS_reconquista_internet/analysis/data/tree_samples/data_split_in_classes\"\n",
    "hate_df.to_csv(join(dst, \"hate.csv\"), index=False)\n",
    "counter_df.to_csv(join(dst, \"counter.csv\"), index=False)\n",
    "neutral_df.to_csv(join(dst, \"neutral.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228b254-1b81-43cb-98b9-8a3686e1763e",
   "metadata": {},
   "source": [
    "# Build sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82153893-d072-4d22-8d3b-187da4e2bc87",
   "metadata": {},
   "source": [
    "## Sampling principles:\n",
    "* 25% of samples from each of the four years (2015, 2016, 2017, 2018)\n",
    "* 40% of samples hatespeech, 40% of samples counterspeech, 20% of samples neutral speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1de88ac6-342a-414d-b355-380ac7737f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_existing_samples = True\n",
    "seed = 42 # note: 1_tree_sample.csv was sampled by Joshua without a seed\n",
    "existing_sample_dir = \"/home/jana/Projects/CSS_reconquista_internet/analysis/data/tree_samples/samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d979d7-12bc-454d-962f-22dfdb40ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_existing_samples:\n",
    "    existing_sample_files = os.listdir(existing_sample_dir)\n",
    "    existing_sample_files.sort()\n",
    "    existing_sample_counter = int(existing_sample_files[-1].split(\"_\")[1])\n",
    "    existing_samples = pd.DataFrame()\n",
    "    for f in existing_sample_files:\n",
    "        tmp = pd.read_csv(join(existing_sample_dir, f))\n",
    "        existing_samples = pd.concat([existing_samples, tmp])\n",
    "        \n",
    "existing_samples['label'] = np.nan\n",
    "existing_samples.loc[existing_samples['hate_score'] >= 0.8, 'label'] = 'hate'\n",
    "existing_samples.loc[existing_samples['counter_score'] >= 0.8, 'label'] = 'counter'\n",
    "existing_samples.loc[(existing_samples['hate_score'] >= 0.44) & \\\n",
    "                     (existing_samples['hate_score'] <= 0.55), 'label'] = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8afdbc6c-969d-4f28-b5f6-bd6a263bb2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hate       800\n",
       "counter    800\n",
       "neutral    400\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_samples['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459cb2cf-de97-4ffb-8d51-2cfd8668ec22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015    500\n",
       "2016    500\n",
       "2017    500\n",
       "2018    500\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_samples[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fac720e5-1a87-4f44-bb3b-296d01f17f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample pool hate: 80537\n",
      "sample pool counter: 22237\n",
      "sample pool neutral: 59149\n"
     ]
    }
   ],
   "source": [
    "# load the available pool of examples\n",
    "src = \"/home/jana/Projects/CSS_reconquista_internet/analysis/data/tree_samples/data_split_in_classes\"\n",
    "hate_df = pd.read_csv(join(src, \"hate.csv\"))\n",
    "counter_df = pd.read_csv(join(src, \"counter.csv\"))\n",
    "neutral_df = pd.read_csv(join(src, \"neutral.csv\"))\n",
    "\n",
    "print(f\"sample pool hate: {len(hate_df)}\")\n",
    "print(f\"sample pool counter: {len(counter_df)}\")\n",
    "print(f\"sample pool neutral: {len(neutral_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1779697-6621-4f5f-8336-3fc6ee97210e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaining samples hate: 79779\n",
      "remaining samples counter: 21379\n",
      "remaining samples neutral: 58851\n"
     ]
    }
   ],
   "source": [
    "# remove the existing samples from the available pool of examples\n",
    "hate_df = hate_df[~hate_df[\"tweet_id\"].isin(existing_samples[\"tweet_id\"])]\n",
    "counter_df = counter_df[~counter_df[\"tweet_id\"].isin(existing_samples[\"tweet_id\"])]\n",
    "neutral_df = neutral_df[~neutral_df[\"tweet_id\"].isin(existing_samples[\"tweet_id\"])]\n",
    "\n",
    "print(f\"remaining samples hate: {len(hate_df)}\")\n",
    "print(f\"remaining samples counter: {len(counter_df)}\")\n",
    "print(f\"remaining samples neutral: {len(neutral_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ad0f6a1-7b66-4e99-8474-a9568fba06a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018    56304\n",
       "2017    17500\n",
       "2016     4175\n",
       "2019     1052\n",
       "2015      499\n",
       "2014      152\n",
       "2013       97\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_df[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cde0f43b-c80d-4fdd-8e01-ac2fe2fed8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018    15096\n",
       "2017     3942\n",
       "2016     1593\n",
       "2019      519\n",
       "2015      133\n",
       "2014       71\n",
       "2013       25\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_df[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cf1e95b-b5fd-4cff-b195-bcfaebb286dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018    36896\n",
       "2017    14967\n",
       "2016     4766\n",
       "2015      946\n",
       "2019      896\n",
       "2014      233\n",
       "2013      147\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_df[\"year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9a4dccb-8dfc-448a-ba65-58f5fd7ef32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = create_sample(hate_df, counter_df, neutral_df, sample_size=100,\n",
    "                             neutral_sample_size=50, seed=seed)\n",
    "\n",
    "assert len(set(sample[\"tweet_id\"]).intersection(set(existing_samples[\"tweet_id\"]))) == 0\n",
    "sample.to_csv(join(existing_sample_dir, \n",
    "                   f\"batch_{existing_sample_counter + 1}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
