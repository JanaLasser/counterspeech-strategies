{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e791908e-3a28-4dbd-b498-a5ed00e7a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from transformers import XLMRobertaForSequenceClassification\n",
    "from transformers import XLMRobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cdeef-71d0-46f0-ac9a-4fd79a22354d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ab2329-5bd1-47db-8bc2-5c73cd20a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_samples(existing_sample_dir, verbose=False):\n",
    "    existing_sample_files = os.listdir(existing_sample_dir)\n",
    "    existing_batches = np.asarray([int(re.findall(r'\\d+', f)[0]) for f in existing_sample_files])\n",
    "    existing_sample_counter = existing_batches.max()\n",
    "    existing_samples = pd.DataFrame()\n",
    "    \n",
    "    print(\"loading existing samples\", existing_sample_files)\n",
    "    for f in existing_sample_files:\n",
    "        tmp = pd.read_csv(join(existing_sample_dir, f))\n",
    "        existing_samples = pd.concat([existing_samples, tmp])\n",
    "        \n",
    "    existing_samples['label'] = np.nan\n",
    "    existing_samples.loc[existing_samples['hate_score'] >= 0.8, 'label'] = 'hate'\n",
    "    existing_samples.loc[existing_samples['counter_score'] >= 0.8, 'label'] = 'counter'\n",
    "    existing_samples.loc[(existing_samples['hate_score'] >= 0.44) & \\\n",
    "                         (existing_samples['hate_score'] <= 0.55), 'label'] = 'neutral'\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"existing sample spread to hate / counter / neutral:\")\n",
    "        print(existing_samples['label'].value_counts())\n",
    "        print()\n",
    "        print(\"existing sample spread to the years 2015 / 2016 / 2017 / 2018\")\n",
    "        print(existing_samples[\"year\"].value_counts())\n",
    "        print()\n",
    "\n",
    "    return existing_samples, existing_sample_counter\n",
    "\n",
    "\n",
    "def get_sample_pool(sample_pool_dir):\n",
    "    hate_df = pd.read_csv(join(sample_pool_dir, \"hate.csv\"))\n",
    "    hate_df[\"hate_label\"] = \"hate\"\n",
    "    counter_df = pd.read_csv(join(sample_pool_dir, \"counter.csv\"))\n",
    "    counter_df[\"hate_label\"] = \"counter\"\n",
    "    neutral_df = pd.read_csv(join(sample_pool_dir, \"neutral.csv\"))\n",
    "    neutral_df[\"hate_label\"] = \"neutral\"\n",
    "    \n",
    "    hate_df = hate_df.drop_duplicates(subset=[\"tweet_id\"])\n",
    "    counter_df = counter_df.drop_duplicates(subset=[\"tweet_id\"])\n",
    "    neutral_df = neutral_df.drop_duplicates(subset=[\"tweet_id\"])\n",
    "    \n",
    "    print(f\"sample pool hate: {len(hate_df)}\")\n",
    "    print(f\"sample pool counter: {len(counter_df)}\")\n",
    "    print(f\"sample pool neutral: {len(neutral_df)}\")\n",
    "    print()\n",
    "    \n",
    "    # combine hate, counter and neutral into one pool of available samples\n",
    "    df = pd.concat([hate_df, counter_df, neutral_df])\n",
    "    \n",
    "    print(\"sample pool 2015: {}\".format(len(df[df[\"year\"] == 2015])))\n",
    "    print(\"sample pool 2016: {}\".format(len(df[df[\"year\"] == 2016])))\n",
    "    print(\"sample pool 2017: {}\".format(len(df[df[\"year\"] == 2017])))\n",
    "    print(\"sample pool 2018: {}\".format(len(df[df[\"year\"] == 2018])))\n",
    "    print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sample(\n",
    "    df, \n",
    "    year_sample_sizes={2015:125, 2016:125, 2017:125, 2018:125},\n",
    "    years=[2015, 2016, 2017, 2018],\n",
    "    minority_dimension=\"strategy\",\n",
    "    minority_classes=[],\n",
    "    seed=None):\n",
    "\n",
    "    if len(minority_classes) > 0:\n",
    "        df = df[df[minority_dimension].isin(minority_classes)]\n",
    "    \n",
    "    frames = []\n",
    "    for year in years:\n",
    "        frames += [\n",
    "            get_tweets_by_year(df, year, \n",
    "                    year_sample_size=year_sample_sizes[year], seed=seed),\n",
    "        ]\n",
    "    \n",
    "    df = pd.concat(frames)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_tweets_by_year(df, year, year_sample_size=100, seed=None):\n",
    "    df = df[df.year==year]\n",
    "    sampled_df = df.sample(n=year_sample_size, random_state=seed)\n",
    "    \n",
    "    # in cases where there aren't many tweets for a given year, the desired\n",
    "    # sample size might be larger than the remaining tweets in the sampling pool\n",
    "    assert len(sampled_df) == year_sample_size\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228b254-1b81-43cb-98b9-8a3686e1763e",
   "metadata": {},
   "source": [
    "# Build sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82153893-d072-4d22-8d3b-187da4e2bc87",
   "metadata": {},
   "source": [
    "## Sampling principles\n",
    "Starting with batch 7, which is the first batch drawn with predictions from transformer models for language and strategy\n",
    "* remove all foreign language tweets\n",
    "* 20% of samples from year 2016, 40% from 2017 and 40% from 2018\n",
    "* combine hate, counter and neutral into a single sample pool\n",
    "* oversample \"construct\", \"sarc\" and \"leave_fact\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a12029-28ac-401c-8434-096c4205f360",
   "metadata": {},
   "source": [
    "## Sampling workflow\n",
    "* (optional): update label mapping, update training data, copy training data to NVcluster\n",
    "* train transformer model for strategy on the latest labelled data on the NVcluster\n",
    "* copy best strategy (or goal) model to `sampling_scripts/strategy_model/` on the NVcluster\n",
    "* (optional): update label mapping in the output of `sampling_scripts/infer_strategy.py`\n",
    "* run strategy (or goal) inference on NVcluster\n",
    "* download the data with the inferred values (rsync command below)\n",
    "* run rest of this notebook to create new sample batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c9b851-22d2-489c-b6ee-a3385d8ace7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receiving incremental file list\n",
      "config.json\n",
      "          1,050 100%    1.00MB/s    0:00:00 (xfr#1, to-chk=2/4)\n",
      "pytorch_model.bin\n",
      "  1,112,278,560 100%  354.29MB/s    0:00:02 (xfr#2, to-chk=1/4)\n",
      "training_args.bin\n",
      "          2,799 100%    2.37kB/s    0:00:01 (xfr#3, to-chk=0/4)\n",
      "\n",
      "sent 233,635 bytes  received 2,754,078 bytes  663,936.22 bytes/sec\n",
      "total size is 1,112,282,409  speedup is 372.29\n"
     ]
    }
   ],
   "source": [
    "# get the best strategy model\n",
    "! rsync -avze ssh jlasse@nvcluster:/home/jlasse/GermanHass/strategy_analysis/roberta/results/best_model/ best_model/ --progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68beb1ac-1fc5-459b-8fc1-e3954199d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending incremental file list\n",
      "config.json\n",
      "          1,050 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=2/4)\n",
      "pytorch_model.bin\n",
      "  1,112,278,560 100%   22.53MB/s    0:00:47 (xfr#2, to-chk=1/4)\n",
      "training_args.bin\n",
      "          2,799 100%   28.77kB/s    0:00:00 (xfr#3, to-chk=0/4)\n",
      "\n",
      "sent 1,028,946,087 bytes  received 233,627 bytes  20,379,796.32 bytes/sec\n",
      "total size is 1,112,282,409  speedup is 1.08\n"
     ]
    }
   ],
   "source": [
    "# upload the best strategy model\n",
    "! rsync -avze ssh best_model/ jlasse@nvcluster:/home/jlasse/GermanHass/sampling_scripts/strategy_model/ --progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925e2f18-168e-4d6c-9c12-a9d73aec98d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending incremental file list\n",
      "./\n",
      "config.json\n",
      "pytorch_model.bin\n",
      "training_args.bin\n",
      "\n",
      "sent 1,028,873,347 bytes  received 76 bytes  37,413,579.02 bytes/sec\n",
      "total size is 1,112,269,490  speedup is 1.08\n"
     ]
    }
   ],
   "source": [
    "# upload the best language model\n",
    "! rsync -avze ssh ../foreign_language_prediction/best_model/ jlasse@nvcluster:/home/jlasse/GermanHass/sampling_scripts/language_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335b3165-2959-41ba-96a0-312c2c93aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receiving incremental file list\n",
      "counter.csv\n",
      "hate.csv\n",
      "neutral.csv\n",
      "\n",
      "sent 67,951 bytes  received 25,317,865 bytes  1,538,534.30 bytes/sec\n",
      "total size is 58,934,255  speedup is 2.32\n"
     ]
    }
   ],
   "source": [
    "# download the inferred values\n",
    "! rsync -avze ssh jlasse@nvcluster:/home/jlasse/GermanHass/sampling_scripts/data_split_in_classes/ /home/jana/Projects/CSS_reconquista_internet/analysis/data/tree_samples/data_split_in_classes_inferred_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee64d0d-2f6a-4005-9892-b31feeb6334d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    33339\n",
       "3    17619\n",
       "0    13445\n",
       "2    11133\n",
       "4     5001\n",
       "Name: strategy, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/jana/Projects/CSS_reconquista_internet/analysis/data/tree_samples/data_split_in_classes_inferred_strategy/hate.csv\")\n",
    "df[\"strategy\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e9c1d5-19f2-42ad-b1a6-14e25e0a69c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    35705\n",
       "5    18966\n",
       "3     9398\n",
       "1     7352\n",
       "4     6067\n",
       "0     3049\n",
       "Name: goal, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"goal\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ad1c42-6132-451b-a07a-5adcf3c2a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condensed id mapping\n",
    "#     'info': 0,\n",
    "#     'opin': 1,\n",
    "#     'quest': 0,\n",
    "#     'conseq': 0,\n",
    "#     'correct': 0,\n",
    "#     'inconsist': 0,\n",
    "#     'sarc': 2,\n",
    "#     'insult-pers': 3,\n",
    "#     'insult-ism': 3,\n",
    "#     'insult-polit': 3,\n",
    "#     'insult-inst': 3,\n",
    "#     'other': 4,\n",
    "#     'unint': 4\n",
    "\n",
    "condensed_id_to_label = {\n",
    "    0:\"construct\",\n",
    "    1:\"opin\",\n",
    "    2:\"sarc\",\n",
    "    3:\"leave_fact\",\n",
    "    4:\"other_new\",\n",
    "}\n",
    "minority_dimension = \"strategy\"\n",
    "minority_classes = [0, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac9ed39-db80-4d5e-8fbd-04face2eb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condensed id mapping\n",
    "#     'strength': 0, # pose\n",
    "#     'just': 0, \n",
    "#     'threat': 1,\n",
    "#     'weak': 2,\n",
    "#     'emph-ground': 3, # emph\n",
    "#     'emph-prob': 3,\n",
    "#     'neutral': 4,\n",
    "#     'unint': 5\n",
    "\n",
    "condensed_id_to_label = {\n",
    "    0:\"pose\",\n",
    "    1:\"threat\",\n",
    "    2:\"weak\",\n",
    "    3:\"emph\",\n",
    "    4:\"neutral\",\n",
    "    5:\"unint\"\n",
    "}\n",
    "minority_dimension = \"goal\"\n",
    "minority_classes = [0, 3]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1cc2a12-3a8b-4cd7-9747-03e9a044a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/jana/Projects/CSS_reconquista_internet/analysis/data/\"\n",
    "existing_sample_dir = join(data_dir, \"tree_samples/samples\")\n",
    "sample_pool_dir = join(data_dir, \"tree_samples/data_split_in_classes_inferred_strategy\")\n",
    "\n",
    "foreign_language_classification = True\n",
    "seed = 42 # note: batch_1_LT_AS.csv was sampled by Joshua without a seed\n",
    "# note: batches 2-11 with strategy dimension oversampling (construct, sarc & leave fact)\n",
    "# note: batches 12, 13 with goal dimension oversampling (pose & emph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23215e1e-2b90-4995-9fe7-bdedb7559ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading existing samples ['batch_3_AH_LT.csv', 'batch_4_AH.csv', 'batch_9_LT_EM.csv', 'batch_9_EM.csv', 'batch_5_LT_AH.csv', 'batch_6_AS_AH.csv', 'batch_7b_LT.csv', 'batch_8_EM_LT.csv', 'batch_11_EM.csv', 'batch_5_LT.csv', 'batch_5_AH.csv', 'batch_13_LT.csv', 'batch_12_LT_EM.csv', 'batch_3_AH.csv', 'batch_12_EM.csv', 'batch_10_LT_EM.csv', 'batch_9_EM_LT.csv', 'batch_4_AS_AH.csv', 'batch_7_LT2_AH.csv', 'batch_8_EM.csv', 'batch_3_AS_AH.csv', 'batch_3_LT_AH.csv', 'batch_11_LT.csv', 'batch_2_LT_AS_AH.csv', 'batch_10_EM.csv', 'batch_13_EM_LT.csv', 'batch_10_LT.csv', 'batch_4_LT_AH.csv', 'batch_4_AH_LT.csv', 'batch_11_LT_EM.csv', 'batch_7_LT1_AH.csv', 'batch_6_AH_LT.csv', 'batch_7a_LT.csv', 'batch_5_AH_LT.csv', 'batch_8_LT_EM.csv', 'batch_6_AH.csv', 'batch_13_EM.csv', 'batch_13_LT_EM.csv', 'batch_3_LT.csv', 'batch_4_LT.csv', 'batch_1_LT_AS.csv', 'batch_6_LT.csv', 'batch_8_LT.csv', 'batch_10_EM_LT.csv', 'batch_9_LT.csv', 'batch_12_EM_LT.csv', 'batch_6_AS.csv', 'batch_4_AS.csv', 'batch_12_LT.csv', 'batch_3_AS.csv', 'batch_11_EM_LT.csv', 'batch_5_AS_AH.csv', 'batch_5_AS.csv', 'batch_6_LT_AH.csv']\n",
      "*** drawing samples for batch 14 for rater EM ***\n",
      "loading existing samples ['batch_3_AH_LT.csv', 'batch_4_AH.csv', 'batch_9_LT_EM.csv', 'batch_9_EM.csv', 'batch_5_LT_AH.csv', 'batch_6_AS_AH.csv', 'batch_7b_LT.csv', 'batch_8_EM_LT.csv', 'batch_11_EM.csv', 'batch_5_LT.csv', 'batch_5_AH.csv', 'batch_13_LT.csv', 'batch_12_LT_EM.csv', 'batch_3_AH.csv', 'batch_12_EM.csv', 'batch_10_LT_EM.csv', 'batch_9_EM_LT.csv', 'batch_4_AS_AH.csv', 'batch_7_LT2_AH.csv', 'batch_8_EM.csv', 'batch_3_AS_AH.csv', 'batch_3_LT_AH.csv', 'batch_11_LT.csv', 'batch_2_LT_AS_AH.csv', 'batch_10_EM.csv', 'batch_13_EM_LT.csv', 'batch_10_LT.csv', 'batch_4_LT_AH.csv', 'batch_4_AH_LT.csv', 'batch_11_LT_EM.csv', 'batch_7_LT1_AH.csv', 'batch_6_AH_LT.csv', 'batch_7a_LT.csv', 'batch_5_AH_LT.csv', 'batch_8_LT_EM.csv', 'batch_6_AH.csv', 'batch_13_EM.csv', 'batch_13_LT_EM.csv', 'batch_3_LT.csv', 'batch_4_LT.csv', 'batch_1_LT_AS.csv', 'batch_6_LT.csv', 'batch_8_LT.csv', 'batch_10_EM_LT.csv', 'batch_9_LT.csv', 'batch_12_EM_LT.csv', 'batch_6_AS.csv', 'batch_4_AS.csv', 'batch_12_LT.csv', 'batch_3_AS.csv', 'batch_11_EM_LT.csv', 'batch_5_AS_AH.csv', 'batch_5_AS.csv', 'batch_6_LT_AH.csv']\n",
      "existing sample spread to hate / counter / neutral:\n",
      "hate       9129\n",
      "neutral    4762\n",
      "counter    2409\n",
      "Name: label, dtype: int64\n",
      "\n",
      "existing sample spread to the years 2015 / 2016 / 2017 / 2018\n",
      "2018    6101\n",
      "2017    5732\n",
      "2016    3538\n",
      "2015     929\n",
      "Name: year, dtype: int64\n",
      "\n",
      "sample pool hate: 80276\n",
      "sample pool counter: 21865\n",
      "sample pool neutral: 59007\n",
      "\n",
      "sample pool 2015: 2014\n",
      "sample pool 2016: 10597\n",
      "sample pool 2017: 36771\n",
      "sample pool 2018: 108603\n",
      "\n",
      "remaining samples hate: 71586\n",
      "remaining samples counter: 18167\n",
      "remaining samples neutral: 54011\n",
      "\n",
      "remaining samples 2015: 1151\n",
      "remaining samples 2016: 6935\n",
      "remaining samples 2017: 30735\n",
      "remaining samples 2018: 101817\n",
      "\n",
      "\n",
      "****************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raters = [\"EM\"]\n",
    "co_raters = {\"EM\":\"AH\"}\n",
    "year_sample_sizes = {2015:0, 2016:100, 2017:200, 2018:200}\n",
    "years = [2015, 2016, 2017, 2018]\n",
    "co_rater_frac = 0.1\n",
    "\n",
    "# following the naming convention, the batch counter does not increase for\n",
    "# batches which are only rated by a single rater. I.e. there will be a\n",
    "# batch_3_AS, batch_3_LT and batch_3_AH file for the three raters AS, LT and AH\n",
    "# for the third batch. To ensure this behaviour, we get the batch counter before\n",
    "# we iterate over the raters to create new files.\n",
    "_, existing_sample_counter = get_existing_samples(existing_sample_dir)\n",
    "\n",
    "for rater in raters:\n",
    "    print(f\"*** drawing samples for batch {existing_sample_counter + 1} for rater {rater} ***\")\n",
    "    \n",
    "    # load the existing samples including samples that were created for the\n",
    "    # current batch, but do not increase the batch counter\n",
    "    existing_samples, _ = get_existing_samples(existing_sample_dir, verbose=True)\n",
    "\n",
    "    # load the available pool of examples\n",
    "    sample_pool = get_sample_pool(sample_pool_dir)\n",
    "    \n",
    "    # remove foreign language entries from the available pool of samples\n",
    "    if foreign_language_classification:\n",
    "        sample_pool = sample_pool[sample_pool[\"foreign\"] == 0]\n",
    "\n",
    "    # remove the existing samples from the available pool of examples\n",
    "    df = sample_pool[~sample_pool[\"tweet_id\"].isin(existing_samples[\"tweet_id\"])].copy()\n",
    "    print(\"remaining samples hate: {}\".format(len(df[df[\"hate_label\"] == \"hate\"])))\n",
    "    print(\"remaining samples counter: {}\".format(len(df[df[\"hate_label\"] == \"counter\"])))\n",
    "    print(\"remaining samples neutral: {}\".format(len(df[df[\"hate_label\"] == \"neutral\"])))\n",
    "    print()\n",
    "    print(\"remaining samples 2015: {}\".format(len(df[df[\"year\"] == 2015])))\n",
    "    print(\"remaining samples 2016: {}\".format(len(df[df[\"year\"] == 2016])))\n",
    "    print(\"remaining samples 2017: {}\".format(len(df[df[\"year\"] == 2017])))\n",
    "    print(\"remaining samples 2018: {}\".format(len(df[df[\"year\"] == 2018])))\n",
    "    print()\n",
    "    #del sample_pool\n",
    "    \n",
    "    sample = create_sample(\n",
    "        df, \n",
    "        year_sample_sizes=year_sample_sizes,\n",
    "        years=years,\n",
    "        minority_dimension=minority_dimension,\n",
    "        minority_classes=minority_classes,\n",
    "        seed=42)\n",
    "    \n",
    "    # replace semicolons with commas, because we use semicolons as delimiters\n",
    "    sample[\"text\"] = sample[\"text\"].apply(lambda x: x.replace(\";\", \",\"))\n",
    "    \n",
    "    # sanity check ensuring that no Tweets in the newly created sample are\n",
    "    # already included in the existing samples\n",
    "    assert len(set(sample[\"tweet_id\"])\\\n",
    "               .intersection(set(existing_samples[\"tweet_id\"]))) == 0\n",
    "    \n",
    "    # sanity check that there are no duplicated tweets in the sample\n",
    "    assert len(sample) == len(sample[\"tweet_id\"].drop_duplicates())\n",
    "    \n",
    "    # save the new sample to disk. The file name encodes the batch number and\n",
    "    # rater\n",
    "    sample_name = f\"batch_{existing_sample_counter + 1}_{rater}.csv\"\n",
    "    sample = sample.drop(columns=[\"foreign\", \"hate_label\", \"strategy\"] + \\\n",
    "                         list(condensed_id_to_label.values()))\n",
    "    sample.to_csv(join(existing_sample_dir, sample_name), index=False)\n",
    "    \n",
    "    total_sample_size = sum(list(year_sample_sizes.values()))\n",
    "    co_rater_sample_size = int(total_sample_size * co_rater_frac)\n",
    "    co_rater_sample = sample.sample(n=co_rater_sample_size, random_state=seed)\n",
    "    co_rater_sample_name = f\"batch_{existing_sample_counter + 1}_{rater}_{co_raters[rater]}.csv\"\n",
    "    co_rater_sample.to_csv(join(existing_sample_dir, co_rater_sample_name),\n",
    "                           index=False)\n",
    "    \n",
    "    print()\n",
    "    print(\"****************************\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05567b5b-15ac-4613-9567-f0bbbd83b953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
